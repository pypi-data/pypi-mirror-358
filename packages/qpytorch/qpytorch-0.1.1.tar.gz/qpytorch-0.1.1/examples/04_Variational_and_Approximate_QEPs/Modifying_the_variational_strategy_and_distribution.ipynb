{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[KeOps] Warning : There were warnings or errors :\n",
      "/bin/sh: brew: command not found\n",
      "\n",
      "[KeOps] Warning : CUDA libraries not found or could not be loaded; Switching to CPU only.\n",
      "[KeOps] Warning : OpenMP library 'libomp' not found.\n",
      "[KeOps] Warning : OpenMP support is not available. Disabling OpenMP.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import qpytorch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the Variational Strategy/Variational Distribution\n",
    "\n",
    "The predictive distribution for approximate QEPs is given by\n",
    "\n",
    "$$\n",
    "p( \\mathbf f(\\mathbf x^*) ) = \\int_{\\mathbf u} p( f(\\mathbf x^*) \\mid \\mathbf u) \\: q(\\mathbf u) \\: d\\mathbf u,\n",
    "\\quad\n",
    "q(\\mathbf u) = \\mathcal Q( \\mathbf m, \\mathbf S).\n",
    "$$\n",
    "\n",
    "$\\mathbf u$ represents the function values at the $m$ inducing points.\n",
    "Here, $\\mathbf m \\in \\mathbb R^m$ and $\\mathbf S \\in \\mathbb R^{m \\times m}$ are learnable parameters.\n",
    "\n",
    "If $m$ (the number of inducing points) is quite large, the number of learnable parameters in $\\mathbf S$ can be quite unwieldy.\n",
    "Furthermore, a large $m$ might make some of the computations rather slow.\n",
    "Here we show a few ways to use different [variational distributions](https://qepytorch.readthedocs.io/en/stable/variational.html#variational-distributions) and\n",
    "[variational strategies](https://qepytorch.readthedocs.io/en/stable/variational.html#variational-strategies) to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental setup\n",
    "\n",
    "We're going to train an approximate QEP on a medium-sized regression dataset, taken from the UCI repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "\n",
    "if not smoke_test and not os.path.isfile('../elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', '../elevators.mat')\n",
    "\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(1000, 3), torch.randn(1000)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('../elevators.mat')['data'])\n",
    "    X = data[:, :-1]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0]) - 1\n",
    "    y = data[:, -1]\n",
    "\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some quick training/testing code\n",
    "\n",
    "This will allow us to train/test different model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "num_epochs = 1 if smoke_test else 10\n",
    "\n",
    "\n",
    "# Our testing script takes in a QPyTorch MLL (objective function) class\n",
    "# and then trains/tests an approximate QEP with it on the supplied dataset\n",
    "\n",
    "def train_and_test_approximate_qep(model_cls):\n",
    "    inducing_points = torch.randn(128, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "    model = model_cls(inducing_points)\n",
    "    likelihood = qpytorch.likelihoods.QExponentialLikelihood(power=model.power)\n",
    "    mll = qpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.numel())\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=f\"Training {model_cls.__name__}\")\n",
    "    for i in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            epochs_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    error = torch.mean(torch.abs(means - test_y.cpu()))\n",
    "    print(f\"Test {model_cls.__name__} MAE: {error.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Standard Approach\n",
    "\n",
    "As a default, we'll use the default [VariationalStrategy](https://qepytorch.readthedocs.io/en/stable/variational.html#id1) class with a [CholeskyVariationalDistribution](https://qepytorch.readthedocs.io/en/stable/variational.html#choleskyvariationaldistribution).\n",
    "The `CholeskyVariationalDistribution` class allows $\\mathbf S$ to be on any positive semidefinite matrix. This is the most general/expressive option for approximate QEPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWER = 1.0\n",
    "class StandardApproximateQEP(qpytorch.models.ApproximateQEP):\n",
    "    def __init__(self, inducing_points):\n",
    "        self.power = torch.tensor(POWER)\n",
    "        variational_distribution = qpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2), power=self.power)\n",
    "        variational_strategy = qpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = qpytorch.means.ConstantMean()\n",
    "        self.covar_module = qpytorch.kernels.ScaleKernel(qpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return qpytorch.distributions.MultivariateQExponential(mean_x, covar_x, power=self.power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2337f30573840598e23d9bc560da255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training StandardApproximateQEP:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test StandardApproximateQEP MAE: 0.08965195715427399\n"
     ]
    }
   ],
   "source": [
    "train_and_test_approximate_qep(StandardApproximateQEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing parameters\n",
    "\n",
    "### MeanFieldVariationalDistribution: a diagonal $\\mathbf S$ matrix \n",
    "\n",
    "One way to reduce the number of parameters is to restrict that $\\mathbf S$ is only diagonal. This is less expressive, but the number of parameters is now linear in $m$ instead of quadratic.\n",
    "\n",
    "All we have to do is take the previous example, and change `CholeskyVariationalDistribution` (full $\\mathbf S$ matrix) to [MeanFieldVariationalDistribution](https://qepytorch.readthedocs.io/en/stable/variational.html#meanfieldvariationaldistribution) (diagonal $\\mathbf S$ matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanFieldApproximateQEP(qpytorch.models.ApproximateQEP):\n",
    "    def __init__(self, inducing_points):\n",
    "        self.power = torch.tensor(POWER)\n",
    "        variational_distribution = qpytorch.variational.MeanFieldVariationalDistribution(inducing_points.size(-2), power=self.power)\n",
    "        variational_strategy = qpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = qpytorch.means.ConstantMean()\n",
    "        self.covar_module = qpytorch.kernels.ScaleKernel(qpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return qpytorch.distributions.MultivariateQExponential(mean_x, covar_x, power=self.power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aaedda28be24508ae96abbc6be8aa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training MeanFieldApproximateQEP:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MeanFieldApproximateQEP MAE: 0.08979436010122299\n"
     ]
    }
   ],
   "source": [
    "train_and_test_approximate_qep(MeanFieldApproximateQEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeltaVariationalDistribution: no $\\mathbf S$ matrix \n",
    "\n",
    "A more extreme method of reducing parameters is to get rid of $\\mathbf S$ entirely. This corresponds to learning a delta distribution ($\\mathbf u = \\mathbf m$) rather than a multivariate Normal distribution for $\\mathbf u$. In other words, this corresponds to performing MAP estimation rather than variational inference.\n",
    "\n",
    "In QPyTorch, getting rid of $\\mathbf S$ can be accomplished by using a [DeltaVariationalDistribution](https://qepytorch.readthedocs.io/en/stable/variational.html#deltavariationaldistribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPApproximateQEP(qpytorch.models.ApproximateQEP):\n",
    "    def __init__(self, inducing_points):\n",
    "        self.power = torch.tensor(POWER)\n",
    "        variational_distribution = qpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2), power=self.power)\n",
    "        variational_strategy = qpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = qpytorch.means.ConstantMean()\n",
    "        self.covar_module = qpytorch.kernels.ScaleKernel(qpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return qpytorch.distributions.MultivariateQExponential(mean_x, covar_x, power=self.power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00da7049f401474280f9fab5ca377f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training MAPApproximateQEP:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPApproximateQEP MAE: 0.08309302479028702\n"
     ]
    }
   ],
   "source": [
    "train_and_test_approximate_qep(MAPApproximateQEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing computation (through decoupled inducing points)\n",
    "\n",
    "One way to reduce the computational complexity is to use separate inducing points for the mean and covariance computations. The [Orthogonally Decoupled Variational Gaussian Processes](https://arxiv.org/abs/1809.08820) method of Salimbeni et al. (2018) uses more inducing points for the (computationally easy) mean computations and fewer inducing points for the (computationally intensive) covariance computations.\n",
    "\n",
    "In QPyTorch we implement this method in a modular way. The [OrthogonallyDecoupledVariationalStrategy](https://qepytorch.readthedocs.io/en/stable/variational.html#qpytorch.variational.OrthogonallyDecoupledVariationalStrategy) defines the variational strategy for the mean inducing points. It wraps an existing variational strategy/distribution that defines the covariance inducing points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_orthogonal_vs(model, train_x):\n",
    "    mean_inducing_points = torch.randn(1000, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "    covar_inducing_points = torch.randn(100, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "\n",
    "    covar_variational_strategy = qpytorch.variational.VariationalStrategy(\n",
    "        model, covar_inducing_points,\n",
    "        qpytorch.variational.CholeskyVariationalDistribution(covar_inducing_points.size(-2), power=torch.tensor(POWER)),\n",
    "        learn_inducing_locations=True,\n",
    "    )\n",
    "\n",
    "    variational_strategy = qpytorch.variational.OrthogonallyDecoupledVariationalStrategy(\n",
    "        covar_variational_strategy, mean_inducing_points,\n",
    "        qpytorch.variational.DeltaVariationalDistribution(mean_inducing_points.size(-2), power=torch.tensor(POWER)),\n",
    "    )\n",
    "    return variational_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthDecoupledApproximateQEP(qpytorch.models.ApproximateQEP):\n",
    "    def __init__(self, inducing_points):\n",
    "        self.power = torch.tensor(POWER)\n",
    "        variational_distribution = qpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-2), power=self.power)\n",
    "        variational_strategy = make_orthogonal_vs(self, train_x)\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = qpytorch.means.ConstantMean()\n",
    "        self.covar_module = qpytorch.kernels.ScaleKernel(qpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return qpytorch.distributions.MultivariateQExponential(mean_x, covar_x, power=self.power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da35a86ef43461c86a2e53b4b61f4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training OrthDecoupledApproximateQEP:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.10/site-packages/linear_operator/utils/sparse.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  if nonzero_indices.storage():\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.10/site-packages/linear_operator/utils/sparse.py:66: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:620.)\n",
      "  res = cls(index_tensor, value_tensor, interp_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test OrthDecoupledApproximateQEP MAE: 0.08983905613422394\n"
     ]
    }
   ],
   "source": [
    "train_and_test_approximate_qep(OrthDecoupledApproximateQEP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
