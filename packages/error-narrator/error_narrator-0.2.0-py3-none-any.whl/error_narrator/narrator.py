import os
import logging
import asyncio
from gradio_client import Client as GradioClient
from openai import OpenAI, AsyncOpenAI
from rich.console import Console
from rich.markdown import Markdown

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –±–∞–∑–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# –ü–æ–Ω–∏–∂–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è httpx –∏ gradio_client, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–π —à—É–º
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("gradio_client").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

class NarratorException(Exception):
    """–ë–∞–∑–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ErrorNarrator."""
    pass

class ApiKeyNotFoundError(NarratorException):
    """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ API-–∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω."""
    pass

class ErrorNarrator:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –æ—à–∏–±–æ–∫ —Å –ø–æ–º–æ—â—å—é AI.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤: 'gradio' (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –±–µ—Å–ø–ª–∞—Ç–Ω–æ) –∏ 'openai'.
    """
    DEFAULT_PROMPT_TEMPLATE = (
        "You are an expert Python developer's assistant. An internal application error occurred. "
        "Your task is to provide a comprehensive analysis of the traceback for the developer in Russian. "
        "Your response must be structured in Markdown and include these sections:\n\n"
        "### üéØ –ü—Ä–∏—á–∏–Ω–∞ –æ—à–∏–±–∫–∏\n"
        "A clear, concise explanation of the error's root cause.\n\n"
        "### üìç –ú–µ—Å—Ç–æ –æ—à–∏–±–∫–∏\n"
        "The exact file and line number, with a code snippet showing the context (the error line and a few lines around it).\n\n"
        "### üõ†Ô∏è –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ\n"
        "A clear, actionable suggestion for fixing the issue. Provide a code snippet using a diff format (lines with `-` for removal, `+` for addition) to illustrate the change.\n\n"
        "### üéì –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç (–û–±—É—á–∞—é—â–∏–π –º–æ–º–µ–Ω—Ç)\n"
        "A brief explanation of the underlying concept that caused the error, to help the developer avoid similar mistakes in the future.\n\n"
        "Here is the technical traceback:\n"
        "```\n{traceback}\n```\n\n"
        "Provide a structured analysis for the developer's logs. Do not address the user. Do not ask for more code or provide any disclaimers."
    )
    DEFAULT_API_KEY_ERROR = (
        "API-–∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.\n"
        "–î–ª—è 'gradio': –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º Space (–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è HUGGINGFACE_API_KEY).\n"
        "–î–ª—è 'openai': –∫–ª—é—á –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω (–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è OPENAI_API_KEY)."
    )
    GRADIO_MODEL_ID = "hysts/mistral-7b"
    OPENAI_MODEL_ID = "gpt-3.5-turbo"

    def __init__(self, provider: str = 'gradio', api_key: str = None, model_id: str = None, prompt_template: str = None, **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ErrorNarrator.

        :param provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π ('gradio' –∏–ª–∏ 'openai').
        :param api_key: API-–∫–ª—é—á. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±—É–¥–µ—Ç –≤–∑—è—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
                        (HUGGINGFACE_API_KEY –¥–ª—è 'gradio', OPENAI_API_KEY –¥–ª—è 'openai').
        :param model_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        :param prompt_template: –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏.
        :param kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, temperature, max_new_tokens).
        """
        self.provider = provider
        self.prompt_template = prompt_template or self.DEFAULT_PROMPT_TEMPLATE
        self.model_params = kwargs
        self.cache = {} # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–µ—à

        if self.provider == 'gradio':
            self.api_key = api_key or os.getenv("HUGGINGFACE_API_KEY")
            self.model_id = model_id or self.GRADIO_MODEL_ID
            self.client = GradioClient(self.model_id, hf_token=self.api_key)
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è Gradio, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
            self.model_params.setdefault('temperature', 0.6)
            self.model_params.setdefault('max_new_tokens', 1024)
            self.model_params.setdefault('top_p', 0.9)
            self.model_params.setdefault('top_k', 50)
            self.model_params.setdefault('repetition_penalty', 1.2)

        elif self.provider == 'openai':
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                raise ApiKeyNotFoundError(self.DEFAULT_API_KEY_ERROR)
            self.model_id = model_id or self.OPENAI_MODEL_ID
            self.client = OpenAI(api_key=self.api_key)
            self.async_client = AsyncOpenAI(api_key=self.api_key)
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è OpenAI, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
            self.model_params.setdefault('temperature', 0.7)
            self.model_params.setdefault('max_tokens', 1024) # OpenAI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 'max_tokens'
        else:
            raise ValueError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä. –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: 'gradio', 'openai'")

    def _build_prompt(self, traceback: str) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏."""
        return self.prompt_template.format(traceback=traceback)

    # --- –ú–µ—Ç–æ–¥—ã –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ Gradio ---

    def _predict_gradio(self, prompt: str) -> str:
        logger.info(f"–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Gradio (–º–æ–¥–µ–ª—å: {self.model_id})...")
        try:
            result = self.client.predict(
                prompt,
                self.model_params.get('max_new_tokens'),
                self.model_params.get('temperature'),
                self.model_params.get('top_p'),
                self.model_params.get('top_k'),
                self.model_params.get('repetition_penalty'),
                api_name="/chat"
            )
            return result.strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Gradio: {e}")
            return f"–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI. (–û—à–∏–±–∫–∞ Gradio: {e})"

    async def _predict_async_gradio(self, prompt: str) -> str:
        logger.info(f"–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Gradio (–º–æ–¥–µ–ª—å: {self.model_id})...")
        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(None, self._predict_gradio, prompt)
            return result
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ –∫ Gradio: {e}")
            return f"–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI. (–û—à–∏–±–∫–∞ Gradio: {e})"
            
    # --- –ú–µ—Ç–æ–¥—ã –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ OpenAI ---

    def _predict_openai(self, prompt: str) -> str:
        logger.info(f"–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ OpenAI (–º–æ–¥–µ–ª—å: {self.model_id})...")
        try:
            completion = self.client.chat.completions.create(
                model=self.model_id,
                messages=[{"role": "user", "content": prompt}],
                **self.model_params
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ OpenAI: {e}")
            return f"–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI. (–û—à–∏–±–∫–∞ OpenAI: {e})"

    async def _predict_async_openai(self, prompt: str) -> str:
        logger.info(f"–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ OpenAI (–º–æ–¥–µ–ª—å: {self.model_id})...")
        try:
            completion = await self.async_client.chat.completions.create(
                model=self.model_id,
                messages=[{"role": "user", "content": prompt}],
                **self.model_params
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ –∫ OpenAI: {e}")
            return f"–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI. (–û—à–∏–±–∫–∞ OpenAI: {e})"

    # --- –î–∏—Å–ø–µ—Ç—á–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ---

    def _predict(self, prompt: str) -> str:
        if self.provider == 'gradio':
            return self._predict_gradio(prompt)
        elif self.provider == 'openai':
            return self._predict_openai(prompt)
        # –≠—Ç–æ—Ç return –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω —Å—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑-–∑–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ __init__
        return "–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä."

    async def _predict_async(self, prompt: str) -> str:
        if self.provider == 'gradio':
            return await self._predict_async_gradio(prompt)
        elif self.provider == 'openai':
            return await self._predict_async_openai(prompt)
        return "–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä."

    def explain_error(self, traceback: str) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–∫–∏ (traceback) —Å –ø–æ–º–æ—â—å—é AI.
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–µ—à –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∑–∞–ø—Ä–æ—Å–∞.
        """
        if traceback in self.cache:
            logger.info("–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∫–µ—à–µ.")
            return self.cache[traceback]

        prompt = self._build_prompt(traceback)
        explanation = self._predict(prompt)
        self.cache[traceback] = explanation # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–µ—à
        return explanation

    async def explain_error_async(self, traceback: str) -> str:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–∫–∏ (traceback) —Å –ø–æ–º–æ—â—å—é AI.
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–µ—à –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∑–∞–ø—Ä–æ—Å–∞.
        """
        if traceback in self.cache:
            logger.info("–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∫–µ—à–µ.")
            return self.cache[traceback]

        prompt = self._build_prompt(traceback)
        explanation = await self._predict_async(prompt)
        self.cache[traceback] = explanation # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–µ—à
        return explanation

    def explain_and_print(self, traceback: str):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –µ–≥–æ —Å –ø–æ–º–æ—â—å—é rich –∏ –≤—ã–≤–æ–¥–∏—Ç –≤ –∫–æ–Ω—Å–æ–ª—å.
        """
        console = Console()
        with console.status("[bold green]–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –æ—à–∏–±–∫—É —Å –ø–æ–º–æ—â—å—é AI...[/]", spinner="dots"):
            explanation_md = self.explain_error(traceback)
        
        console.print(Markdown(explanation_md, style="default"))

    async def explain_and_print_async(self, traceback: str):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏ –≤—ã–≤–æ–¥–∏—Ç –≤ –∫–æ–Ω—Å–æ–ª—å.
        """
        console = Console()
        with console.status("[bold green]–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –æ—à–∏–±–∫—É —Å –ø–æ–º–æ—â—å—é AI...[/]", spinner="dots"):
            explanation_md = await self.explain_error_async(traceback)
        
        console.print(Markdown(explanation_md, style="default"))