# ğŸ§  customrag

**customrag** is a customizable **Retrieval-Augmented Generation (RAG)** pipeline that supports **multiple LLMs and embedding models** via a simple YAML config. Itâ€™s built for developers who want a plug-and-play RAG setup that works across:

* âœ… OpenAI (ChatGPT, Embeddings)
* âœ… Gemini (Cloud Console SDK or Gemini Studio via LangChain)
* âœ… HuggingFace Hub
* âœ… xAI
* âœ… Local models via Sentence Transformers

---

## ğŸš€ Features

* ğŸ“† Easy pip install (`pip install customrag`)
* âš™ï¸ YAML-based config â€” switch providers anytime
* ğŸ”— Supports **multiple file formats** (`.txt`, `.pdf`, `.csv`, `.json`, `.docx`, `.md`)
* ğŸ“ Saves vectorstore using FAISS
* ğŸ§ Built-in SDK support for Gemini Cloud Console (not supported by LangChain)
* ğŸ› ï¸ LangChain-native support for OpenAI, xAI, Gemini Studio, and HuggingFace

---

## ğŸ‘… Installation

```bash
pip install customrag
```

---

## ğŸ› ï¸ One-Time Setup

Create a default config file in your project directory:

```bash
customrag-setup
```

This generates a `config.yaml` file with placeholders for your API keys and model settings.

---

## ğŸ“ Example `config.yaml`

```yaml
embedding:
  provider: gemini            # Options: gemini, openai, huggingface, sentence-transformers, xai, gemini_studio
  model: models/embedding-001 # Model for embeddings

llm:
  provider: gemini            # Options: gemini, gemini_studio, openai, huggingface, xai
  model: gemini-1.5-pro       # Chat model

api_keys:
  gemini: your_gemini_api_key_here
  gemini_studio: your_gemini_studio_api_key_here
  openai: your_openai_api_key_here
  huggingface: your_huggingface_token_here
  xai: your_xai_api_key_here
```

---

## ğŸ”§ Usage

### 1âƒ£ Initialize the Pipeline

```python
from customrag import RAGPipeline

pipeline = RAGPipeline(config_path="config.yaml")
```

---

### 2âƒ£ Build a Vectorstore from Documents

```python
pipeline.build_vectorstore("resume.pdf")  # Accepts .pdf, .txt, .docx, .md, .json, .csv
```

This will:

* Load and chunk your document
* Embed it using the configured embedding model
* Save the FAISS vectorstore locally

---

### 3âƒ£ Ask a Question

```python
answer = pipeline.query("What are my key skills?")
```

Depending on your config, it will:

* Retrieve top matching chunks using FAISS
* Generate an answer using either LangChain or Gemini SDK

---

## ğŸ¤– Supported Providers

| Provider                          | Embeddings âœ…               | Chat (LLM) âœ…           | Chat SDK Support |
| --------------------------------- | -------------------------- | ---------------------- | ---------------- |
| **OpenAI**                        | âœ… `text-embedding-ada-002` | âœ… `gpt-3.5 / gpt-4`    | âŒ                |
| **Gemini**                        | âœ… `models/embedding-001`   | âŒ *(SDK only)*         | âœ…                |
| **Gemini Studio**                 | âœ…                          | âœ… `gemini-pro`         | âŒ                |
| **HuggingFace**                   | âœ…                          | âœ… via `HuggingFaceHub` | âŒ                |
| **xAI**                           | âœ…                          | âœ… `Grok (xAI)`         | âŒ                |
| **Local (sentence-transformers)** | âœ…                          | âŒ                      | âŒ                |

---

## ğŸ“† Example Project Structure

```
your-project/
â”œâ”€â”€ config.yaml
â”œâ”€â”€ resume.pdf
â”œâ”€â”€ script.py
â””â”€â”€ faiss_index/
```

---

## ğŸ‘¨â€ğŸ’¼ CLI Tool

Run this once in your project root:

```bash
customrag-setup
```

It will create a `config.yaml` you can edit with your API keys and model names.

---

## ğŸ’ƒ Supported Document Formats

You can ingest files of type:

* ğŸ“„ `.txt`, `.pdf`, `.docx`, `.md`
* ğŸ“ˆ `.csv`
* ğŸ§¾ `.json` (array of objects)

---

## ğŸ§  How It Works

```mermaid
graph TD
    A[User Input] -->|Query| B[RAGPipeline]
    B --> C[FAISS Vectorstore]
    C --> D[Top-K Context]
    D --> E[LLM or Gemini SDK]
    E --> F[Answer Returned]
```

---

## ğŸ‘¨â€ğŸ’» Author

Made by [Anuj Goel](https://github.com/goelanuj371)

---

## ğŸ“¬ Contribute

Issues and PRs are welcome. Add support for more LLMs or improve CLI! ğŸš€

---

## ğŸ“„ License

MIT License â€“ free for personal and commercial use.
