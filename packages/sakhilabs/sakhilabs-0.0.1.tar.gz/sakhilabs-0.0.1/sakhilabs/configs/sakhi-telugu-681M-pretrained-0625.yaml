paths:
  tokenizer_path: ""
  dataset_path: ""
  save_dir: ""

model_parameters:
  embed_dim: 2048
  num_heads: 8
  ff_dim: 4096
  chunk_length: 1024
  num_layers: 10
  vocab_size: 64000

train_parameters:
  batch_size: 10
  num_epochs: 1
  init_learning_rate: 1e-5
  min_learning_rate: 1e-8
  seed: 42
  master_addr: "localhost"
  master_port: "12355"
  num_gpus: -1
  save_every_n_steps: 25000
  log_every_n_steps: 100
  gradient_clipping_max_norm: 3.0
  call_torch_compile_on_model: False
  gradient_accumulation_steps: 2
  resume: ""

logger:
  wandb: True

data_loader:
  num_workers: 0
  pin_memory: true

inference_parameters:
  max_new_tokens: 300
  model: ""
