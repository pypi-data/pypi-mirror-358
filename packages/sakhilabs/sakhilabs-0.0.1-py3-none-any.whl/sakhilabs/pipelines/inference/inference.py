# NOTE: Inference hyperparameters generated by Claude. Please verify for potential bugs

import torch
import torch.nn.functional as F
from tqdm import tqdm
from transformers import PreTrainedTokenizerFast

from sakhilabs.configs.utils.config import SakhiConfig
from sakhilabs.model.model import SakhiModel
from sakhilabs.pipelines.utils.training_utils import set_seed


def _load_model(model_path: str, model_config: SakhiConfig, device: str) -> SakhiModel:
    """Load the trained model from checkpoint with provided config."""
    model = SakhiModel(
        embed_dim=model_config.model_parameters.embed_dim,
        num_heads=model_config.model_parameters.num_heads,
        ff_dim=model_config.model_parameters.ff_dim,
        num_layers=model_config.model_parameters.num_layers,
        vocab_size=model_config.model_parameters.vocab_size,
    )
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint)
    model.to(device)
    model.eval()
    return model


def generate_text(
    model: SakhiModel,
    tokenizer: PreTrainedTokenizerFast,
    prompt: str,
    max_new_tokens: int,
    device: str,
    temperature: float = 0.4,
    top_k: int = 100,
    top_p: float = 0.90,
    repetition_penalty: float = 1.2,
    no_repeat_ngram_size: int = 4,
) -> str:
    """Generate text given a prompt with controlled sampling."""
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    generated = input_ids.clone()

    with torch.no_grad():
        for _ in tqdm(range(max_new_tokens), desc="Generating"):
            output = model(generated)
            logits = output[:, -1, :]

            # Repetition penalty
            for token_id in set(generated[0].tolist()):
                logits[0, token_id] /= repetition_penalty

            # Top-k filtering
            if top_k > 0:
                top_k_values, _ = torch.topk(logits, top_k)
                logits[logits < top_k_values[:, -1].unsqueeze(1)] = -float("Inf")

            # Top-p (nucleus) filtering
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(
                    F.softmax(sorted_logits, dim=-1), dim=-1
                )
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[
                    :, :-1
                ].clone()
                sorted_indices_to_remove[:, 0] = 0
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[0, indices_to_remove] = -float("Inf")

            # Sampling with temperature
            probs = F.softmax(logits / temperature, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            generated = torch.cat([generated, next_token], dim=1)

            # Stop if repeating n-grams
            if no_repeat_ngram_size > 0 and generated.shape[1] > no_repeat_ngram_size:
                last_ngram = generated[0, -no_repeat_ngram_size:].tolist()
                all_ngrams = [
                    generated[0, i : i + no_repeat_ngram_size].tolist()
                    for i in range(generated.shape[1] - no_repeat_ngram_size)
                ]
                if last_ngram in all_ngrams[:-1]:
                    break

    generated_text = tokenizer.decode(
        generated[0][input_ids.shape[1] :], skip_special_tokens=True
    )
    return generated_text


def main(config: SakhiConfig, prompt: str):
    set_seed(seed=config.train_parameters.seed)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    tokenizer = PreTrainedTokenizerFast.from_pretrained(config.paths.tokenizer_path)
    special_tokens_dict = {
        "additional_special_tokens": ["<|instruction|>", "<|response|>"]
    }
    tokenizer.add_special_tokens(special_tokens_dict)

    model = _load_model(
        model_path=config.inference_parameters.model,
        model_config=config,
        device=device,
    )
    max_new_tokens = config.inference_parameters.max_new_tokens

    generated_text = generate_text(
        model=model,
        tokenizer=tokenizer,
        prompt=prompt,
        max_new_tokens=max_new_tokens,
        device=device,
    )

    print(f"\n\n[Prompt]: {prompt}\n[Generated]: {generated_text}\n")
