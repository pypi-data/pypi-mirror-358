#!/usr/bin/env python
# -*- coding: utf-8 -*-
# aiosyslogd/web.py

"""
Web interface for aiosyslogd, providing a UI to browse and search syslog messages
stored in a SQLite database. This module uses Quart, an asynchronous web framework,
to serve the interface. It supports filtering by time, host, and full-text search
using SQLite's FTS5 virtual table.

Note: This web UI is designed for the SQLite backend only. If Meilisearch is
configured, the server will exit with an error message.
"""

from .config import load_config
from datetime import datetime, timedelta
from loguru import logger
from quart import Quart, render_template, request, abort, Response
from types import ModuleType
from typing import Any, Dict, List, Tuple
import aiosqlite
import asyncio
import glob
import os
import sqlite3
import sys
import time

uvloop: ModuleType | None = None
try:
    if sys.platform == "win32":
        import winloop as uvloop
    else:
        import uvloop
except ImportError:
    pass  # uvloop or winloop is an optional dependency for performance enhancement

# --- Global Configuration ---
CFG: Dict[str, Any] = load_config()
WEB_SERVER_CFG: Dict[str, Any] = CFG.get("web_server", {})
DEBUG: bool = WEB_SERVER_CFG.get("debug", False)

# --- Logger Configuration ---
# Configure the logger to match Quart's default format for consistency.
log_level = "DEBUG" if DEBUG else "INFO"
logger.remove()
logger.add(
    sys.stderr,
    format="[{time:YYYY-MM-DD HH:mm:ss ZZ}] [{process}] [{level}] {message}",
    level=log_level,
)

# --- Quart Application ---
app: Quart = Quart(__name__)
# Enable the 'do' extension for Jinja2 templates to support advanced templating.
app.jinja_env.add_extension("jinja2.ext.do")
# Replace Quart's default logger with loguru for consistent logging.
app.logger = logger  # type: ignore[assignment]


# --- Datetime Type Adapters for SQLite ---
def adapt_datetime_iso(val: datetime) -> str:
    """Adapt datetime.datetime to timezone-aware ISO 8601 string for SQLite."""
    return val.isoformat()


def convert_timestamp_iso(val: bytes) -> datetime:
    """Convert ISO 8601 string from SQLite back to a datetime.datetime object."""
    return datetime.fromisoformat(val.decode())


aiosqlite.register_adapter(datetime, adapt_datetime_iso)
aiosqlite.register_converter("TIMESTAMP", convert_timestamp_iso)


def check_backend() -> bool:
    """
    Checks if the configured database backend is SQLite.
    If Meilisearch is selected, logs a warning and returns False.
    """
    if CFG.get("database", {}).get("driver") != "sqlite":
        logger.info("Meilisearch backend is selected.")
        logger.warning(
            "This web UI is for the SQLite backend only. "
            "Please use the Meilisearch UI for searching logs."
        )
        return False
    return True


def get_available_databases() -> List[str]:
    """
    Finds all available monthly SQLite database files based on the configured
    database path pattern. Returns a list sorted in descending order (newest first).
    """
    db_template: str = (
        CFG.get("database", {})
        .get("sqlite", {})
        .get("database", "syslog.sqlite3")
    )
    base, ext = os.path.splitext(db_template)
    search_pattern: str = f"{base}_*{ext}"
    files: List[str] = glob.glob(search_pattern)
    files.sort(reverse=True)
    return files


async def get_time_boundary_ids(
    conn: aiosqlite.Connection, min_time_filter: str, max_time_filter: str
) -> Tuple[int | None, int | None, List[str]]:
    """
    Finds the starting and ending log IDs for a given time window using an
    efficient, iterative, chunk-based search to minimize query overhead.

    Args:
        conn: The SQLite database connection.
        min_time_filter: The minimum timestamp (ISO format or YYYY-MM-DD HH:MM).
        max_time_filter: The maximum timestamp (ISO format or YYYY-MM-DD HH:MM).

    Returns:
        A tuple containing (start_id, end_id, debug_queries).
        - start_id: The first ID in the time range, or None if no logs are found.
        - end_id: The last ID in the time range, or None if no logs are found.
        - debug_queries: A list of debug information for queries executed.
    """
    start_id: int | None = None
    end_id: int | None = None
    debug_queries: List[str] = []

    db_time_format = "%Y-%m-%d %H:%M:%S"
    chunk_sizes_minutes = [5, 15, 30, 60]

    def _parse_time_string(time_str: str) -> datetime:
        """Parses a time string, handling formats with or without seconds."""
        time_str = time_str.replace("T", " ")
        try:
            return datetime.strptime(time_str, db_time_format)
        except ValueError:
            return datetime.strptime(time_str, "%Y-%m-%d %H:%M")

    # --- Find Start ID ---
    if min_time_filter:
        start_debug_chunks = []
        total_start_time_ms = 0.0
        current_start_dt = _parse_time_string(min_time_filter)
        final_end_dt = (
            _parse_time_string(max_time_filter)
            if max_time_filter
            else datetime.now()
        )

        chunk_index = 0
        while start_id is None and current_start_dt < final_end_dt:
            minutes_to_add = chunk_sizes_minutes[
                min(chunk_index, len(chunk_sizes_minutes) - 1)
            ]
            chunk_end_dt = current_start_dt + timedelta(minutes=minutes_to_add)

            start_sql = "SELECT ID FROM SystemEvents WHERE ReceivedAt >= ? AND ReceivedAt < ? ORDER BY ID ASC LIMIT 1"
            start_params = (
                current_start_dt.strftime(db_time_format),
                chunk_end_dt.strftime(db_time_format),
            )

            start_time = time.perf_counter()
            async with conn.execute(start_sql, start_params) as cursor:
                row = await cursor.fetchone()
                start_id = row["ID"] if row else None
            elapsed_ms = (time.perf_counter() - start_time) * 1000
            total_start_time_ms += elapsed_ms

            start_debug_chunks.append(
                f"  - Chunk ({minutes_to_add}m): {start_params} -> Found: {start_id is not None} ({elapsed_ms:.2f}ms)"
            )
            current_start_dt = chunk_end_dt
            chunk_index += 1

        debug_queries.append(
            f"Boundary Query (Start):\n  Result ID: {start_id}\n  Total Time: {total_start_time_ms:.2f}ms\n"
            + "\n".join(start_debug_chunks)
        )

    # --- Find End ID ---
    if max_time_filter:
        end_debug_chunks = []
        total_end_time_ms = 0.0
        end_dt = _parse_time_string(max_time_filter)

        next_id_after_end = None
        current_search_dt = end_dt

        total_search_duration = timedelta(0)
        max_search_forward = timedelta(days=1)
        chunk_index = 0

        while (
            next_id_after_end is None
            and total_search_duration < max_search_forward
        ):
            minutes_to_add = chunk_sizes_minutes[
                min(chunk_index, len(chunk_sizes_minutes) - 1)
            ]
            chunk_duration = timedelta(minutes=minutes_to_add)
            chunk_end_dt = current_search_dt + chunk_duration

            end_boundary_sql = (
                "SELECT ID FROM SystemEvents WHERE ReceivedAt > ? AND ReceivedAt <= ? "
                "ORDER BY ID ASC LIMIT 1"
            )
            end_params = (
                current_search_dt.strftime(db_time_format),
                chunk_end_dt.strftime(db_time_format),
            )

            start_time = time.perf_counter()
            async with conn.execute(end_boundary_sql, end_params) as cursor:
                row = await cursor.fetchone()
                next_id_after_end = row["ID"] if row else None
            elapsed_ms = (time.perf_counter() - start_time) * 1000
            total_end_time_ms += elapsed_ms

            end_debug_chunks.append(
                f"  - Chunk ({minutes_to_add}m): {end_params} -> Found: {next_id_after_end is not None} ({elapsed_ms:.2f}ms)"
            )
            current_search_dt = chunk_end_dt
            total_search_duration += chunk_duration
            chunk_index += 1

        if next_id_after_end is not None:
            end_id = next_id_after_end - 1
        else:
            async with conn.execute(
                "SELECT ID FROM SystemEvents ORDER BY ID DESC LIMIT 1"
            ) as cursor:
                row = await cursor.fetchone()
                end_id = row["ID"] if row else None

        debug_queries.append(
            f"Boundary Query (End):\n  Result ID: {end_id}\n  Total Time: {total_end_time_ms:.2f}ms\n"
            + "\n".join(end_debug_chunks)
        )

    return start_id, end_id, debug_queries


def build_log_query(
    search_query: str,
    filters: Dict[str, Any],
    last_id: int | None,
    page_size: int,
    direction: str,
    start_id: int | None,
    end_id: int | None,
) -> Dict[str, Any]:
    """
    Builds SQL queries and parameters for retrieving logs with pagination and filtering.
    For cases without FTS or FromHost filters, uses ID difference to calculate total logs
    and optimizes the first page by querying directly from end_id.

    Args:
        search_query: The full-text search query for the FTS5 table.
        filters: Dictionary of additional filters (e.g., from_host).
        last_id: The ID of the last log viewed for pagination.
        page_size: Number of logs per page.
        direction: Pagination direction ("next" or "prev").
        start_id: The first ID in the time range.
        end_id: The last ID in the time range.

    Returns:
        A dictionary containing:
            - main_sql: The SQL query for retrieving logs.
            - main_params: Parameters for the main query.
            - count_sql: The SQL query for counting total logs (optional).
            - count_params: Parameters for the count query (optional).
            - total_logs: Total number of logs (if calculated via ID difference).
    """
    main_sql = "SELECT ID, FromHost, ReceivedAt, Message FROM SystemEvents"
    main_params = []
    count_params = []
    conditions = []

    # Special case: No FTS, no FromHost filter, and valid start_id/end_id
    use_id_diff = (
        not search_query
        and "from_host" not in filters
        and start_id is not None
        and end_id is not None
        and end_id >= start_id
    )

    if use_id_diff:
        # Calculate total logs using end_id - start_id
        total_logs = end_id - start_id + 1
        # Optimize first page (or near end_id)
        if last_id is None or (
            direction == "next" and last_id >= end_id - page_size
        ):
            main_sql += " WHERE ID >= ? AND ID <= ? ORDER BY ID DESC"
            main_params = [max(start_id, end_id - page_size), end_id]
            logger.info(f"Using ID difference: total_logs={total_logs}")
            return {
                "main_sql": main_sql,
                "main_params": main_params,
                "total_logs": total_logs,
            }
        else:
            # Other pages use the full range
            conditions.append("ID >= ? AND ID <= ?")
            main_params.extend([start_id, end_id])
            count_params.extend([start_id, end_id])
    else:
        # Standard case: FTS or FromHost filter
        if start_id is not None:
            conditions.append("ID >= ?")
            main_params.append(start_id)
            count_params.append(start_id)
        if end_id is not None:
            conditions.append("ID <= ?")
            main_params.append(end_id)
            count_params.append(end_id)
        if "from_host" in filters:
            conditions.append("FromHost = ?")
            main_params.append(filters["from_host"])
            count_params.append(filters["from_host"])
        if search_query:
            conditions.append(
                "ID IN (SELECT rowid FROM SystemEvents_FTS WHERE Message MATCH ? AND rowid >= ? AND rowid <= ?)"
            )
            subquery_params = [search_query, start_id or 0, end_id or 2**31 - 1]
            main_params.extend(subquery_params)
            count_params.extend(subquery_params)

    # Add conditions to main_sql
    if conditions:
        main_sql += " WHERE " + " AND ".join(conditions)

    # Pagination
    if last_id is not None:
        if direction == "next":
            conditions.append("ID < ?")
            main_params.append(last_id)
        else:  # prev
            conditions.append("ID > ?")
            main_params.append(last_id)
            main_sql += " ORDER BY ID ASC"
    else:
        main_sql += " ORDER BY ID DESC"

    main_sql += f" LIMIT {page_size + 1}"

    # Count query for standard case
    count_sql = "SELECT COUNT(*) FROM SystemEvents"
    if conditions:
        count_sql += " WHERE " + " AND ".join(conditions)

    return {
        "main_sql": main_sql,
        "main_params": main_params,
        "count_sql": count_sql,
        "count_params": count_params,
    }


@app.route("/")
async def index() -> str:
    """
    Main route for the web interface. Displays logs with support for filtering
    by time, host, and full-text search. Implements pagination and displays
    debug information if enabled.

    Query Parameters:
        - db: The SQLite database file to query.
        - q: Full-text search query.
        - min_time: Minimum timestamp for filtering (ISO or YYYY-MM-DD HH:MM).
        - max_time: Maximum timestamp for filtering (ISO or YYYY-MM-DD HH:MM).
        - from_host: Filter by log source host.
        - last_id: ID of the last log viewed for pagination.
        - direction: Pagination direction ("next" or "prev").

    Returns:
        Rendered HTML template with logs and pagination controls.
    """
    available_dbs = get_available_databases()
    if not available_dbs:
        return await render_template(
            "index.html",
            error="No SQLite database files found.",
            available_databases=[],
        )

    db_file = request.args.get("db", available_dbs[0])
    if db_file not in available_dbs:
        abort(400, "Invalid database file")

    search_query = request.args.get("q", "").strip()
    min_time_filter = request.args.get("min_time", "")
    max_time_filter = request.args.get("max_time", "")
    from_host = request.args.get("from_host", "").strip()
    last_id = request.args.get("last_id", type=int)
    direction = request.args.get("direction", "next")
    page_size = 50

    filters = {}
    if from_host:
        filters["from_host"] = from_host

    try:
        conn = await aiosqlite.connect(
            db_file,
            detect_types=sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,
        )
        conn.row_factory = aiosqlite.Row
        start_id, end_id, debug_queries = await get_time_boundary_ids(
            conn, min_time_filter, max_time_filter
        )
        query_data = build_log_query(
            search_query,
            filters,
            last_id,
            page_size,
            direction,
            start_id,
            end_id,
        )

        # Execute main query
        async with conn.execute(
            query_data["main_sql"], query_data["main_params"]
        ) as cursor:
            logs = await cursor.fetchall()

        # Get total logs (either from ID difference or COUNT query)
        total_logs = query_data.get("total_logs")
        if total_logs is None:
            async with conn.execute(
                query_data["count_sql"], query_data["count_params"]
            ) as cursor:
                total_logs = (await cursor.fetchone())[0]

        await conn.close()

        # Handle pagination
        has_next = len(logs) > page_size
        logs = logs[:page_size] if has_next else logs

        return await render_template(
            "index.html",
            logs=logs,
            total_logs=total_logs,
            has_next=has_next,
            has_prev=last_id is not None,
            last_id=logs[-1]["ID"] if logs else None,
            search_query=search_query,
            min_time=min_time_filter,
            max_time=max_time_filter,
            from_host=from_host,
            db_file=db_file,
            available_databases=available_dbs,
            debug_queries=(
                debug_queries if WEB_SERVER_CFG.get("debug", False) else []
            ),
        )
    except (sqlite3.Error, ValueError) as e:
        await conn.close()
        return await render_template(
            "index.html",
            error=f"Database error: {str(e)}",
            available_databases=available_dbs,
        )


def main() -> None:
    """Entry point for running the web server."""
    if not check_backend():
        sys.exit(0)

    if uvloop:
        uvloop.install()
    logger.info(
        f"Using {asyncio.get_event_loop().__module__} for the event loop."
    )
    app.run(
        host=WEB_SERVER_CFG.get("bind_ip", "127.0.0.1"),
        port=WEB_SERVER_CFG.get("bind_port", 5141),
        debug=DEBUG,
    )


if __name__ == "__main__":
    main()
