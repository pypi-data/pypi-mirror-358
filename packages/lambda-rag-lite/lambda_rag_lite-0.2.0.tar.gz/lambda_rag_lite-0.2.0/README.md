# Lambda RAG Lite

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Package CI](https://github.com/dmux/lambda-rag-lite/actions/workflows/ci.yml/badge.svg)](https://github.com/dmux/lambda-rag-lite/actions/workflows/ci.yml)

Uma biblioteca Python **ultra-leve** para RAG (Retrieval-Augmented Generation) **criada especificamente para AWS Lambda**, totalmente compatÃ­vel com **LangChain**, que **elimina dependÃªncias pesadas** como NumPy, TensorFlow ou qualquer biblioteca de ML.

> ğŸ¯ **Criada para AWS Lambda**: Esta biblioteca foi desenvolvida especificamente para resolver os problemas de tamanho de pacote e cold start em funÃ§Ãµes AWS Lambda, onde bibliotecas como NumPy podem aumentar significativamente o tempo de inicializaÃ§Ã£o e o tamanho do deployment.

## âœ¨ CaracterÃ­sticas

- ğŸ¦ **Feita para AWS Lambda**: Projetada para minimizar cold starts e tamanho de deployment
- ğŸª¶ **Ultra-leve**: **ZERO dependÃªncias de ML** (sem NumPy, scikit-learn, TensorFlow, PyTorch)
- âš¡ **Cold Start RÃ¡pido**: InicializaÃ§Ã£o em milissegundos vs. segundos com bibliotecas tradicionais
- ğŸ“¦ **Pacote Pequeno**: ~50KB vs. ~100MB+ de bibliotecas tradicionais de ML
- ğŸ”— **CompatÃ­vel com LangChain**: Implementa interfaces padrÃ£o (`VectorStore`, `Embeddings`)
- ğŸš€ **Puro Python**: Funciona em qualquer ambiente Python 3.10+ (Lambda, Container, local)
- ğŸ¯ **FÃ¡cil de usar**: API simples e intuitiva
- âœ… **Pronto para produÃ§Ã£o**: Inclui testes, documentaÃ§Ã£o e exemplos
- ğŸ” **Busca avanÃ§ada**: Suporte a MMR (Maximum Marginal Relevance) para diversificar resultados
- ğŸ“Š **FlexÃ­vel**: MÃºltiplos tipos de embedding e loaders

## ğŸš€ InÃ­cio RÃ¡pido

### Por que Lambda RAG Lite?

**Problema**: Bibliotecas tradicionais de RAG (como LangChain com embeddings de ML) incluem dependÃªncias massivas:

- NumPy: ~15-20MB
- scikit-learn: ~30-40MB
- Transformers: ~500MB+
- Torch: ~800MB+

**Resultado em AWS Lambda**:

- âŒ Cold starts de 10-30 segundos
- âŒ Pacotes de deployment de 100MB-1GB+
- âŒ Limites de tamanho excedidos
- âŒ Custos elevados de armazenamento

**SoluÃ§Ã£o Lambda RAG Lite**:

- âœ… Cold starts < 1 segundo
- âœ… Pacote de deployment < 1MB
- âœ… Zero dependÃªncias externas
- âœ… Custo mÃ­nimo de armazenamento

### InstalaÃ§Ã£o

```bash
pip install lambda-rag-lite
```

### Exemplo BÃ¡sico

```python
from lambda_rag_lite import SimpleHashEmbedding, PurePythonVectorStore

# Criar embedding e vector store
embedder = SimpleHashEmbedding(dim=256)
vector_store = PurePythonVectorStore(embedder)

# Adicionar textos
texts = [
    "Python Ã© uma linguagem de programaÃ§Ã£o versÃ¡til.",
    "Machine learning Ã© uma Ã¡rea da IA.",
    "RAG combina recuperaÃ§Ã£o com geraÃ§Ã£o de texto."
]

vector_store.add_texts(texts)

# Buscar documentos similares
results = vector_store.similarity_search("O que Ã© Python?", k=2)

for doc in results:
    print(doc.page_content)
```

### Carregando Documentos

```python
from lambda_rag_lite import MarkdownLoader, PurePythonVectorStore, SimpleHashEmbedding

# Carregar documentos Markdown
loader = MarkdownLoader("docs/")
documents = loader.load()

# Criar vector store
embedder = SimpleHashEmbedding(dim=512)
vector_store = PurePythonVectorStore.from_documents(documents, embedder)

# Buscar com scores
results = vector_store.similarity_search_with_score("Como usar?", k=3)

for doc, score in results:
    print(f"Score: {score:.3f} - {doc.page_content[:100]}...")
```

## ğŸ“– DocumentaÃ§Ã£o Completa

### Embeddings

#### SimpleHashEmbedding

Converte texto em vetores usando hashing SHA-1. Ideal para casos onde vocÃª precisa de embeddings rÃ¡pidos sem modelos de ML.

```python
from lambda_rag_lite import SimpleHashEmbedding

embedder = SimpleHashEmbedding(dim=256)

# Embed uma consulta
query_vector = embedder.embed_query("exemplo de texto")

# Embed mÃºltiplos documentos
doc_vectors = embedder.embed_documents([
    "primeiro documento",
    "segundo documento"
])
```

#### TFIDFHashEmbedding

VersÃ£o avanÃ§ada que incorpora TF-IDF para melhor qualidade dos embeddings.

```python
from lambda_rag_lite.embeddings import TFIDFHashEmbedding

embedder = TFIDFHashEmbedding(dim=512)

# Treina com documentos (coleta estatÃ­sticas)
embeddings = embedder.embed_documents(training_texts)

# Usa para consultas
query_embedding = embedder.embed_query("minha consulta")
```

### Vector Store

#### PurePythonVectorStore

ImplementaÃ§Ã£o completa da interface LangChain VectorStore.

```python
from lambda_rag_lite import PurePythonVectorStore, SimpleHashEmbedding

embedder = SimpleHashEmbedding()
store = PurePythonVectorStore(embedder)

# MÃ©todos principais
store.add_texts(texts, metadatas)
store.similarity_search(query, k=4)
store.similarity_search_with_score(query, k=4)
store.max_marginal_relevance_search(query, k=4)
store.delete(ids)
store.get_by_ids(ids)
```

### Loaders

#### MarkdownLoader

Carrega arquivos Markdown recursivamente.

```python
from lambda_rag_lite import MarkdownLoader

# Carrega um arquivo
loader = MarkdownLoader("documento.md")
docs = loader.load()

# Carrega diretÃ³rio inteiro
loader = MarkdownLoader("docs/", auto_detect_encoding=True)
docs = loader.load()
```

#### TextLoader

Loader genÃ©rico para diferentes tipos de arquivo de texto.

```python
from lambda_rag_lite.loaders import TextLoader

loader = TextLoader(
    "src/",
    extensions=['.py', '.js', '.md', '.txt']
)
docs = loader.load()
```

#### DirectoryLoader

Loader automÃ¡tico que detecta tipos de arquivo.

```python
from lambda_rag_lite.loaders import DirectoryLoader

loader = DirectoryLoader("projeto/", recursive=True)
docs = loader.load()  # Carrega automaticamente .md, .py, .txt, etc.
```

### UtilitÃ¡rios

#### Processamento de Texto

```python
from lambda_rag_lite.utils import chunk_text, clean_text, TextProcessor

# Dividir texto em chunks
chunks = chunk_text(
    text="texto longo aqui...",
    chunk_size=1000,
    chunk_overlap=200
)

# Limpar texto
clean = clean_text(text, remove_extra_whitespace=True)

# Processador avanÃ§ado
processor = TextProcessor(
    chunk_size=500,
    chunk_overlap=100,
    clean_text=True,
    extract_keywords=True
)

processed = processor.process_text(text, metadata={"source": "doc1"})
```

## ï¿½ Deploy em AWS Lambda

### Exemplo de FunÃ§Ã£o Lambda

```python
import json
from lambda_rag_lite import SimpleHashEmbedding, PurePythonVectorStore

# InicializaÃ§Ã£o global (fora do handler para reutilizar entre invocaÃ§Ãµes)
embedder = SimpleHashEmbedding(dim=256)
vector_store = None

def lambda_handler(event, context):
    global vector_store

    # Inicializar vector store na primeira execuÃ§Ã£o
    if vector_store is None:
        # Carregar seus documentos aqui
        documents = load_your_documents()  # Implementar conforme necessÃ¡rio
        vector_store = PurePythonVectorStore.from_texts(documents, embedder)

    # Processar consulta
    query = event.get('query', '')
    results = vector_store.similarity_search(query, k=3)

    return {
        'statusCode': 200,
        'body': json.dumps({
            'results': [{'content': doc.page_content, 'metadata': doc.metadata}
                       for doc in results]
        })
    }

def load_your_documents():
    # Exemplo: carregar de S3, DynamoDB, ou incluir no pacote
    return [
        "Documento 1: ConteÃºdo sobre Python e AWS Lambda",
        "Documento 2: Guia de RAG sem NumPy",
        "Documento 3: Performance em serverless"
    ]
```

### Deployment com AWS SAM

```yaml
# template.yaml
AWSTemplateFormatVersion: "2010-09-09"
Transform: AWS::Serverless-2016-10-31

Resources:
  RagFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: src/
      Handler: app.lambda_handler
      Runtime: python3.11
      MemorySize: 512 # Muito menos que libs tradicionais!
      Timeout: 30
      Environment:
        Variables:
          EMBEDDING_DIM: 256
```

### Deployment com Terraform

```hcl
resource "aws_lambda_function" "rag_function" {
  filename         = "lambda-rag-lite.zip"
  function_name    = "rag-search"
  role            = aws_iam_role.lambda_role.arn
  handler         = "app.lambda_handler"
  runtime         = "python3.11"
  memory_size     = 512
  timeout         = 30

  # Pacote serÃ¡ < 1MB vs 100MB+ com bibliotecas tradicionais
}
```

## ï¿½ğŸ”§ IntegraÃ§Ã£o com LangChain

Lambda RAG Lite implementa as interfaces padrÃ£o do LangChain, entÃ£o vocÃª pode usar diretamente com outros componentes:

```python
from langchain.chains import RetrievalQA
from lambda_rag_lite import PurePythonVectorStore, SimpleHashEmbedding

# Criar retriever
embedder = SimpleHashEmbedding(dim=256)
vector_store = PurePythonVectorStore.from_texts(texts, embedder)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# Usar com LangChain Chain (exemplo conceitual)
# qa_chain = RetrievalQA.from_chain_type(
#     llm=your_llm,
#     retriever=retriever,
#     return_source_documents=True
# )

# result = qa_chain({"query": "Sua pergunta aqui"})
```

## âš¡ Performance

Lambda RAG Lite Ã© otimizada especificamente para **AWS Lambda**:

### Performance em Lambda

| MÃ©trica                 | Lambda RAG Lite | Libs Tradicionais (NumPy/ML) |
| ----------------------- | --------------- | ---------------------------- |
| **Cold Start**          | < 1s            | 10-30s                       |
| **Tamanho do Pacote**   | < 1MB           | 100MB-1GB+                   |
| **Tempo de Deploy**     | < 30s           | 5-15min                      |
| **MemÃ³ria Inicial**     | ~50MB           | 200MB-1GB+                   |
| **Tempo de ImportaÃ§Ã£o** | ~50ms           | 2-10s                        |

### Performance Operacional

| OperaÃ§Ã£o           | Tempo (1000 docs) | MemÃ³ria     |
| ------------------ | ----------------- | ----------- |
| IndexaÃ§Ã£o          | ~0.5s             | ~50MB       |
| Busca (k=5)        | ~5ms              | ~50MB       |
| AdiÃ§Ã£o incremental | ~0.5ms/doc        | +0.05MB/doc |

### Casos de Uso Ideais para AWS Lambda

- âœ… **APIs de busca semÃ¢ntica** com baixa latÃªncia
- âœ… **Chatbots serverless** com RAG
- âœ… **Processamento de documentos** em lote
- âœ… **MicroserviÃ§os de recomendaÃ§Ã£o**
- âœ… **AnÃ¡lise de texto** sob demanda

## ğŸ› ï¸ Desenvolvimento

### ConfiguraÃ§Ã£o do Ambiente

```bash
# Clonar repositÃ³rio
git clone https://github.com/yourusername/lambda-rag-lite.git
cd lambda-rag-lite

# Instalar dependÃªncias de desenvolvimento
pip install -e ".[dev]"

# Executar testes
pytest

# Executar linting
black .
isort .
ruff check .
```

### Executar Exemplos

```bash
# Exemplo bÃ¡sico
python examples/basic_usage.py

# IntegraÃ§Ã£o com LangChain
python examples/langchain_integration.py

# Exemplo interativo (requer pasta docs_markdown/)
python example.py
```

## ï¿½ Quando Usar Lambda RAG Lite

### âœ… Use Lambda RAG Lite quando

- VocÃª estÃ¡ deployando em **AWS Lambda** ou outros ambientes serverless
- Precisa de **cold starts rÃ¡pidos** (< 1 segundo)
- Tem **limitaÃ§Ãµes de tamanho de pacote** (< 250MB no Lambda)
- Quer evitar **custos de dependÃªncias pesadas**
- Precisa de uma soluÃ§Ã£o **simples e eficiente** para busca semÃ¢ntica
- EstÃ¡ construindo **APIs de baixa latÃªncia**
- Quer **compatibilidade com LangChain** sem overhead

### âŒ NÃƒO use Lambda RAG Lite quando

- Precisa da **mÃ¡xima qualidade de embeddings** (use OpenAI, Cohere, etc.)
- EstÃ¡ deployando em **servidores dedicados** com recursos abundantes
- Precisa de **algoritmos de ML avanÃ§ados** (clustering, dimensionality reduction)
- Tem **datasets muito grandes** (>100k documentos) que precisam de otimizaÃ§Ãµes especiais
- Precisa de **embeddings prÃ©-treinados** especÃ­ficos do domÃ­nio

### ğŸ”€ Alternativas e Quando UsÃ¡-las

| Biblioteca                   | Melhor Para                 | Tamanho | Cold Start |
| ---------------------------- | --------------------------- | ------- | ---------- |
| **Lambda RAG Lite**          | AWS Lambda, Serverless      | < 1MB   | < 1s       |
| **LangChain + OpenAI**       | MÃ¡xima qualidade            | ~10MB   | ~2s        |
| **LangChain + Transformers** | Embeddings locais avanÃ§ados | ~500MB  | ~15s       |
| **Pinecone/Weaviate**        | ProduÃ§Ã£o em larga escala    | Cliente | ~1s        |
| **FAISS + NumPy**            | Servidores dedicados        | ~100MB  | ~5s        |

## ï¿½ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

### Executar Testes

```bash
# Todos os testes
pytest

# Com cobertura
pytest --cov=lambda_rag_lite --cov-report=html

# Testes especÃ­ficos
pytest tests/test_embeddings.py -v
```

## ğŸ“‹ Roadmap

- [ ] Mais algoritmos de embedding (BM25, LSH)
- [ ] Suporte a diferentes mÃ©tricas de distÃ¢ncia
- [ ] OtimizaÃ§Ãµes de performance
- [ ] IntegraÃ§Ã£o com mais formatos de documento
- [ ] Suporte a Ã­ndices persistentes
- [ ] ImplementaÃ§Ã£o de quantizaÃ§Ã£o para reduzir memÃ³ria

## âœ¨ Por que Lambda RAG Lite Existe?

Esta biblioteca foi criada para resolver um problema especÃ­fico: **implementar RAG em AWS Lambda sem sofrer com cold starts lentos e pacotes gigantes**.

### O Problema Real

Quando tentamos usar bibliotecas tradicionais de RAG/ML em Lambda:

```python
# âŒ Bibliotecas tradicionais
import numpy as np           # +15MB
import pandas as pd          # +20MB
import sklearn               # +30MB
from sentence_transformers import SentenceTransformer  # +500MB
```

**Resultado**: Pacote de 500MB+, cold start de 15-30 segundos, custos elevados.

### Nossa SoluÃ§Ã£o

```python
# âœ… Lambda RAG Lite
from lambda_rag_lite import SimpleHashEmbedding, PurePythonVectorStore
```

**Resultado**: Pacote de <1MB, cold start de <1 segundo, custo mÃ­nimo.

### Filosofia de Design

1. **Zero dependÃªncias externas** - Apenas Python puro
2. **Compatibilidade total com LangChain** - Drop-in replacement
3. **Performance otimizada para Lambda** - Minimizar cold start
4. **Simplicidade** - API fÃ¡cil de usar
5. **Qualidade suficiente** - Boa para 80% dos casos de uso

---

ğŸ’¡ **Dica**: Se vocÃª precisa da mÃ¡xima qualidade de embeddings, use OpenAI/Cohere APIs. Se vocÃª precisa de deployment rÃ¡pido e barato em Lambda, use Lambda RAG Lite.

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ™ Agradecimentos

- [LangChain](https://github.com/langchain-ai/langchain) pela excelente arquitetura
- Comunidade Python pela inspiraÃ§Ã£o
- Todos os contribuidores que ajudaram a melhorar este projeto

## ğŸ“ Suporte

- ğŸ“§ Email: <rafael.sales@gmail.com>
- ğŸ› Issues: [GitHub Issues](https://github.com/dmux/lambda-rag-lite/issues)
- ğŸ’¬ DiscussÃµes: [GitHub Discussions](https://github.com/dmux/lambda-rag-lite/discussions)

---

â­ **Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela no GitHub!**
