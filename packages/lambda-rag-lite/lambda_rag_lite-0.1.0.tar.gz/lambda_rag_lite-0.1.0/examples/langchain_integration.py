"""
Exemplo de integra√ß√£o com LangChain para cria√ß√£o de um sistema RAG completo.

Este exemplo mostra como usar Lambda RAG Lite com outros componentes
LangChain para criar um pipeline RAG funcional.
"""

from lambda_rag_lite import PurePythonVectorStore, SimpleHashEmbedding
from lambda_rag_lite.utils import TextProcessor


def create_knowledge_base():
    """Cria uma base de conhecimento de exemplo."""

    # Documentos de exemplo sobre programa√ß√£o
    documents = {
        "python_basics.md": """
# Python B√°sico

Python √© uma linguagem de programa√ß√£o de alto n√≠vel, interpretada e de prop√≥sito geral.

## Caracter√≠sticas principais:
- Sintaxe simples e leg√≠vel
- Tipagem din√¢mica
- Orientada a objetos
- Grande comunidade e biblioteca padr√£o rica

## Exemplo de c√≥digo:
```python
def hello_world():
    print("Hello, World!")
    
hello_world()
```
        """,
        "machine_learning.md": """
# Machine Learning

Machine Learning √© um subcampo da intelig√™ncia artificial que permite que computadores aprendam sem serem explicitamente programados.

## Tipos de ML:
1. **Aprendizado Supervisionado**: Usa dados rotulados
2. **Aprendizado N√£o Supervisionado**: Encontra padr√µes em dados n√£o rotulados
3. **Aprendizado por Refor√ßo**: Aprende atrav√©s de recompensas e puni√ß√µes

## Aplica√ß√µes:
- Reconhecimento de imagem
- Processamento de linguagem natural
- Sistemas de recomenda√ß√£o
- An√°lise preditiva
        """,
        "langchain_intro.md": """
# LangChain

LangChain √© um framework para desenvolver aplica√ß√µes powered por modelos de linguagem grandes (LLMs).

## Componentes principais:
- **Models**: Interface para diferentes LLMs
- **Prompts**: Templates para formata√ß√£o de prompts
- **Chains**: Combina√ß√£o de componentes para workflows
- **Memory**: Persist√™ncia de estado entre intera√ß√µes
- **Agents**: Sistemas que podem usar ferramentas

## RAG com LangChain:
RAG (Retrieval-Augmented Generation) combina recupera√ß√£o de informa√ß√µes com gera√ß√£o de texto.
O processo t√≠pico inclui:
1. Indexar documentos em um vector store
2. Recuperar documentos relevantes para uma consulta
3. Usar documentos como contexto para gera√ß√£o
        """,
    }

    return documents


def main():
    print("=== Sistema RAG com Lambda RAG Lite ===\n")

    # 1. Criar base de conhecimento
    knowledge_base = create_knowledge_base()

    # 2. Configurar processador de texto
    processor = TextProcessor(
        chunk_size=500, chunk_overlap=100, clean_text=True, extract_keywords=True
    )

    # 3. Criar embedding
    embedder = SimpleHashEmbedding(dim=512)  # Dimens√£o maior para melhor qualidade

    # 4. Processar documentos
    all_chunks = []
    all_metadatas = []

    for filename, content in knowledge_base.items():
        # Processa texto em chunks
        processed_chunks = processor.process_text(
            content, metadata={"source": filename, "document_type": "markdown"}
        )

        for chunk_data in processed_chunks:
            all_chunks.append(chunk_data["text"])
            all_metadatas.append(chunk_data["metadata"])

    print(
        f"üìö Processados {len(all_chunks)} chunks de {len(knowledge_base)} documentos"
    )

    # 5. Criar vector store
    vector_store = PurePythonVectorStore.from_texts(
        texts=all_chunks, embedding=embedder, metadatas=all_metadatas
    )

    print(f"‚úÖ Vector store criado com {len(vector_store)} documentos\n")

    # 6. Fun√ß√£o de RAG simples
    def rag_query(question: str, k: int = 3):
        """Executa consulta RAG."""

        # Recupera documentos relevantes
        relevant_docs = vector_store.similarity_search_with_score(question, k=k)

        if not relevant_docs:
            return "N√£o encontrei informa√ß√µes relevantes para sua pergunta."

        # Monta contexto
        context_parts = []
        for i, (doc, score) in enumerate(relevant_docs, 1):
            source = doc.metadata.get("source", "Unknown")
            context_parts.append(
                f"[Fonte {i}: {source}, Score: {score:.3f}]\n{doc.page_content}"
            )

        context = "\n\n".join(context_parts)

        # Simula resposta baseada no contexto (sem LLM real)
        response = f"""
Baseado nos documentos encontrados, aqui est√£o as informa√ß√µes relevantes:

CONTEXTO RECUPERADO:
{context}

RESPOSTA:
Com base nos documentos acima, posso fornecer informa√ß√µes relevantes sobre sua pergunta: "{question}"

Para uma resposta mais elaborada, voc√™ poderia integrar este contexto com um LLM como GPT, Claude, ou qualquer outro modelo compat√≠vel com LangChain.
        """

        return response

    # 7. Exemplos de consultas
    queries = [
        "O que √© Python e quais s√£o suas caracter√≠sticas?",
        "Como funciona o machine learning supervisionado?",
        "O que √© RAG e como implementar com LangChain?",
        "Quais s√£o os tipos de aprendizado de m√°quina?",
    ]

    for query in queries:
        print(f"üîç PERGUNTA: {query}")
        print("=" * 60)

        response = rag_query(query)

        # Mostra apenas os documentos relevantes para economizar espa√ßo
        relevant_docs = vector_store.similarity_search_with_score(query, k=2)

        print("DOCUMENTOS RELEVANTES:")
        for i, (doc, score) in enumerate(relevant_docs, 1):
            source = doc.metadata.get("source", "Unknown")
            chunk_idx = doc.metadata.get("chunk_index", 0)
            keywords = doc.metadata.get("keywords", [])

            print(f"\n{i}. Fonte: {source} (chunk {chunk_idx})")
            print(f"   Score: {score:.3f}")
            print(f"   Keywords: {', '.join(keywords[:5])}")
            print(f"   Conte√∫do: {doc.page_content[:200]}...")

        print("\n" + "=" * 60 + "\n")

    # 8. Demonstra Maximum Marginal Relevance
    print("üéØ DEMONSTRA√á√ÉO MMR (Maximum Marginal Relevance)")
    print("=" * 60)

    query = "linguagem programa√ß√£o"

    print(f"Consulta: {query}\n")

    # Busca normal
    normal_results = vector_store.similarity_search_with_score(query, k=4)
    print("BUSCA NORMAL (por similaridade):")
    for i, (doc, score) in enumerate(normal_results, 1):
        source = doc.metadata.get("source", "Unknown")
        print(f"{i}. {source} - Score: {score:.3f}")
        print(f"   {doc.page_content[:100]}...")

    print()

    # Busca MMR
    mmr_results = vector_store.max_marginal_relevance_search(query, k=4, fetch_k=8)
    print("BUSCA MMR (diversificada):")
    for i, doc in enumerate(mmr_results, 1):
        source = doc.metadata.get("source", "Unknown")
        print(f"{i}. {source}")
        print(f"   {doc.page_content[:100]}...")

    print("\n" + "=" * 60)

    # 9. Estat√≠sticas finais
    print("üìä ESTAT√çSTICAS DO SISTEMA")
    print("=" * 30)
    print(f"Total de documentos originais: {len(knowledge_base)}")
    print(f"Total de chunks processados: {len(vector_store)}")
    print(f"Dimens√£o dos embeddings: {embedder.dim}")

    # Analisa distribui√ß√£o de chunks por fonte
    source_count = {}
    for metadata in all_metadatas:
        source = metadata.get("source", "Unknown")
        source_count[source] = source_count.get(source, 0) + 1

    print("\nDistribui√ß√£o de chunks por fonte:")
    for source, count in source_count.items():
        print(f"  {source}: {count} chunks")

    print("\n‚úÖ Demonstra√ß√£o conclu√≠da!")
    print("\nPara integrar com um LLM real, voc√™ pode:")
    print("1. Usar langchain.llms com OpenAI, Anthropic, etc.")
    print("2. Criar um prompt template que inclua o contexto recuperado")
    print("3. Implementar chains mais sofisticadas para diferentes tipos de consulta")


if __name__ == "__main__":
    main()
