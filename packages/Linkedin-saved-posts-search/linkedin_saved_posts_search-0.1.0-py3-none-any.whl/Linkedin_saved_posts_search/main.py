# important note : this current script, doesn't suppress the warnings!

import os
import warnings
import sys

# Suppress TensorFlow / absl and other noise
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # 0 = all, 3 = fatal only

# Suppress warnings module output (if any)
warnings.filterwarnings("ignore")


import json
import re
import time
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from rapidfuzz import fuzz

from subprocess import DEVNULL

# --- Setup Chrome not globally (after user chooses search mode)---

driver = None  # Declare globally but donâ€™t initialize

def create_headless_driver_with_cookies(cookies):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")

    service = Service(ChromeDriverManager().install())
    service.creationflags = 0x08000000
    service.log_file = open(os.devnull, "w")

    headless_driver = webdriver.Chrome(service=service, options=options)
    headless_driver.get("https://www.linkedin.com")  # Initial load

    for cookie in cookies:
        if 'sameSite' in cookie and cookie['sameSite'] == 'None':
            cookie['sameSite'] = 'Strict'
        try:
            headless_driver.add_cookie(cookie)
        except Exception:
            pass

    headless_driver.get("https://www.linkedin.com/my-items/saved-posts/")
    return headless_driver


def wait_for_user_login():
    global driver
    print("ğŸš€ Opening LinkedIn login page...")

    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")

    service = Service(ChromeDriverManager().install())
    service.creationflags = 0x08000000
    service.log_file = open(os.devnull, "w")

    driver = webdriver.Chrome(service=service, options=options)
    driver.get("https://www.linkedin.com/login")

    print("\nğŸ” Please log in manually in the Chrome window.")
    input("â³ After logging in successfully, press Enter here to continue...")

    cookies = driver.get_cookies()
    return cookies



def convert_relative_time_to_timestamp(relative_time_str):
    now = datetime.now()
    amount_match = re.search(r"\d+", relative_time_str)
    if not amount_match:
        return now.strftime("%Y-%m-%d")
    amount = int(amount_match.group())
    if "h" in relative_time_str:
        return (now - timedelta(hours=amount)).strftime("%Y-%m-%d")
    elif "d" in relative_time_str:
        return (now - timedelta(days=amount)).strftime("%Y-%m-%d")
    elif "w" in relative_time_str:
        return (now - timedelta(weeks=amount)).strftime("%Y-%m-%d")
    elif "mo" in relative_time_str:
        return (now - timedelta(days=30 * amount)).strftime("%Y-%m-%d")
    elif "y" in relative_time_str:
        return (now - timedelta(days=365 * amount)).strftime("%Y-%m-%d")
    return now.strftime("%Y-%m-%d")

def extract_original_author_from_time_tag(text):
    match = re.search(r"reposted from\s+(.*?)\s+[\u2022\u00B7\.]", text, re.IGNORECASE)
    return match.group(1).strip() if match else ""



def fuzzy_match(text, keyword, threshold=70):
    return fuzz.partial_ratio(text.lower(), keyword.lower()) >= threshold

def search_saved_posts():

    try:
        with open("saved_posts.json", "r", encoding="utf-8") as f:
            posts = json.load(f)
            if not posts:
                print("âŒ No saved posts found in the file. Please scrape first.")
                return
    except FileNotFoundError:
        print("âŒ No saved posts found. Please run scraping first.")
        return

    print("\nğŸ” Enter keywords one by one. Type '0' when done.")
    keywords = []
    while True:
        kw = input("Keyword: ").strip()
        if kw == "0":
            break
        if kw:
            keywords.append(kw)

    if not keywords:
        print("âš ï¸ No keywords entered. Exiting search.")
        return

    print("\nChoose match type:")
    print("1 - Match ANY keyword (OR search)")
    print("2 - Match ALL keywords (AND search)")
    match_mode = input("Enter 1 or 2: ").strip()

    print("\nğŸ—“ï¸ Do you want to apply a date filter? (y/n): ")
    use_date_filter = input().strip().lower() == "y"

    start_date = end_date = None
    if use_date_filter:
        start_input = input("Start date (YYYY-MM-DD) or press Enter to skip: ").strip()
        end_input = input("End date (YYYY-MM-DD) or press Enter to skip: ").strip()

        try:
            if start_input:
                start_date = datetime.strptime(start_input, "%Y-%m-%d")
            if end_input:
                end_date = datetime.strptime(end_input, "%Y-%m-%d")
        except ValueError:
            print("âŒ Invalid date format. Skipping date filtering.")

    # try:
    #     with open("saved_posts.json", "r", encoding="utf-8") as f:
    #         posts = json.load(f)
    # except FileNotFoundError:
    #     print("âŒ No saved posts found.")
    #     return

    print("\nğŸ” Searching...\n")
    matched_posts = []
    for post in posts:
        
        # content = (post.get("text", "") + " " + post.get("original_text", ""))
        content = " ".join([
            post.get("text", ""),
            post.get("original_text", ""),
            post.get("author", ""),
            post.get("original_author", "")
        ])


        if match_mode == "1":
            matched = any(fuzzy_match(content, kw) for kw in keywords)
        else:
            matched = all(fuzzy_match(content, kw) for kw in keywords)

        if not matched:
            continue

        # Time filter
        if use_date_filter:
            try:
                post_time = datetime.strptime(post["time"], "%Y-%m-%d")
                if (start_date and post_time < start_date) or (end_date and post_time > end_date):
                    continue
            except Exception:
                pass  # In case post["time"] is malformed

        matched_posts.append(post)

    # if not matched_posts:
    #     print("âŒ No matching posts found.")
    # else:
    #     print(f"âœ… Found {len(matched_posts)} matching post(s):\n")
    #     for i, post in enumerate(matched_posts, 1):
    #         print(f"{i}. {post['time']} | {post['author']} â€” {post['url']}")
    if not matched_posts:
        print("âŒ No matching posts found.")
        return

    print(f"âœ… Found {len(matched_posts)} matching post(s):\n")
    for i, post in enumerate(matched_posts, 1):
        print(f"{i}. {post['time']} | {post['author']} â€” {post['url']}")

    # âœ… Ask if user wants to export
    export_choice = input("\nğŸ’¾ Do you want to export these results to a file? (y/n): ").strip().lower()
    if export_choice == "y":
        export_filename = input("ğŸ“ Enter filename (e.g., matched_results.json): ").strip()
        if not export_filename.endswith(".json"):
            export_filename += ".json"
        with open(export_filename, "w", encoding="utf-8") as f:
            json.dump(matched_posts, f, indent=2, ensure_ascii=False)
        print(f"âœ… Results exported to '{export_filename}'")



def fetch_saved_posts():
    start_time = time.time()

    SAVED_POSTS_URL = "https://www.linkedin.com/my-items/saved-posts/"
    driver.get(SAVED_POSTS_URL)

    try:
        with open("saved_urls.json", "r", encoding="utf-8") as f:
            previously_saved_urls = set(json.load(f))
    except FileNotFoundError:
        previously_saved_urls = set()

    try:
        with open("saved_posts.json", "r", encoding="utf-8") as f:
            all_posts = json.load(f)
    except FileNotFoundError:
        all_posts = []

    print("\nChoose scraping mode:")
    print("1 - Scroll-based (default)")
    print("2 - Cutoff date (stop after threshold of old posts)")
    mode = input("Enter 1 or 2: ").strip()

    cutoff_date = None
    max_old_hits = 5
    scroll_count = 10
    if mode == "2":
        while True:
            user_input = input("ğŸ“… Enter cutoff date (YYYY-MM-DD): ").strip()
            try:
                cutoff_date = datetime.strptime(user_input, "%Y-%m-%d")
                break
            except ValueError:
                print("âŒ Invalid format. Please try again.")
        max_old_hits = int(input("ğŸ” How many old posts to tolerate before stopping? (default: 5): ") or "5")
    else:
        scroll_count = int(input("ğŸ” How many times should we scroll to load your saved posts? (e.g., 10): "))

    new_urls = set()
    new_posts = []
    skipped_from_current_run = 0
    skipped_from_previous_runs = 0

    print("\nğŸ“¥ Loading saved posts...")

    old_hit_count = 0
    scrolls_done = 0

    max_cutoff_scrolls = 300  # Prevent infinite scroll loops for very old cutoff dates

    while (mode == "1" and scrolls_done < scroll_count) or (mode == "2" and old_hit_count <= max_old_hits):
        # driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        # time.sleep(2)
        # scrolls_done += 1
        # print(f"ğŸ”„ Scroll {scrolls_done}")

        #debug
        # ğŸ§  Track how many <li> were already on the page
        previous_soup = BeautifulSoup(driver.page_source, "html.parser")
        previous_li_count = len(previous_soup.find_all("li"))

        # ğŸ”½ Scroll to bottom
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

        # â³ Wait until more <li> elements are detected
        try:
            WebDriverWait(driver, 10).until(
                lambda d: len(BeautifulSoup(d.page_source, "html.parser").find_all("li")) > previous_li_count
            )
        except:
            print("â³ Timeout waiting for new content to load (no new <li> detected).")

        # âœ… Continue tracking scrolls
        scrolls_done += 1
        print(f"ğŸ”„ Scroll {scrolls_done}")
        ###########################################################

        soup = BeautifulSoup(driver.page_source, "html.parser")

        #debug
        li_count = len(soup.find_all("li"))
        print(f"ğŸ§© Parsed {li_count} <li> elements after scroll {scrolls_done}")


        see_more_buttons = driver.find_elements(By.XPATH, "//span[contains(., '...see more') or contains(., 'See more')]")
        for btn in see_more_buttons:
            try:
                driver.execute_script("arguments[0].click();", btn)
                time.sleep(0.2)
            except:
                pass

        found_any = False

        for li in soup.find_all("li"):
            link_tag = li.select_one("a[href*='/feed/update']")
            if not link_tag:
                continue

            url = link_tag["href"]

            #debug
            print(f"ğŸ”— Found potential post: {url}")

            if url in new_urls:
                print("â†ªï¸ Already found in current run, skipping.")
                skipped_from_current_run += 1
                continue

            elif url in previously_saved_urls:
                print("â†ªï¸ Already found in previous runs, skipping.")
                skipped_from_previous_runs += 1
                continue


            new_urls.add(url)
            found_any = True

            author = "(unknown)"
            author_span = li.select_one("a[href*='/in/'] span[aria-hidden='true']")
            if author_span:
                author = author_span.get_text(strip=True)
            else:
                page_span = li.select_one("a[href*='/company/'] span[aria-hidden='true']")
                if page_span:
                    author = page_span.get_text(strip=True)

            cursor_pointer_divs = li.find_all("div", class_="cursor-pointer")
            role = cursor_pointer_divs[0].get_text(strip=True) if cursor_pointer_divs else ""
            post_text = cursor_pointer_divs[1].get_text(separator="\n", strip=True) if len(cursor_pointer_divs) > 1 else ""
            for suffix in ["â€¦see more", "See more"]:
                if post_text.endswith(suffix):
                    post_text = post_text[: -len(suffix)].rstrip()

            relative_time = ""
            original_author = ""
            time_tag = li.select_one("p.t-black--light.t-12 span[aria-hidden='true']")
            if time_tag:
                raw_text = time_tag.get_text(strip=True)
                match = re.search(r"(\d+\s*(?:h|d|w|mo|y|yr|yrs))\b", raw_text)
                if match:
                    relative_time = match.group(1).replace(" ", "")
                    relative_time = re.sub(r"yrs?$", "y", relative_time)
                original_author = extract_original_author_from_time_tag(raw_text)

            post_time_str = convert_relative_time_to_timestamp(relative_time)
            post_time = datetime.strptime(post_time_str, "%Y-%m-%d")

            if cutoff_date and post_time < cutoff_date:
                old_hit_count += 1
                print(f"âš ï¸ {old_hit_count}/{max_old_hits} old post(s) seen â€” {post_time_str} < cutoff")
                if old_hit_count > max_old_hits:
                    print("ğŸ›‘ Reached max old post threshold. Stopping.")
                    break
            else:
                old_hit_count = 0
                
            needs_original = not post_text or len(post_text.split()) < 10
            original_text = ""

            if needs_original:
                print(f"â†ªï¸ Visiting original post for deeper content: {author}")
                original_window = driver.current_window_handle
                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[-1])
                driver.get(url)

                try:
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CLASS_NAME, "update-components-actor__title"))
                    )
                except:
                    print("âš ï¸ Timeout waiting for original content.")

                try:
                    see_more = driver.find_element(By.XPATH, "//span[contains(., '...see more') or contains(., 'See more')]")
                    driver.execute_script("arguments[0].click();", see_more)
                    time.sleep(0.3)
                except:
                    pass

                ref_soup = BeautifulSoup(driver.page_source, "html.parser")
                text_div = ref_soup.select_one("div.update-components-text.relative.update-components-update-v2__commentary")
                original_text = text_div.get_text(separator="\n", strip=True) if text_div else ""

                driver.close()
                driver.switch_to.window(original_window)

            new_posts.append({
                "author": author,
                "role": role,
                "text": post_text,
                "original_text": original_text,
                "original_author": original_author,
                "relative_time": relative_time,
                "time": post_time_str,
                "url": url
            })

            print(f"âœ… {author} â€” {role[:30]} â€” {relative_time} â€” {post_time_str} â€” {post_text[:60]}...")

        # ğŸ”§ Fix: Stop only if in scroll mode AND found nothing new
        if mode == "1" and not found_any:
            print("âš ï¸ No new posts found. Ending scroll.")
            break

        # ğŸ”§ Enhancement: max scroll safeguard in cutoff mode
        if mode == "2" and scrolls_done >= max_cutoff_scrolls:
            print("ğŸ›‘ Reached max scroll limit in cutoff mode. Ending.")
            break

    all_posts.extend(new_posts)
    all_urls = previously_saved_urls.union(new_urls)

    with open("saved_posts.json", "w", encoding="utf-8") as f:
        json.dump(all_posts, f, indent=2, ensure_ascii=False)
        
    with open("saved_urls.json", "w", encoding="utf-8") as f:
        # json.dump(list(all_urls), f, indent=2)
        json.dump([post["url"] for post in all_posts], f, indent=2)


    print(f"ğŸ“Š Scroll {scrolls_done} stats:")
    print(f"    ğŸ”¹ New URLs this scroll: {len(new_urls)} total so far")
    print(f"    ğŸ”¹ Posts added in this scroll: {len(new_posts)} total so far")
    print(f"    ğŸ”¹ Skipped duplicates (current run): {skipped_from_current_run}")
    print(f"    ğŸ”¹ Skipped duplicates (previous runs): {skipped_from_previous_runs}")


def main():
    global driver # fix?
    print("Select mode:")
    print("1 - Scrape and save posts")
    print("2 - Search previously saved posts")
    main_mode = input("Enter 1 or 2: ").strip()

    if main_mode == "1":
        cookies = wait_for_user_login()
        print("\nğŸ•µï¸ Do you want to continue scraping in headless mode? (y/n):")
        use_headless = input().strip().lower() == "y"

        if use_headless:
            driver.quit()
            driver = create_headless_driver_with_cookies(cookies)
        fetch_saved_posts()
        if driver:
            driver.quit()
    elif main_mode == "2":
        search_saved_posts()
    else:
        print("âŒ Invalid mode. Exiting.")

