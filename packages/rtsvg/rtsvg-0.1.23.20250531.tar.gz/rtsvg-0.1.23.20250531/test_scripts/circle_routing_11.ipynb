{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy  as np\n",
    "import networkx as nx\n",
    "from math import sin, cos, pi, sqrt, atan2\n",
    "from os.path import exists\n",
    "import time\n",
    "from rtsvg import *\n",
    "rt = RACETrack()\n",
    "ts1 = time.time()\n",
    "df = pl.concat([pl.read_csv('../../data/2013_vast_challenge/mc3_netflow/nf/nf-chunk1.csv'),\n",
    "                pl.read_csv('../../data/2013_vast_challenge/mc3_netflow/nf/nf-chunk2.csv'),\n",
    "                pl.read_csv('../../data/2013_vast_challenge/mc3_netflow/nf/nf-chunk3.csv')])\n",
    "df = rt.columnsAreTimestamps(df, 'parsedDate')\n",
    "ts2 = time.time()\n",
    "print(f'Loading Time ... {ts2 - ts1:0.2} sec')\n",
    "\n",
    "df = df.drop(['TimeSeconds',\n",
    "              #'parsedDate',\n",
    "              'dateTimeStr',\n",
    "              #'ipLayerProtocol',\n",
    "              'ipLayerProtocolCode',\n",
    "              #'firstSeenSrcIp',\n",
    "              #'firstSeenDestIp',\n",
    "              #'firstSeenSrcPort',\n",
    "              #'firstSeenDestPort',\n",
    "              'moreFragments',\n",
    "              'contFragments',\n",
    "              #'durationSeconds',\n",
    "              'firstSeenSrcPayloadBytes',\n",
    "              'firstSeenDestPayloadBytes',\n",
    "              #'firstSeenSrcTotalBytes',\n",
    "              #'firstSeenDestTotalBytes',\n",
    "              #'firstSeenSrcPacketCount',\n",
    "              #'firstSeenDestPacketCount',\n",
    "              'recordForceOut'])\n",
    "\n",
    "df = df.rename({'parsedDate':'ts',\n",
    "               'ipLayerProtocol':'pro',\n",
    "               'firstSeenSrcIp':'sip',\n",
    "               'firstSeenDestIp':'dip',\n",
    "               'firstSeenSrcPort':'spt',\n",
    "               'firstSeenDestPort':'dpt',\n",
    "               'durationSeconds':'dur',\n",
    "               'firstSeenSrcTotalBytes':'soct',\n",
    "               'firstSeenDestTotalBytes':'doct',\n",
    "               'firstSeenSrcPacketCount':'spkt',\n",
    "               'firstSeenDestPacketCount':'dpkt'})\n",
    "\n",
    "print('total nodes = ', len(set(df['sip']) | set(df['dip'])))\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_file = '../../data/2013_vast_challenge/mc3_netflow/spring_layout.csv'\n",
    "relates = [('sip','dip')]\n",
    "g       = rt.createNetworkXGraph(df, relates)\n",
    "pos = {} if exists(layout_file) else nx.spring_layout(g)\n",
    "#rt.link(df, relates, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniqs = df.unique(subset=['sip','dip'])\n",
    "total_nodes = len(set(df_uniqs['sip']) | set(df_uniqs['dip']))\n",
    "print(f'{len(df_uniqs)=} | {total_nodes=}')\n",
    "_igl_ = rt.interactiveGraphLayout(df_uniqs, {'relationships':relates, 'pos':pos, 'draw_labels':False, 'bounds_percent':0.02}, w=1200, h=800)\n",
    "if exists(layout_file): _igl_.loadLayout(layout_file)\n",
    "#_igl_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_igl_.saveLayout(layout_file)\n",
    "#print(_igl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse the original graph into one based on position of nodes in xy coordinates\n",
    "g      = rt.createNetworkXGraph(df_uniqs, relates)\n",
    "#_link_ = rt.link(df_uniqs, relates, pos, link_size=None, w=1200, h=900)\n",
    "_link_ = rt.link(df_uniqs, relates, pos, w=1200, h=900)\n",
    "_link_.renderSVG() # force a render so that xT and yT exist\n",
    "all_nodes    = set(df_uniqs['sip']) | set(df_uniqs['dip'])\n",
    "all_nodes_ls = list(all_nodes)\n",
    "x_min, y_min, x_max, y_max = 1e9, 1e9, -1e9, -1e9 # probably not a safe assumption :(\n",
    "node_to_xy = {}\n",
    "xy_to_node = {}\n",
    "for node in all_nodes:\n",
    "    x,y              = _link_.xT(pos[node][0]), _link_.yT(pos[node][1])\n",
    "    xy               = (x,y)\n",
    "    x_min, x_max     = min(x_min, x), max(x_max, x)\n",
    "    y_min, y_max     = min(y_min, y), max(y_max, y)\n",
    "    node_to_xy[node] = xy\n",
    "    if xy not in xy_to_node: xy_to_node[xy] = []\n",
    "    xy_to_node[xy].append(node)\n",
    "#_link_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_not_useful_ = '''\n",
    "import hdbscan\n",
    "vecs  = []\n",
    "xy_lu = {'x':[], 'y':[], 'c':[]}\n",
    "for node in all_nodes_ls:\n",
    "    xy  = node_to_xy[node]\n",
    "    v_i = [(xy[0]-x_min)/(x_max-x_min), (xy[1]-y_min)/(y_max-y_min)]\n",
    "    vecs.append(v_i)\n",
    "    xy_lu['x'].append(xy[0]), xy_lu['y'].append(xy[1])\n",
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer.fit(vecs)\n",
    "for c in clusterer.labels_: xy_lu['c'].append(c)\n",
    "rt.xy(pl.DataFrame(xy_lu), x_field='x', y_field='y', color_by='c', w=800, h=600)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_degree_analysis_ = '''\n",
    "degrees_lu = {'degree':[]}\n",
    "for node in all_nodes_ls: degrees_lu['degree'].append(g.degree(node))\n",
    "rt.histogram(pl.DataFrame(degrees_lu), bin_by='degree')\n",
    "degree_histogram_lu = {'threshold':[], 'count':[]}\n",
    "for i in range (500, 1000):\n",
    "    count = 0\n",
    "    for node in all_nodes_ls:\n",
    "        if g.degree(node) >= i: count += 1\n",
    "    degree_histogram_lu['threshold'].append(i), degree_histogram_lu['count'].append(count)\n",
    "rt.xy(pl.DataFrame(degree_histogram_lu), x_field='threshold', y_field='count')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_not_this_ = '''\n",
    "# Degree sorting (and node indexing from 0...N-1)\n",
    "degree_sorter = []\n",
    "for node in all_nodes_ls: degree_sorter.append((g.degree(node), node))\n",
    "degree_sorter.sort(reverse=True)\n",
    "node_index, rev_node_index = {}, {}\n",
    "for i, node in enumerate(all_nodes_ls): node_index[node], rev_node_index[i] = i, node\n",
    "\n",
    "# Only create enough nborhoods to cover distinguishable colors\n",
    "distinguishable_colors = rt.co_mgr.colorgoricalColors()\n",
    "my_origins             = []\n",
    "for i in range(len(distinguishable_colors)): my_origins.append(i) # nodes with the highest degree\n",
    "\n",
    "# Raster creation\n",
    "_link_ = rt.link(df_uniqs, relates, pos, w=600,h=400)\n",
    "_link_.renderSVG() # force a render so that xT and yT exist\n",
    "w,h = _link_.w, _link_.h\n",
    "my_raster = [[None for x in range(w)] for y in range(h)]\n",
    "for _node_ in pos.keys():\n",
    "    x, y = int(_link_.xT(pos[_node_][0])), int(_link_.yT(pos[_node_][1]))\n",
    "    if my_raster[y][x] is None: my_raster[y][x] = set()\n",
    "    my_raster[y][x].add(node_index[_node_])\n",
    "\n",
    "# Perform the balanced level set algorithm\n",
    "# ... 33.6s @ 1200x900 (amd 7900x w/ 96g memory)\n",
    "# ...  7.2s @  600x400 (amd 7900x w/ 96g memory)\n",
    "# ...  5.3s @  500x350 (amd 7900x w/ 96g memory)\n",
    "# ...  3.2s @  400x300 (amd 7900x w/ 96g memory)\n",
    "# ...  1.8s @  300x200 (amd 7900x w/ 96g memory)\n",
    "t0 = time.time()\n",
    "my_state, my_found_time, my_finds, my_progress_lu = rt.levelSetBalanced(my_raster, my_origins, 0)\n",
    "t1 = time.time()\n",
    "print(f'{t1-t0:0.2f}s')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_debug_step_ = '''\n",
    "rt.tile([rt.levelSetStateAndFoundTimeSVG(my_state,my_found_time),\n",
    "         rt.xy(pl.DataFrame(my_progress_lu), x_field='iteration', y_field='heapsize', color_by='origin', dot_size='tiny', w=1024, h=128)], horz=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_paired_with_not_this_ = '''\n",
    "my_convex_hulls = {}\n",
    "group_i = 0\n",
    "for x in my_finds:\n",
    "    my_convex_hulls[f'group {group_i}'] = []\n",
    "    for node_id in my_finds[x]:\n",
    "        node = rev_node_index[node_id]\n",
    "        my_convex_hulls[f'group {group_i}'].append(node)\n",
    "    rt.co_mgr.str_to_color_lu[f'group {group_i}'] = distinguishable_colors[group_i]\n",
    "    group_i += 1\n",
    "\n",
    "_link_ = rt.link(df_uniqs, relates, pos, link_size=None, node_size='small', w=900, h=600, convex_hull_lu=my_convex_hulls)\n",
    "_link_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_voronoi_version_ = '''\n",
    "# Degree sorting (and node indexing from 0...N-1)\n",
    "degree_sorter = []\n",
    "for node in all_nodes_ls: degree_sorter.append((g.degree(node), node))\n",
    "degree_sorter.sort(reverse=True)\n",
    "node_index, rev_node_index = {}, {}\n",
    "for i, node in enumerate(all_nodes_ls): node_index[node], rev_node_index[i] = i, node\n",
    "\n",
    "# Only create enough nborhoods to cover distinguishable colors\n",
    "distinguishable_colors = rt.co_mgr.brewerColors('qualitative', 12)\n",
    "my_origins             = []\n",
    "for i in range(len(distinguishable_colors)):\n",
    "    rt.co_mgr.str_to_color_lu[i] = distinguishable_colors[i]\n",
    "    my_origins.append(i) # nodes with the highest degree\n",
    "\n",
    "# Voronoi creation\n",
    "_link_ = rt.link(df_uniqs, relates, pos, w=600,h=400)\n",
    "_link_.renderSVG() # force a render so that xT and yT exist\n",
    "w,h = _link_.w, _link_.h\n",
    "voronoi_pts = []\n",
    "for i in range(len(distinguishable_colors)):\n",
    "    _node_ = degree_sorter[i][1]\n",
    "    x, y = _link_.xT(pos[_node_][0]), _link_.yT(pos[_node_][1])\n",
    "    voronoi_pts.append((x,y))\n",
    "nborhoods = rt.isedgarVoronoi(voronoi_pts, Box=[(0,h),(w,h),(w,0),(0,0)])\n",
    "_v_svg_ = [f'<svg x=\"0\" y=\"0\" width=\"{w}\" height=\"{h}\"/>']\n",
    "for _poly_i_ in range(len(nborhoods)):\n",
    "    _poly_ = nborhoods[_poly_i_]\n",
    "    d = f'M {_poly_[0][0]} {_poly_[0][1]}  '\n",
    "    for i in range(1, len(_poly_)): d += f'L {_poly_[i][0]} {_poly_[i][1]} '\n",
    "    d += 'Z'\n",
    "    _v_svg_.append(f'<path d=\"{d}\" fill=\"{rt.co_mgr.str_to_color_lu[_poly_i_]}\" />')\n",
    "_v_svg_.append('</svg>')\n",
    "rt.tile([''.join(_v_svg_)])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means version\n",
    "import random\n",
    "_node_ls_  = list(pos.keys())\n",
    "_node_to_i_ = {}\n",
    "for i in range(len(_node_ls_)): _node_to_i_[_node_ls_[i]] = i\n",
    "_node_sxy_ = []\n",
    "for _node_ in _node_ls_: \n",
    "    sx, sy = _link_.xT(pos[_node_][0]), _link_.yT(pos[_node_][1])\n",
    "    _node_sxy_.append((sx, sy))\n",
    "sx_min, sx_max, sy_min, sy_max = sx, sx, sy, sy\n",
    "for _node_ in _node_ls_:\n",
    "    sx_min, sx_max = min(sx_min, sx), max(sx_max, sx)\n",
    "    sy_min, sy_max = min(sy_min, sy), max(sy_max, sy)\n",
    "\n",
    "# Distinguishable colors\n",
    "distinguishable_colors = rt.co_mgr.brewerColors('qualitative', 10)\n",
    "\n",
    "# Parameters for K-Means\n",
    "k, iters = len(distinguishable_colors), 100\n",
    "\n",
    "# Make random cluster centers\n",
    "cluster_centers = {}\n",
    "for i in range(k): \n",
    "    sx, sy = random.random() * (sx_max - sx_min) + sx_min, random.random() * (sy_max - sy_min) + sy_min\n",
    "    cluster_centers[i] = (sx, sy)\n",
    "\n",
    "# Iterate K-Means\n",
    "for _iter_ in range(iters):\n",
    "    # Assign nodes to their closest center\n",
    "    center_assignments = {}\n",
    "    for j in range(len(_node_ls_)):\n",
    "        _node_ = _node_ls_[j]\n",
    "        min_dist       = (_node_sxy_[j][0] - cluster_centers[0][0])**2 + (_node_sxy_[j][1] - cluster_centers[0][1])**2\n",
    "        closest_center = 0\n",
    "        for i in range(1, k):\n",
    "            dist = (_node_sxy_[j][0] - cluster_centers[i][0])**2 + (_node_sxy_[j][1] - cluster_centers[i][1])**2\n",
    "            if dist < min_dist:\n",
    "                min_dist       = dist\n",
    "                closest_center = i\n",
    "        if closest_center not in center_assignments: center_assignments[closest_center] = []\n",
    "        center_assignments[closest_center].append(_node_)\n",
    "    # If there are any centers without nodes, assign a random node to them\n",
    "    for i in range(k): \n",
    "        if i not in center_assignments: center_assignments[i] = [random.choice(_node_ls_)]\n",
    "    # Update centers\n",
    "    for i in range(k): \n",
    "        sx, sy = 0, 0\n",
    "        for _node_ in center_assignments[i]: \n",
    "            sx, sy = sx + _node_sxy_[_node_to_i_[_node_]][0], sy + _node_sxy_[_node_to_i_[_node_]][1]\n",
    "        cluster_centers[i] = (sx/len(center_assignments[i]), sy/len(center_assignments[i]))\n",
    "\n",
    "voronoi_pts = []\n",
    "for i in cluster_centers: voronoi_pts.append(cluster_centers[i])\n",
    "\n",
    "nborhoods = rt.isedgarVoronoi(voronoi_pts, Box=[(0,_link_.h),(_link_.w,_link_.h),(_link_.w,0),(0,0)])\n",
    "voronoi_svg = [f'<svg x=\"0\" y=\"0\" width=\"{_link_.w}\" height=\"{_link_.h}\"/><g opacity=\"0.4\">']\n",
    "for _poly_i_ in range(len(nborhoods)):\n",
    "    _poly_ = nborhoods[_poly_i_]\n",
    "    d = f'M {_poly_[0][0]} {_poly_[0][1]}  '\n",
    "    for i in range(1, len(_poly_)): d += f'L {_poly_[i][0]} {_poly_[i][1]} '\n",
    "    d += 'Z'\n",
    "    voronoi_svg.append(f'<path d=\"{d}\" fill=\"{distinguishable_colors[_poly_i_]}\" />')\n",
    "voronoi_svg.append('</g></svg>')\n",
    "\n",
    "k_svg = []\n",
    "for i in cluster_centers:\n",
    "    sx, sy = cluster_centers[i][0], cluster_centers[i][1]\n",
    "    k_svg.append(f'<circle cx=\"{sx}\" cy=\"{sy}\" r=\"5\" fill=\"{distinguishable_colors[i]}\"/>')\n",
    "    k_svg.append(f'<text x=\"{sx}\" y=\"{sy+16} stroke=\"#ff0000\" text-anchor=\"middle\">{len(center_assignments[i])}</text>')\n",
    "\n",
    "# rt.tile(['<svg w=\"{_link_.w}\" h=\"{_link_.h}\">' + '<g opacity=\"0.8\">' + _link_.renderSVG() + '</g>' + ''.join(voronoi_svg) + ''.join(k_svg) + '</svg>'])\n",
    "rt.tile(['<svg w=\"{_link_.w}\" h=\"{_link_.h}\">' + ''.join(voronoi_svg) + ''.join(k_svg) + '</svg>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_in_cluster_svg   = [f'<svg x=\"0\" y=\"0\" width=\"{_link_.w}\" height=\"{_link_.h}\">']\n",
    "node_to_cluster_center = {}\n",
    "for i in center_assignments:\n",
    "    for _node_ in center_assignments[i]: \n",
    "        sx, sy = _link_.xT(pos[_node_][0]), _link_.yT(pos[_node_][1])\n",
    "        nodes_in_cluster_svg.append(f'<circle cx=\"{sx}\" cy=\"{sy}\" r=\"3\" fill=\"{distinguishable_colors[i]}\"/>')\n",
    "        node_to_cluster_center[_node_] = i\n",
    "nodes_in_cluster_svg.append('</svg>')\n",
    "# rt.tile([''.join(nodes_in_cluster_svg)]) # this just matches the voronoi diagram from the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col('dip').replace_strict(node_to_cluster_center).alias('dip_cluster'))\n",
    "df_degree_count     = df.unique(['sip','dip','dip_cluster']).group_by(['sip','dip_cluster']).agg(pl.len())\n",
    "#df_connection_count = df.group_by(['sip','dip_cluster']).agg(pl.len())\n",
    "color_order = rt.colorRenderOrder(df_degree_count, 'dip_cluster', 'len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrinkPoints(center_xy, xys, r=4):\n",
    "    ret = []\n",
    "    for xy in xys:\n",
    "        uv = rt.unitVector((xy, center_xy))\n",
    "        ret.append((xy[0] + uv[0]*r, xy[1] + uv[1]*r))\n",
    "    return ret\n",
    "\n",
    "voronoi_fills_svg = [f'<svg x=\"0\" y=\"0\" width=\"{_link_.w}\" height=\"{_link_.h}\"/>']\n",
    "voronoi_edges_svg = [f'<svg x=\"0\" y=\"0\" width=\"{_link_.w}\" height=\"{_link_.h}\"/>']\n",
    "for _poly_i_ in range(len(nborhoods)):\n",
    "    center_xy = cluster_centers[_poly_i_]\n",
    "    # _poly_ = shrinkPoints(center_xy, nborhoods[_poly_i_])\n",
    "    _poly_ = nborhoods[_poly_i_]\n",
    "    d = f'M {_poly_[0][0]} {_poly_[0][1]}  '\n",
    "    for i in range(1, len(_poly_)): d += f'L {_poly_[i][0]} {_poly_[i][1]} '\n",
    "    d += 'Z'\n",
    "    voronoi_edges_svg.append(f'<path d=\"{d}\" fill=\"none\" stroke-width=\"5\" stroke=\"{distinguishable_colors[_poly_i_]}\" />')\n",
    "    voronoi_fills_svg.append(f'<path d=\"{d}\" fill=\"{distinguishable_colors[_poly_i_]}\" />')\n",
    "voronoi_edges_svg.append('</svg>')\n",
    "voronoi_fills_svg.append('</svg>')\n",
    "#rt.tile([''.join(voronoi_fills_svg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(distinguishable_colors)):\n",
    "    rt.co_mgr.str_to_color_lu[i]      = distinguishable_colors[i]\n",
    "    rt.co_mgr.str_to_color_lu[str(i)] = distinguishable_colors[i]\n",
    "\n",
    "glyph_svgs = [f'<svg x=\"0\" y=\"0\" width=\"{_link_.w}\" height=\"{_link_.h}\">']\n",
    "for i in center_assignments:\n",
    "    _nodes_ = set(center_assignments[i])\n",
    "    _df_    = df_degree_count.filter(pl.col('sip').is_in(_nodes_))\n",
    "    for k, k_df in _df_.group_by('sip'):\n",
    "        sx, sy = _link_.xT(pos[k[0]][0]), _link_.yT(pos[k[0]][1])\n",
    "        _within_  = k_df.filter( pl.col('dip_cluster') == i)\n",
    "        _without_ = k_df.filter(~pl.col('dip_cluster') != i)\n",
    "        glyph_svg = rt.concentricGlyph(_without_, sx, sy, 0.0, len(set(_within_['dip_cluster']))/len(_nodes_), order=color_order, nbor='dip_cluster', count_by='len')\n",
    "        glyph_svgs.append(glyph_svg)\n",
    "glyph_svgs.append('</svg>')\n",
    "rt.tile([f'<svg x=\"0\" y=\"0\" width=\"{_link_.w}\" height=\"{_link_.h}\">' + \n",
    "         ''.join(voronoi_fills_svg) + \n",
    "         ''.join(glyph_svgs) + \n",
    "         '</svg>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_link_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
