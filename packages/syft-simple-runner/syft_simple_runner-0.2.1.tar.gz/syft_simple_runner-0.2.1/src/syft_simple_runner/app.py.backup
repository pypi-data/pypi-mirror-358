#!/usr/bin/env python3
"""
Syft Simple Runner App - SyftBox Integration

This module runs as a SyftBox app, called periodically by SyftBox to process
approved code execution jobs. It checks for approved jobs, executes them, and updates their status.
"""

import time
from pathlib import Path
from loguru import logger
from datetime import datetime

try:
    from syft_core import Client as SyftBoxClient
except ImportError:
    logger.warning("syft_core not available - running in standalone mode")
    # Fallback for development/testing
    class MockSyftBoxClient:
        def __init__(self):
            self.email = "demo@example.com"
        
        def app_data(self, app_name):
            import tempfile
            return Path(tempfile.gettempdir()) / f"syftbox_demo_{app_name}"
        
        @classmethod
        def load(cls):
            return cls()
    
    SyftBoxClient = MockSyftBoxClient

from .models import JobStatus, QueueConfig, SimpleJob
from .runner import SafeCodeRunner


class RunnerApp:
    """
    SyftBox app for processing code execution queue.
    
    This runs periodically (called by SyftBox) to process approved jobs
    and execute them safely.
    """
    
    def __init__(self):
        self.syftbox_client = SyftBoxClient.load()
        self.queue_config = QueueConfig(self._get_queue_dir())
        self.runner = SafeCodeRunner()
        self.max_concurrent_jobs = 3
        
        logger.info(f"Initialized Simple Runner App for {self.syftbox_client.email}")
    
    @property
    def email(self) -> str:
        """Get current user's email."""
        return self.syftbox_client.email
    
    def run(self):
        """
        Main app entry point - processes the queue once and exits.
        """
        logger.info("Starting queue processing cycle...")
        
        try:
            # Log pending jobs (data owners need to manually approve these)
            self._log_pending_jobs()
            
            # Execute approved jobs
            self._execute_approved_jobs()
            
            # Clean up old jobs
            self._cleanup_old_jobs()
            
            logger.info("Queue processing cycle completed successfully")
            
        except Exception as e:
            logger.error(f"Error in queue processing: {e}")
            raise
    
    def _log_pending_jobs(self):
        """Log information about pending jobs waiting for approval."""
        pending_jobs = self._get_jobs_by_status(JobStatus.PENDING)
        
        # Only show jobs targeted at this datasite
        my_pending = [job for job in pending_jobs if job.target_email == self.email]
        
        if my_pending:
            logger.info(f"ðŸ“‹ {len(my_pending)} job(s) pending approval:")
            for job in my_pending:
                logger.info(f"   â€¢ {job.name} from {job.requester_email}")
        else:
            logger.debug("No jobs pending approval")
    
    def _execute_approved_jobs(self):
        """Execute jobs that have been manually approved."""
        approved_jobs = self._get_jobs_by_status(JobStatus.APPROVED)
        
        # Only process jobs targeted at this datasite
        my_approved = [job for job in approved_jobs if job.target_email == self.email]
        
        if not my_approved:
            logger.debug("No approved jobs to execute")
            return
        
        # Check how many jobs are currently running
        running_jobs = self._get_jobs_by_status(JobStatus.RUNNING)
        my_running = [job for job in running_jobs if job.target_email == self.email]
        
        # Limit concurrent executions
        available_slots = self.max_concurrent_jobs - len(my_running)
        if available_slots <= 0:
            logger.info(f"Maximum concurrent jobs ({self.max_concurrent_jobs}) reached")
            return
        
        # Execute jobs up to the limit
        jobs_to_execute = my_approved[:available_slots]
        
        logger.info(f"ðŸš€ Executing {len(jobs_to_execute)} approved job(s)")
        
        for job in jobs_to_execute:
            self._execute_single_job(job)
    
    def _execute_single_job(self, job: SimpleJob):
        """Execute a single job."""
        try:
            logger.info(f"Starting execution of job: {job.name}")
            
            # Update status to running
            job.status = JobStatus.RUNNING
            job_file = self.queue_config.get_job_file(job.uid)
            job.save_to_file(job_file)
            
            # Execute the job
            exit_code, stdout, stderr = self.runner.run_job(job)
            
            # Update job status and logs based on result
            job.exit_code = exit_code
            job.logs = f"STDOUT:\n{stdout}\n\nSTDERR:\n{stderr}"
            job.completed_at = datetime.now().isoformat()
            
            if exit_code == 0:
                job.status = JobStatus.COMPLETED
                logger.info(f"Job {job.uid} completed successfully")
            else:
                job.status = JobStatus.FAILED
                logger.warning(f"Job {job.uid} failed with exit code {exit_code}")
            
        except Exception as e:
            error_msg = f"Job execution error: {e}"
            job.status = JobStatus.FAILED
            job.logs = error_msg
            job.completed_at = datetime.now().isoformat()
            logger.error(f"Job {job.uid} failed: {error_msg}")
        
        finally:
            # Always save the final job state
            job_file = self.queue_config.get_job_file(job.uid)
            job.save_to_file(job_file)
    
    def _cleanup_old_jobs(self):
        """Clean up old completed jobs (older than 24 hours)."""
        cutoff_time = datetime.now().timestamp() - (24 * 60 * 60)  # 24 hours ago
        
        queue_dir = self._get_queue_dir()
        if not queue_dir.exists():
            return
        
        cleaned_count = 0
        for job_file in queue_dir.glob("*.json"):
            try:
                job = SimpleJob.from_file(job_file)
                if job.status in [JobStatus.COMPLETED, JobStatus.FAILED] and job.completed_at:
                    completed_time = datetime.fromisoformat(job.completed_at).timestamp()
                    if completed_time < cutoff_time:
                        logger.debug(f"Cleaning up old job: {job.uid}")
                        job_file.unlink()
                        cleaned_count += 1
                        
            except Exception as e:
                logger.warning(f"Failed to cleanup job {job_file}: {e}")
        
        if cleaned_count > 0:
            logger.info(f"ðŸ§¹ Cleaned up {cleaned_count} old job(s)")
    
    def _get_jobs_by_status(self, status: JobStatus):
        """Get all jobs with a specific status."""
        jobs = []
        
        for job_id in self.queue_config.list_jobs():
            try:
                job_file = self.queue_config.get_job_file(job_id)
                job = SimpleJob.from_file(job_file)
                if job.status == status:
                    jobs.append(job)
            except Exception as e:
                logger.warning(f"Failed to load job {job_id}: {e}")
        
        return jobs
    
    def _get_queue_dir(self) -> Path:
        """Get the queue directory."""
        return self.syftbox_client.app_data("code-queue")


def main():
    """Main entry point for the SyftBox app."""
    try:
        app = RunnerApp()
        app.run()
    except Exception as e:
        logger.error(f"App failed: {e}")
        exit(1)


if __name__ == "__main__":
    main()
