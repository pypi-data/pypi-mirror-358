Metadata-Version: 2.4
Name: forgellm
Version: 0.1.1
Summary: A comprehensive toolkit for end-to-end continued pre-training, fine-tuning, monitoring, testing and publishing of language models with MLX-LM
Author: Mnemosyne Team
License: MIT
Project-URL: Homepage, https://github.com/lpalbou/forgellm
Project-URL: Bug Tracker, https://github.com/lpalbou/forgellm/issues
Project-URL: Documentation, https://github.com/lpalbou/forgellm/tree/main/docs
Project-URL: Source Code, https://github.com/lpalbou/forgellm
Keywords: llm,language-models,machine-learning,mlx,mlx-lm,fine-tuning,pre-training
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: mlx-lm>=0.0.3
Requires-Dist: mlx>=0.0.7
Requires-Dist: numpy>=1.20.0
Requires-Dist: flask>=2.0.0
Requires-Dist: flask-socketio>=5.0.0
Requires-Dist: plotly>=5.0.0
Requires-Dist: psutil>=5.8.0
Requires-Dist: huggingface-hub>=0.12.0
Requires-Dist: matplotlib>=3.4.0
Requires-Dist: filelock>=3.0.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.0.0; extra == "dev"
Requires-Dist: mypy>=0.900; extra == "dev"
Requires-Dist: build>=0.10.0; extra == "dev"
Requires-Dist: twine>=4.0.0; extra == "dev"
Dynamic: license-file

# ForgeLLM

CAUTION : THIS IS A WORK IN PROGRESS ! THIS REPOSITORY IS NOT YET STABLE. AT THIS TIME, ONLY USE IT FOR EXPLORATORY RESEARCH / LEARNING.

A comprehensive toolkit for continued pre-training and fine-tuning of language models with MLX-LM.

ForgeLLM builds upon the capabilities of MLX and MLX-LM, providing convenient features, monitoring tools, and simplified workflows for language model training and inference. See our [Acknowledgments](ACKNOWLEDGEMENTS.md) for more details.

## Features

- **Continued Pre-Training**: Train models on your domain-specific data
- **Instruction Fine-Tuning**: Create instruction-following models
- **Web Interface**: Monitor training progress and test models
- **Configuration Management**: Easily create and manage training configurations
- **Dashboard Generation**: Visualize training metrics
- **Model Publishing**: Convert checkpoints to shareable formats

## Installation

```bash
# Install from the current directory
pip install -e .

# Or install directly from GitHub
pip install git+https://github.com/lpalbou/forgellm.git
```

## Quick Start

### Web Interface

```bash
# Start the web interface
python -m forgellm.forgellm_web
```

Access the web interface at http://localhost:5001

### Command Line Interface

```bash
# Create default configurations
python -m forgellm.cli.commands config create-defaults

# Start continued pre-training
python -m forgellm.cli.commands train --config configs/cpt_default.yaml

# Start instruction tuning
python -m forgellm.cli.commands instruct --config configs/ift_default.yaml

# Generate text with a trained model
python -m forgellm.cli.commands generate \
    --model "mlx-community/gemma-3-4b-it-bf16" \
    --adapter-path "models/cpt/my_model/adapters.safetensors" \
    --prompt "Write a short story about AI"
```

## Documentation

For more detailed documentation, see:

- [Testing Guide](../docs/TESTING_GUIDE.md)
- [Architecture](../docs/architecture.md)
- [Usage Guide](../docs/USAGE_GUIDE.md)

## Python API

```python
from forgellm.training.config import TrainingConfig
from forgellm.training.trainer import ContinuedPretrainer
from forgellm.models.model_manager import ModelManager

# Create configuration
config = TrainingConfig(
    model_name="mlx-community/gemma-3-4b-it-bf16",
    input_dir="my_preprocessed_dataset",
    output_dir="my_output_folder",
    batch_size=4,
    learning_rate=5e-6,
    max_iterations=1000
)

# Initialize trainer
trainer = ContinuedPretrainer(config)

# Run training
trainer.run_training()

# Generate text
model_manager = ModelManager()
model_manager.load("mlx-community/gemma-3-4b-it-bf16", "models/cpt/my_model/adapters.safetensors")
response = model_manager.generate("Write a short story about AI", max_tokens=200)
print(response)
```

## Requirements

- Python 3.8+
- MLX 0.0.7+
- MLX-LM 0.0.3+
- Flask 2.0.0+
- Flask-SocketIO 5.0.0+

## Acknowledgments

This project builds upon the exceptional work of the MLX and MLX-LM teams. See [ACKNOWLEDGEMENTS.md](ACKNOWLEDGEMENTS.md) for full details.

## License

[MIT](LICENSE) 
