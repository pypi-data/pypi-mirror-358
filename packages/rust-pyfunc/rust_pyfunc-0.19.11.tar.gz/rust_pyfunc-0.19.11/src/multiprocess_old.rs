use pyo3::prelude::*;
use pyo3::types::{PyList, PyDict};
use std::process::{Command, Stdio, Child};
use std::io::{Write, BufRead, BufReader};
use std::sync::mpsc::{channel, Receiver, Sender};
use std::sync::{Arc, Mutex};
use std::thread;
use crossbeam::channel::{unbounded, Receiver as CrossbeamReceiver, Sender as CrossbeamSender};
use std::time::{SystemTime, UNIX_EPOCH, Instant};
use serde::{Serialize, Deserialize};
use crate::backup::BackupManager;

/// ä»»åŠ¡æ•°æ®ç»“æ„
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Task {
    pub date: i32,
    pub code: String,
}

/// å‘é€ç»™å·¥ä½œè¿›ç¨‹çš„æŒ‡ä»¤
#[derive(Serialize, Deserialize, Debug)]
pub enum WorkerCommand {
    Task(Task),
    GoClass(String),
    FunctionCode(String),
    Execute {},
    Ping {},
    Exit {},
}

/// å·¥ä½œè¿›ç¨‹è¯·æ±‚
#[derive(Serialize, Deserialize, Debug)]
pub struct WorkerRequest {
    pub tasks: Vec<Task>,
    pub function_code: String,
    pub go_class_serialized: Option<String>,
}

/// å·¥ä½œè¿›ç¨‹å“åº”
#[derive(Serialize, Deserialize, Debug)]
pub struct WorkerResponse {
    pub results: Vec<Vec<f64>>,
    pub errors: Vec<String>,
    pub task_count: usize,
}

/// Ping å“åº”
#[derive(Serialize, Deserialize, Debug)]
pub struct PingResponse {
    pub status: String,
}

/// è®¡ç®—ç»“æœ
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct ProcessResult {
    pub date: i32,
    pub code: String,
    pub timestamp: i64,
    pub facs: Vec<f64>,
}

/// è¿”å›ç»“æœï¼ˆä¸å«timestampï¼‰
#[derive(Clone, Debug)]
pub struct ReturnResult {
    pub date: i32,
    pub code: String,
    pub facs: Vec<f64>,
}

impl ProcessResult {
    pub fn to_return_result(&self) -> ReturnResult {
        ReturnResult {
            date: self.date,
            code: self.code.clone(),
            facs: self.facs.clone(),
        }
    }
}

/// å·¥ä½œè¿›ç¨‹ç®¡ç†å™¨
pub struct WorkerProcess {
    child: Child,
    stdin: std::process::ChildStdin,
    stdout_reader: BufReader<std::process::ChildStdout>,
    id: usize,
}

/// ä»»åŠ¡åˆ†å‘å™¨çŠ¶æ€
#[derive(Debug)]
struct TaskDispatcherState {
    total_tasks: usize,
    completed_tasks: usize,
    dispatched_tasks: usize,
}

/// è¿›åº¦æ›´æ–°ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct ProgressInfo {
    pub completed: usize,
    pub total: usize,
    pub elapsed_secs: f64,
    pub estimated_remaining_secs: f64,
}

impl WorkerProcess {
    /// åˆ›å»ºæ–°çš„å·¥ä½œè¿›ç¨‹
    pub fn new(id: usize, python_path: &str) -> PyResult<Self> {
        // è·å–å·¥ä½œè„šæœ¬è·¯å¾„ - ä½¿ç”¨ç»å¯¹è·¯å¾„é¿å…å½“å‰ç›®å½•é—®é¢˜
        // let script_path = std::path::PathBuf::from("/home/chenzongwei/rust_pyfunc/python/worker_process.py");
        let mut script_path = std::path::PathBuf::from(env!("CARGO_MANIFEST_DIR"));
        script_path.push("python");
        script_path.push("worker_process.py");
        println!("script_path: {}", script_path.display());
        
        // æ£€æŸ¥è„šæœ¬æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if !script_path.exists() {
            return Err(PyErr::new::<pyo3::exceptions::PyFileNotFoundError, _>(
                format!("å·¥ä½œè¿›ç¨‹è„šæœ¬ä¸å­˜åœ¨: {:?}", script_path)
            ));
        }
            
        // åˆ›å»ºPythonå·¥ä½œè¿›ç¨‹
        let mut child = Command::new(python_path)
            .arg(script_path)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .spawn()
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                format!("å¯åŠ¨å·¥ä½œè¿›ç¨‹å¤±è´¥: {}", e)
            ))?;

        let stdin = child.stdin.take().ok_or_else(|| {
            PyErr::new::<pyo3::exceptions::PyRuntimeError, _>("æ— æ³•è·å–è¿›ç¨‹stdin")
        })?;

        let stdout = child.stdout.take().ok_or_else(|| {
            PyErr::new::<pyo3::exceptions::PyRuntimeError, _>("æ— æ³•è·å–è¿›ç¨‹stdout")
        })?;

        let stdout_reader = BufReader::new(stdout);

        let worker = WorkerProcess {
            child,
            stdin,
            stdout_reader,
            id,
        };

        // ç»™å·¥ä½œè¿›ç¨‹ä¸€äº›æ—¶é—´å¯åŠ¨
        std::thread::sleep(std::time::Duration::from_millis(100));

        Ok(worker)
    }

    /// å‘å·¥ä½œè¿›ç¨‹å‘é€æŒ‡ä»¤
    pub fn send_command(&mut self, command: &WorkerCommand) -> PyResult<()> {
        match self.child.try_wait() {
            Ok(Some(status)) => {
                return Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                    format!("å·¥ä½œè¿›ç¨‹å·²é€€å‡ºï¼ŒçŠ¶æ€ç : {:?}", status)
                ));
            }
            Ok(None) => {
                // è¿›ç¨‹ä»åœ¨è¿è¡Œï¼Œç»§ç»­
            }
            Err(e) => {
                return Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                    format!("æ£€æŸ¥è¿›ç¨‹çŠ¶æ€å¤±è´¥: {}", e)
                ));
            }
        }

        let json_command = serde_json::to_string(command)
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                format!("åºåˆ—åŒ–æŒ‡ä»¤å¤±è´¥: {}", e)
            ))?;

        if let Err(e) = writeln!(self.stdin, "{}", json_command) {
            // å°è¯•è·å–è¿›ç¨‹çš„stderrä¿¡æ¯
            let mut stderr_output = String::new();
            if let Some(ref mut stderr) = self.child.stderr.as_mut() {
                use std::io::Read;
                let _ = stderr.read_to_string(&mut stderr_output);
            }
            
            return Err(PyErr::new::<pyo3::exceptions::PyIOError, _>(
                format!("å‘å·¥ä½œè¿›ç¨‹ {} å†™å…¥å¤±è´¥: {}. Stderr: {}", self.id, e, stderr_output)
            ));
        }
        Ok(())
    }

    /// ä»å·¥ä½œè¿›ç¨‹æ¥æ”¶é€šç”¨å“åº”
    fn receive_response<T: for<'de> serde::Deserialize<'de>>(&mut self) -> PyResult<T> {
        let mut line = String::new();
        // è®¾ç½®è¯»å–è¶…æ—¶
        // self.stdout_reader.get_mut().set_read_timeout(Some(std::time::Duration::from_secs(60)))
        //     .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(format!("è®¾ç½®è¯»å–è¶…æ—¶å¤±è´¥: {}", e)))?;

        self.stdout_reader.read_line(&mut line)
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(
                format!("ä»å·¥ä½œè¿›ç¨‹ {} è¯»å–å¤±è´¥: {}", self.id, e)
            ))?;
        
        if line.is_empty() {
            return Err(PyErr::new::<pyo3::exceptions::PyIOError, _>(
                format!("ä»å·¥ä½œè¿›ç¨‹ {} è¯»å–åˆ°ç©ºè¡Œï¼Œå¯èƒ½å·²é€€å‡º", self.id)
            ));
        }
        
        serde_json::from_str(&line.trim())
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                format!("ååºåˆ—åŒ–æ¥è‡ªå·¥ä½œè¿›ç¨‹ {} çš„å“åº”å¤±è´¥: {}. å“åº”å†…å®¹: '{}'", self.id, e, line.trim())
            ))
    }

    /// ä»å·¥ä½œè¿›ç¨‹æ¥æ”¶ç»“æœ
    pub fn receive_result(&mut self) -> PyResult<WorkerResponse> {
        self.receive_response::<WorkerResponse>()
    }

    /// pingå·¥ä½œè¿›ç¨‹
    pub fn ping(&mut self) -> PyResult<()> {
        self.send_command(&WorkerCommand::Ping {})?;
        let response: PingResponse = self.receive_response::<PingResponse>()?;
        if response.status == "pong" {
            Ok(())
        } else {
            Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                "Ping å¤±è´¥ï¼Œæ”¶åˆ°æœªçŸ¥å“åº”"
            ))
        }
    }

    /// ç»ˆæ­¢å·¥ä½œè¿›ç¨‹
    pub fn terminate(&mut self) -> PyResult<()> {
        // é¦–å…ˆå°è¯•ä¼˜é›…å…³é—­ï¼Œå¿½ç•¥å‘é€é”™è¯¯ï¼ˆå› ä¸ºè¿›ç¨‹å¯èƒ½å·²ç»å…³é—­ï¼‰
        let _ = self.send_command(&WorkerCommand::Exit {});
        
        // ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©è¿›ç¨‹è‡ªå·±é€€å‡º
        std::thread::sleep(std::time::Duration::from_millis(100));
        
        // å¦‚æœè¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œå¼ºåˆ¶æ€æ­»
        match self.child.try_wait() {
            Ok(Some(_)) => {
                // è¿›ç¨‹å·²ç»é€€å‡º
            }
            Ok(None) => {
                // è¿›ç¨‹ä»åœ¨è¿è¡Œï¼Œå¼ºåˆ¶æ€æ­»
                let _ = self.child.kill();
                let _ = self.child.wait();
            }
            Err(_) => {
                // æ£€æŸ¥çŠ¶æ€å¤±è´¥ï¼Œç›´æ¥æ€æ­»
                let _ = self.child.kill();
                let _ = self.child.wait();
            }
        }
        
        Ok(())
    }
}

/// å¤šè¿›ç¨‹æ± ç®¡ç†å™¨
pub struct ProcessPool {
    workers: Vec<WorkerProcess>,
    num_processes: usize,
    python_path: String,
}

impl ProcessPool {
    /// åˆ›å»ºæ–°çš„è¿›ç¨‹æ± 
    pub fn new(num_processes: usize, python_path: &str) -> PyResult<Self> {
        let mut workers = Vec::new();
        
        println!("åˆ›å»º {} ä¸ªå·¥ä½œè¿›ç¨‹...", num_processes);
        
        for i in 0..num_processes {
            let worker = WorkerProcess::new(i, python_path)?;
            workers.push(worker);
        }
        
        println!("è¿›ç¨‹æ± åˆ›å»ºå®Œæˆ");
        
        Ok(ProcessPool {
            workers,
            num_processes,
            python_path: python_path.to_string(),
        })
    }

    /// å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    pub fn execute_tasks_async(
        &mut self,
        _py: Python,
        function_code: &str,
        tasks: Vec<Task>,
        go_class_serialized: Option<String>,
        progress_callback: Option<&PyAny>,
        backup_sender: Option<Sender<ProcessResult>>,
    ) -> PyResult<Vec<ProcessResult>> {
        let total_tasks = tasks.len();
        if total_tasks == 0 {
            return Ok(Vec::new());
        }

        println!("å¼€å§‹å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œï¼Œæ€»ä»»åŠ¡æ•°: {}", total_tasks);
        let start_time = Instant::now();

        // åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœæ”¶é›†é€šé“
        let (task_sender, task_receiver): (CrossbeamSender<Task>, CrossbeamReceiver<Task>) = unbounded();
        let (result_sender, result_receiver) = channel::<(usize, ProcessResult)>();
        
        // å°†æ‰€æœ‰ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        for task in tasks {
            task_sender.send(task).unwrap();
        }
        drop(task_sender); // å…³é—­å‘é€ç«¯ï¼Œè¡¨ç¤ºæ²¡æœ‰æ›´å¤šä»»åŠ¡

        // å…±äº«çŠ¶æ€
        let state = Arc::new(Mutex::new(TaskDispatcherState {
            total_tasks,
            completed_tasks: 0,
            dispatched_tasks: 0,
        }));

        // 1. åˆå§‹åŒ–æ‰€æœ‰å·¥ä½œè¿›ç¨‹
        for (i, worker) in &mut self.workers.iter_mut().enumerate() {
            if let Err(e) = worker.ping() {
                return Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                    format!("å·¥ä½œè¿›ç¨‹ {} æ— å“åº”: {}", i, e)
                ));
            }
            worker.send_command(&WorkerCommand::FunctionCode(function_code.to_string()))?;
            if let Some(go_ser) = &go_class_serialized {
                worker.send_command(&WorkerCommand::GoClass(go_ser.clone()))?;
            }
        }

        // 2. å¯åŠ¨å·¥ä½œè¿›ç¨‹çº¿ç¨‹
        let workers_drained: Vec<_> = self.workers.drain(..).collect();
        let mut worker_handles = Vec::new();

        for mut worker in workers_drained {
            let task_receiver = task_receiver.clone();
            let result_sender = result_sender.clone();
            let state = Arc::clone(&state);
            
            let handle = thread::spawn(move || {
                Self::worker_loop(worker.id, &mut worker, task_receiver, result_sender, state)
            });
            worker_handles.push(handle);
        }

        // 3. å¯åŠ¨ç»“æœæ”¶é›†å’Œè¿›åº¦æ›´æ–°çº¿ç¨‹
        let results = Arc::new(Mutex::new(Vec::new()));
        let results_clone = Arc::clone(&results);
        let state_clone = Arc::clone(&state);
        
        let progress_handle = if progress_callback.is_some() {
            let cb_ptr = progress_callback.unwrap() as *const PyAny;
            Some(thread::spawn(move || {
                Self::progress_update_loop(cb_ptr, state_clone, start_time);
            }))
        } else {
            None
        };

        let collection_handle = thread::spawn(move || {
            Self::result_collection_loop(result_receiver, results_clone, backup_sender, state, total_tasks)
        });

        // 4. ç­‰å¾…æ‰€æœ‰å·¥ä½œå®Œæˆ
        for handle in worker_handles {
            let _ = handle.join();
        }
        
        // ç­‰å¾…ç»“æœæ”¶é›†å®Œæˆ
        let _ = collection_handle.join();
        
        // åœæ­¢è¿›åº¦æ›´æ–°
        if let Some(handle) = progress_handle {
            let _ = handle.join();
        }

        // é‡å»ºè¿›ç¨‹æ± 
        for i in 0..self.num_processes {
            let new_worker = WorkerProcess::new(i, &self.python_path)?;
            self.workers.push(new_worker);
        }

        let final_results = Arc::try_unwrap(results).unwrap().into_inner().unwrap();
        println!("å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼Œæ€»è€—æ—¶: {:.2}ç§’", start_time.elapsed().as_secs_f64());
        
        Ok(final_results)
    }

    /// å·¥ä½œè¿›ç¨‹å¾ªç¯ï¼šæŒç»­ä»é˜Ÿåˆ—è·å–ä»»åŠ¡å¹¶å¤„ç†
    fn worker_loop(
        worker_id: usize,
        worker: &mut WorkerProcess,
        task_receiver: CrossbeamReceiver<Task>,
        result_sender: Sender<(usize, ProcessResult)>,
        state: Arc<Mutex<TaskDispatcherState>>,
    ) {
        loop {
            // ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
            match task_receiver.recv() {
                Ok(task) => {
                    // æ›´æ–°åˆ†å‘è®¡æ•°
                    {
                        let mut state = state.lock().unwrap();
                        state.dispatched_tasks += 1;
                    }

                    // å‘é€ä»»åŠ¡ç»™å·¥ä½œè¿›ç¨‹
                    if let Err(e) = worker.send_command(&WorkerCommand::Task(task.clone())) {
                        eprintln!("å·¥ä½œè¿›ç¨‹ {} å‘é€ä»»åŠ¡å¤±è´¥: {}", worker_id, e);
                        break;
                    }
                    
                    if let Err(e) = worker.send_command(&WorkerCommand::Execute {}) {
                        eprintln!("å·¥ä½œè¿›ç¨‹ {} å‘é€æ‰§è¡ŒæŒ‡ä»¤å¤±è´¥: {}", worker_id, e);
                        break;
                    }

                    // æ¥æ”¶ç»“æœ
                    match worker.receive_result() {
                        Ok(response) => {
                            if !response.errors.is_empty() {
                                for error_msg in response.errors {
                                    eprintln!("å·¥ä½œè¿›ç¨‹ {} è¿”å›é”™è¯¯: {}", worker_id, error_msg);
                                }
                            }

                            // å¤„ç†ç»“æœï¼ˆåº”è¯¥åªæœ‰ä¸€ä¸ªç»“æœï¼Œå› ä¸ºæˆ‘ä»¬ä¸€æ¬¡åªå‘é€ä¸€ä¸ªä»»åŠ¡ï¼‰
                            if let Some(facs) = response.results.into_iter().next() {
                                let now = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs() as i64;
                                let result = ProcessResult {
                                    date: task.date,
                                    code: task.code.clone(),
                                    timestamp: now,
                                    facs,
                                };
                                
                                if result_sender.send((worker_id, result)).is_err() {
                                    eprintln!("å·¥ä½œè¿›ç¨‹ {} å‘é€ç»“æœå¤±è´¥ï¼Œç»“æœæ”¶é›†å™¨å¯èƒ½å·²å…³é—­", worker_id);
                                    break;
                                }
                            }
                        }
                        Err(e) => {
                            eprintln!("å·¥ä½œè¿›ç¨‹ {} æ¥æ”¶ç»“æœå¤±è´¥: {}", worker_id, e);
                            break;
                        }
                    }
                }
                Err(_) => {
                    // ä»»åŠ¡é˜Ÿåˆ—å·²å…³é—­ï¼Œé€€å‡ºå¾ªç¯
                    break;
                }
            }
        }
        
        // æ¸…ç†å·¥ä½œè¿›ç¨‹
        let _ = worker.terminate();
        println!("å·¥ä½œè¿›ç¨‹ {} å·²é€€å‡º", worker_id);
    }

    /// ç»“æœæ”¶é›†å¾ªç¯
    fn result_collection_loop(
        result_receiver: Receiver<(usize, ProcessResult)>,
        results: Arc<Mutex<Vec<ProcessResult>>>,
        backup_sender: Option<Sender<ProcessResult>>,
        state: Arc<Mutex<TaskDispatcherState>>,
        total_tasks: usize,
    ) {
        while let Ok((_worker_id, result)) = result_receiver.recv() {
            // å‘é€åˆ°å¤‡ä»½çº¿ç¨‹
            if let Some(ref sender) = backup_sender {
                let _ = sender.send(result.clone());
            }

            // å­˜å‚¨ç»“æœ
            {
                let mut results = results.lock().unwrap();
                results.push(result);
            }

            // æ›´æ–°å®Œæˆè®¡æ•°
            let completed = {
                let mut state = state.lock().unwrap();
                state.completed_tasks += 1;
                state.completed_tasks
            };

            // å¦‚æœæ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†ï¼Œé€€å‡º
            if completed >= total_tasks {
                break;
            }
        }
    }

    /// è¿›åº¦æ›´æ–°å¾ªç¯
    fn progress_update_loop(
        callback_ptr: *const PyAny,
        state: Arc<Mutex<TaskDispatcherState>>,
        start_time: Instant,
    ) {
        let mut last_update_time = Instant::now();
        
        loop {
            thread::sleep(std::time::Duration::from_millis(500)); // æ¯500msæ›´æ–°ä¸€æ¬¡
            
            let (completed, total, finished) = {
                let state = state.lock().unwrap();
                (state.completed_tasks, state.total_tasks, state.completed_tasks >= state.total_tasks)
            };

            if finished {
                break;
            }

            // é™åˆ¶æ›´æ–°é¢‘ç‡
            if last_update_time.elapsed().as_millis() < 500 {
                continue;
            }
            last_update_time = Instant::now();

            let elapsed = start_time.elapsed().as_secs_f64();
            let estimated_remaining = if completed > 0 {
                (elapsed / completed as f64) * (total - completed) as f64
            } else {
                0.0
            };

            // å®‰å…¨åœ°è°ƒç”¨Pythonå›è°ƒ
            Python::with_gil(|py| {
                unsafe {
                    let callback = &*callback_ptr;
                    let progress_info: Vec<PyObject> = vec![
                        completed.to_object(py),
                        total.to_object(py),
                        estimated_remaining.to_object(py),
                        elapsed.to_object(py),
                    ];
                    
                    if let Err(e) = callback.call_method1("update", (progress_info,)) {
                        eprintln!("è°ƒç”¨è¿›åº¦å›è°ƒå¤±è´¥: {}", e);
                    }
                }
            });
        }
    }

impl Drop for ProcessPool {
    fn drop(&mut self) {
        // ç»ˆæ­¢æ‰€æœ‰å·¥ä½œè¿›ç¨‹
        for worker in &mut self.workers {
            let _ = worker.terminate();
        }
    }
}

/// å¤šè¿›ç¨‹æ‰§è¡Œé…ç½®
#[derive(Clone)]
pub struct MultiProcessConfig {
    pub num_processes: Option<usize>,
    pub backup_batch_size: usize,
    pub backup_file: Option<String>,
    pub storage_format: String,
    pub resume_from_backup: bool,
    pub python_path: String,
}

impl Default for MultiProcessConfig {
    fn default() -> Self {
        Self {
            num_processes: None,
            backup_batch_size: 1000,
            backup_file: None,
            storage_format: "binary".to_string(),
            resume_from_backup: false,
            python_path: "/home/chenzongwei/.conda/envs/chenzongwei311/bin/python".to_string(),
        }
    }
}

/// å¤šè¿›ç¨‹æ‰§è¡Œå™¨
pub struct MultiProcessExecutor {
    config: MultiProcessConfig,
    backup_manager: Option<BackupManager>,
}

impl MultiProcessExecutor {
    pub fn new(config: MultiProcessConfig) -> PyResult<Self> {
        let backup_manager = if let Some(backup_file) = &config.backup_file {
            Some(BackupManager::new(backup_file, &config.storage_format)?)
        } else {
            None
        };

        Ok(Self {
            config,
            backup_manager,
        })
    }

    /// æå–å‡½æ•°ä»£ç 
    fn extract_function_code(&self, py: Python, func: &PyAny) -> PyResult<String> {
        println!("æ­£åœ¨æå–å‡½æ•°ä»£ç ...");
        
        let inspect = py.import("inspect")?;
        
        match inspect.call_method1("getsource", (func,)) {
            Ok(source) => {
                let source_str: String = source.extract()?;
                println!("âœ… æˆåŠŸè·å–å‡½æ•°æºä»£ç ï¼Œé•¿åº¦: {} å­—ç¬¦", source_str.len());
                
                if source_str.trim().is_empty() {
                    return Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                        "å‡½æ•°æºä»£ç ä¸ºç©º"
                    ));
                }
                
                Ok(source_str)
            }
            Err(e) => {
                println!("âš ï¸ æ— æ³•è·å–å‡½æ•°æºä»£ç : {}", e);
                
                let func_name = func.getattr("__name__")
                    .and_then(|name| name.extract::<String>())
                    .unwrap_or_else(|_| "user_function".to_string());
                
                println!("ğŸ“ åˆ›å»ºå‡½æ•°åŒ…è£…ï¼Œå‡½æ•°å: {}", func_name);
                
                match py.import("dill") {
                    Ok(dill) => {
                        let serialized = dill.call_method1("dumps", (func,))?;
                        let bytes: Vec<u8> = serialized.extract()?;
                        use base64::Engine;
                        let encoded = base64::engine::general_purpose::STANDARD.encode(bytes);
                        println!("âœ… æˆåŠŸä½¿ç”¨dillåºåˆ—åŒ–ï¼Œé•¿åº¦: {} å­—ç¬¦", encoded.len());
                        Ok(encoded)
                    }
                    Err(_) => {
                        Err(PyErr::new::<pyo3::exceptions::PyRuntimeError, _>(
                            format!("æ— æ³•åºåˆ—åŒ–å‡½æ•° '{}'ï¼Œè¯·ç¡®ä¿å‡½æ•°æ˜¯å…¨å±€å®šä¹‰çš„æˆ–å®‰è£…dillåº“", func_name)
                        ))
                    }
                }
            }
        }
    }

    /// ä¸»è¦çš„å¤šè¿›ç¨‹æ‰§è¡Œå‡½æ•°
    pub fn run_multiprocess(
        &mut self,
        py: Python,
        func: &PyAny,
        args: Vec<(i32, String)>,
        go_class: Option<&PyAny>,
        progress_callback: Option<&PyAny>,
        _chunk_size: Option<usize>, // å¼‚æ­¥æ¨¡å¼ä¸‹ä¸å†ä½¿ç”¨chunk_size
    ) -> PyResult<Vec<ReturnResult>> {
        let total_tasks = args.len();
        println!("å¼€å§‹å¤šè¿›ç¨‹æ‰§è¡Œï¼Œæ€»ä»»åŠ¡æ•°: {}", total_tasks);

        // å°†å‚æ•°è½¬æ¢ä¸ºTaskç»“æ„
        let tasks: Vec<Task> = args.into_iter().map(|(date, code)| Task { date, code }).collect();

        // æ£€æŸ¥æ˜¯å¦éœ€è¦ä»å¤‡ä»½æ¢å¤
        let (remaining_tasks, existing_results) = if self.config.resume_from_backup {
            self.load_existing_results(&tasks)?
        } else {
            (tasks, Vec::new())
        };

        let remaining_count = remaining_tasks.len();
        println!("éœ€è¦è®¡ç®—çš„ä»»åŠ¡æ•°: {}", remaining_count);

        if remaining_count == 0 {
            return Ok(existing_results.into_iter().map(|r| r.to_return_result()).collect());
        }

        // å¤‡ä»½çº¿ç¨‹è®¾ç½®
        let (backup_sender, backup_receiver) = if self.backup_manager.is_some() {
            let (tx, rx) = channel::<ProcessResult>();
            (Some(tx), Some(rx))
        } else {
            (None, None)
        };

        let backup_handle = if let Some(receiver) = backup_receiver {
            if let Some(backup_manager) = self.backup_manager.take() {
                let batch_size = self.config.backup_batch_size;
                Some(thread::spawn(move || {
                    Self::backup_worker(backup_manager, receiver, batch_size);
                }))
            } else { None }
        } else { None };

        // åˆ›å»ºè¿›ç¨‹æ± 
        let num_processes = self.config.num_processes.unwrap_or_else(|| {
            std::thread::available_parallelism().map(|n| n.get()).unwrap_or(4)
        });
        let mut process_pool = ProcessPool::new(num_processes, &self.config.python_path)?;

        // æå–å’Œåºåˆ—åŒ–ä»£ç /å¯¹è±¡
        let function_code = self.extract_function_code(py, func)?;
        let go_class_serialized = if let Some(go_class) = go_class {
            py.import("dill").ok().and_then(|dill| {
                dill.call_method1("dumps", (go_class,)).ok().and_then(|s| {
                    s.extract::<Vec<u8>>().ok().map(|bytes| {
                        use base64::Engine;
                        base64::engine::general_purpose::STANDARD.encode(bytes)
                    })
                })
            })
        } else { None };
        
        // å¼‚æ­¥æµæ°´çº¿æ‰§è¡Œ
        let async_results = match process_pool.execute_tasks_async(
            py,
            &function_code,
            remaining_tasks,
            go_class_serialized,
            progress_callback,
            backup_sender.clone(),
        ) {
            Ok(res) => res,
            Err(e) => {
                if let Some(cb) = progress_callback {
                    Python::with_gil(|_py| {
                        let _ = cb.call_method1("set_error", (e.to_string(),));
                    });
                }
                return Err(e);
            }
        };

        // åˆå¹¶ç»“æœ
        let mut all_results = existing_results;
        all_results.extend(async_results);

        // æ ‡è®°ä»»åŠ¡å®Œæˆ
        if let Some(cb) = progress_callback {
            Python::with_gil(|_py| {
                let _ = cb.call_method0("finish");
            });
        }

        // ç­‰å¾…å¤‡ä»½å®Œæˆ
        if let Some(sender) = backup_sender {
            drop(sender);
        }
        if let Some(handle) = backup_handle {
            let _ = handle.join();
        }

        println!("å¤šè¿›ç¨‹æ‰§è¡Œå®Œæˆï¼Œæ€»ä»»åŠ¡æ•°: {}", total_tasks);

        Ok(all_results.into_iter().map(|r| r.to_return_result()).collect())
    }

    /// å¤‡ä»½å·¥ä½œçº¿ç¨‹
    fn backup_worker(
        mut backup_manager: BackupManager,
        receiver: Receiver<ProcessResult>,
        batch_size: usize,
    ) {
        let mut batch = Vec::with_capacity(batch_size);
        
        loop {
            match receiver.recv() {
                Ok(result) => {
                    // è½¬æ¢ä¸ºComputeResult
                    let compute_result = crate::parallel::ComputeResult {
                        date: result.date,
                        code: result.code,
                        timestamp: result.timestamp,
                        facs: result.facs,
                    };
                    batch.push(compute_result);
                    
                    if batch.len() >= batch_size {
                        if let Err(e) = backup_manager.save_batch(&batch) {
                            eprintln!("å¤‡ä»½å¤±è´¥: {}", e);
                        }
                        batch.clear();
                    }
                }
                Err(_) => {
                    // é€šé“å…³é—­ï¼Œä¿å­˜å‰©ä½™æ•°æ®
                    if !batch.is_empty() {
                        if let Err(e) = backup_manager.save_batch(&batch) {
                            eprintln!("æœ€ç»ˆå¤‡ä»½å¤±è´¥: {}", e);
                        }
                    }
                    break;
                }
            }
        }
    }

    /// åŠ è½½å·²æœ‰ç»“æœ
    fn load_existing_results(
        &self,
        tasks: &[Task],
    ) -> PyResult<(Vec<Task>, Vec<ProcessResult>)> {
        if let Some(backup_manager) = &self.backup_manager {
            // è½¬æ¢ä¸º(i32, String)æ ¼å¼ä»¥å…¼å®¹ç°æœ‰çš„å¤‡ä»½ç®¡ç†å™¨
            let args: Vec<(i32, String)> = tasks.iter()
                .map(|task| (task.date, task.code.clone()))
                .collect();
                
            let existing = backup_manager.load_existing_results(&args)?;
            let existing_keys: std::collections::HashSet<(i32, String)> = 
                existing.iter().map(|r| (r.date, r.code.clone())).collect();
            
            let remaining: Vec<Task> = tasks
                .iter()
                .filter(|task| !existing_keys.contains(&(task.date, task.code.clone())))
                .cloned()
                .collect();

            // è¾“å‡ºå¤‡ä»½ä¿¡æ¯
            if !existing.is_empty() && !remaining.is_empty() {
                let latest_backup_date = existing.iter().map(|r| r.date).max().unwrap_or(0);
                let earliest_remaining_date = remaining.iter().map(|t| t.date).min().unwrap_or(0);
                println!("å¤‡ä»½ä¸­æœ€æ™šæ—¥æœŸä¸º{}ï¼Œå³å°†ä»{}æ—¥æœŸå¼€å§‹è®¡ç®—", latest_backup_date, earliest_remaining_date);
            }

            // è½¬æ¢ä¸ºProcessResult
            let process_results: Vec<ProcessResult> = existing.into_iter()
                .map(|r| ProcessResult {
                    date: r.date,
                    code: r.code,
                    timestamp: r.timestamp,
                    facs: r.facs,
                })
                .collect();

            Ok((remaining, process_results))
        } else {
            Ok((tasks.to_vec(), Vec::new()))
        }
    }
}
}