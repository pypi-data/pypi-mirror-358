{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sparkenforce import Dataset, DatasetValidationError, validate\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"James\", \"\", \"Smith\", \"1991-04-01\", 3000),\n",
    "    (\"Michael\", \"Rose\", \"\", \"2000-05-19\", 4000),\n",
    "    (\"Robert\", \"\", \"Williams\", \"1978-09-05\", 4000),\n",
    "    (\"Maria\", \"Anne\", \"Jones\", \"1967-12-01\", 4000),\n",
    "    (\"Jen\", \"Mary\", \"Brown\", \"1980-02-17\", -1),\n",
    "]\n",
    "\n",
    "columns = [\"firstname\", \"middlename\", \"lastname\", \"dob\", \"salary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+\n",
      "|firstname|middlename|lastname|       dob|salary|\n",
      "+---------+----------+--------+----------+------+\n",
      "|    James|          |   Smith|1991-04-01|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|    -1|\n",
      "+---------+----------+--------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Return Value Validation Demonstration\n",
    "\n",
    "This new functionality allows validating that the value returned by a function matches the specified Dataset type annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation successful!\n",
      "+-------+------+\n",
      "|   name|length|\n",
      "+-------+------+\n",
      "|  James|     5|\n",
      "|Michael|     7|\n",
      "| Robert|     6|\n",
      "|  Maria|     5|\n",
      "|    Jen|     3|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Example 1: Successful return value validation\n",
    "from pyspark.sql import functions as fn\n",
    "\n",
    "\n",
    "@validate\n",
    "def transform_data(\n",
    "    df: Dataset[\"firstname\":str, ...],\n",
    ") -> Dataset[\"name\":str, \"length\":int]:\n",
    "    \"\"\"Function that validates both input and output.\"\"\"\n",
    "    return df.select(df.firstname.alias(\"name\"), fn.length(df.firstname).alias(\"length\"))\n",
    "\n",
    "\n",
    "# This should work correctly\n",
    "try:\n",
    "    result = transform_data(df)\n",
    "    print(\"✅ Validation successful!\")\n",
    "    result.show()\n",
    "except DatasetValidationError as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expected error in return validation:\n",
      "return value columns mismatch. Expected exactly {'length', 'name'}, got {'firstname', 'lastname'}. missing columns: {'name', 'length'}, unexpected columns: {'firstname', 'lastname'}\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Error due to incorrect schema in return value\n",
    "@validate\n",
    "def incorrect_return_schema(\n",
    "    df: Dataset[\"firstname\":str, ...],\n",
    ") -> Dataset[\"name\":str, \"length\":int]:\n",
    "    \"\"\"Function that returns an incorrect schema.\"\"\"\n",
    "    return df.select(\"firstname\", \"lastname\")  # Incorrect columns\n",
    "\n",
    "\n",
    "# This should fail\n",
    "try:\n",
    "    result = incorrect_return_schema(df)\n",
    "    print(\"❌ Should not reach here\")\n",
    "except DatasetValidationError as e:\n",
    "    print(\"✅ Expected error in return validation:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expected error - not DataFrame:\n",
      "return value must be a PySpark DataFrame, got <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Error due to returning incorrect type (not DataFrame)\n",
    "@validate\n",
    "def non_dataframe_return(df: Dataset[\"firstname\":str, ...]) -> Dataset[\"result\":str]:\n",
    "    \"\"\"Function that returns something that is not a DataFrame.\"\"\"\n",
    "    return \"Not a DataFrame\"\n",
    "\n",
    "\n",
    "# This should fail\n",
    "try:\n",
    "    result = non_dataframe_return(df)\n",
    "    print(\"❌ Should not reach here\")\n",
    "except DatasetValidationError as e:\n",
    "    print(\"✅ Expected error - not DataFrame:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No annotation: Can return anything\n",
      "✅ With explicit None: 42\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Backward compatibility - no return annotation\n",
    "@validate\n",
    "def no_return_annotation(df: Dataset[\"firstname\":str, ...]):\n",
    "    \"\"\"Function without return annotation - return is not validated.\"\"\"\n",
    "    return \"Can return anything\"\n",
    "\n",
    "\n",
    "@validate\n",
    "def explicit_none_return(df: Dataset[\"firstname\":str, ...]) -> None:\n",
    "    \"\"\"Function with explicit None return - not validated.\"\"\"\n",
    "    return 42\n",
    "\n",
    "\n",
    "# Both should work without return validation\n",
    "try:\n",
    "    result1 = no_return_annotation(df)\n",
    "    result2 = explicit_none_return(df)\n",
    "    print(f\"✅ No annotation: {result1}\")\n",
    "    print(f\"✅ With explicit None: {result2}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ellipsis validation successful!\n",
      "+---------+---------+--------+\n",
      "|firstname|  summary|lastname|\n",
      "+---------+---------+--------+\n",
      "|    James|processed|   Smith|\n",
      "|  Michael|processed|        |\n",
      "|   Robert|processed|Williams|\n",
      "+---------+---------+--------+\n",
      "only showing top 3 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Example 5: Validation with ellipsis (minimum columns)\n",
    "@validate\n",
    "def ellipsis_return_example(\n",
    "    df: Dataset[\"firstname\":str, ...],\n",
    ") -> Dataset[\"firstname\":str, \"summary\":str, ...]:\n",
    "    \"\"\"Function that allows additional columns in the return.\"\"\"\n",
    "    # Add additional columns (allowed with ellipsis)\n",
    "    return df.select(\n",
    "        \"firstname\",\n",
    "        fn.lit(\"processed\").alias(\"summary\"),\n",
    "        \"lastname\",  # Additional column allowed\n",
    "        \"salary\",  # Another additional column allowed\n",
    "    )\n",
    "\n",
    "\n",
    "# This should work correctly\n",
    "try:\n",
    "    result = ellipsis_return_example(df)\n",
    "    print(\"✅ Ellipsis validation successful!\")\n",
    "    result.select(\"firstname\", \"summary\", \"lastname\").show(3)\n",
    "except DatasetValidationError as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
