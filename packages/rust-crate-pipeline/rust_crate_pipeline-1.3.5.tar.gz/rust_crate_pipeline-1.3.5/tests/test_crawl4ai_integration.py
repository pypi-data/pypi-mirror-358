#!/usr/bin/env python3
"""
Comprehensive Crawl4AI Integration Test Suite
Tests all aspects of Crawl4AI integration with the Rust Crate Pipeline
"""

import asyncio
import os
import sys
from typing import Callable, Dict, List, Tuple

# Add the workspace root to Python path for module imports
workspace_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, workspace_root)


def test_enhanced_scraping_initialization() -> bool:
    """Test the enhanced scraping module initialization."""
    print("ğŸ§ª Testing Enhanced Scraping Module Initialization...")
    try:
        from enhanced_scraping import (
            CrateDocumentationScraper,
            EnhancedScraper,
        )

        print("âœ… Enhanced scraping imports successful")

        # Initialization should succeed if Crawl4AI is installed
        scraper = EnhancedScraper()
        print("âœ… EnhancedScraper initialized successfully.")
        assert scraper is not None

        crate_scraper = CrateDocumentationScraper()
        print("âœ… CrateDocumentationScraper initialized successfully.")
        assert crate_scraper is not None

        return True
    except Exception as e:
        print(f"âŒ Enhanced Scraping Module initialization failed: {e}")
        return False


def test_pipeline_config_integration() -> bool:
    """Test Crawl4AI integration in pipeline configuration."""
    print("\nğŸ§ª Testing Pipeline Configuration Integration...")
    try:
        from rust_crate_pipeline.config import PipelineConfig

        # Default config should have Crawl4AI enabled
        config = PipelineConfig()
        assert config.enable_crawl4ai is True
        print("âœ… Default PipelineConfig has Crawl4AI enabled.")

        # Config with specific model
        model_path = "/fake/path/to/model.gguf"
        config = PipelineConfig(crawl4ai_model=model_path)
        assert config.crawl4ai_model == model_path
        print("âœ… PipelineConfig with custom Crawl4AI model created.")

        return True
    except Exception as e:
        print(f"âŒ Pipeline Configuration Integration failed: {e}")
        return False


def test_cli_integration() -> bool:
    """Test CLI integration with Crawl4AI options."""
    print("\nğŸ§ª Testing CLI Integration...")
    try:
        from rust_crate_pipeline.main import parse_arguments

        # Test --disable-crawl4ai flag
        test_args_disable = ["--disable-crawl4ai", "--limit", "1"]
        original_argv = sys.argv
        sys.argv = ["main.py"] + test_args_disable
        try:
            args = parse_arguments()
            assert args.disable_crawl4ai is True
            print("âœ… CLI parsing for --disable-crawl4ai successful.")
        finally:
            sys.argv = original_argv

        # Test --crawl4ai-model argument
        model_path = "/another/fake/model.gguf"
        test_args_model = ["--crawl4ai-model", model_path, "--limit", "1"]
        sys.argv = ["main.py"] + test_args_model
        try:
            args = parse_arguments()
            assert args.crawl4ai_model == model_path
            print("âœ… CLI parsing for --crawl4ai-model successful.")
        finally:
            sys.argv = original_argv

        return True
    except Exception as e:
        print(f"âŒ CLI Integration failed: {e}")
        return False


async def test_async_scraping_functionality() -> bool:
    """Test async scraping functionality with a live URL."""
    print("\nğŸ§ª Testing Async Scraping Functionality...")
    # This test requires a network connection and a valid GGUF model path.
    # It may be skipped in certain CI environments.
    model_path = os.path.expanduser(
        "~/models/deepseek/deepseek-coder-6.7b-instruct.Q4_K_M.gguf"
    )
    if not os.path.exists(model_path):
        print("âš ï¸  SKIPPING: GGUF model not found at", model_path)
        return True  # Skip test if model is not present

    try:
        from enhanced_scraping import EnhancedScraper, EnhancedScrapingResult

        # Use a well-known, stable URL for testing
        url = "https://docs.rs/serde/latest/serde/"
        print(f"Scraping URL: {url}")

        scraper = EnhancedScraper(llm_model=model_path)
        result = await scraper.scrape_documentation(url, doc_type="docs")
        await scraper.close()

        assert isinstance(result, EnhancedScrapingResult)
        assert result.error is None
        assert result.extraction_method == "crawl4ai"
        assert len(result.content) > 100
        assert result.quality_score > 0.5
        assert "serde" in result.title.lower()

        print("âœ… Async scraping successful:")
        print(f"   - Title: {result.title}")
        print(f"   - Quality Score: {result.quality_score:.2f}")
        print(f"   - Content Length: {len(result.content)}")
        return True
    except Exception as e:
        print(f"âŒ Async Scraping Functionality failed: {e}")
        return False


def main() -> bool:
    """Run all integration tests."""
    print("ğŸš€ Crawl4AI Integration Test Suite")
    print("=" * 50)

    # Define synchronous tests
    sync_tests: List[Tuple[str, Callable[[], bool]]] = [
        (
            "Enhanced Scraping Initialization",
            test_enhanced_scraping_initialization,
        ),
        (
            "Pipeline Configuration Integration",
            test_pipeline_config_integration,
        ),
        ("CLI Integration", test_cli_integration),
    ]

    results: Dict[str, bool] = {}
    all_passed = True

    # Run synchronous tests
    for test_name, test_func in sync_tests:
        try:
            result = test_func()
            results[test_name] = result
            if not result:
                all_passed = False
        except Exception as e:
            print(f"âŒ {test_name} crashed with exception: {e}")
            results[test_name] = False
            all_passed = False

    # Run asynchronous test
    print("\n" + "-" * 50)
    try:
        async_result = asyncio.run(test_async_scraping_functionality())
        results["Async Scraping Functionality"] = async_result
        if not async_result:
            all_passed = False
    except Exception as e:
        print(f"âŒ Async Scraping Functionality crashed with exception: {e}")
        results["Async Scraping Functionality"] = False
        all_passed = False

    # Print results summary
    print("\n" + "=" * 50)
    print("ğŸ¯ Test Results Summary:")
    for test_name, result in results.items():
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"   {status}: {test_name}")

    passed_count = sum(1 for result in results.values() if result)
    total_count = len(results)
    print(f"\nğŸ“Š Overall: {passed_count}/{total_count} tests passed")

    if all_passed:
        print("\nğŸ‰ All tests passed! Crawl4AI integration is robust.")
    else:
        print("\nâš ï¸  Some tests failed. Please review the errors above.")

    return all_passed


if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)
