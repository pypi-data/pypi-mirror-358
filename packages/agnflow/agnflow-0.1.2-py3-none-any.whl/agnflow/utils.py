"""

### TODO æ¸…å•

- [x] llm
- [x] memory
- [x] rag
- [x] mcp tool
- [x] ReAct(reasoning + action)
- [x] TAO(thought + action + observation)
- [x] ToT(Chain of Thought)
- [x] CoT(Chain of Thought)
- [x] hitl(human in the loop)
- [x] supervisor swarm

"""

from typing import Literal, List, Dict
import os
from datetime import datetime
import uuid
import yaml
import json
import requests
from dotenv import load_dotenv
from openai import OpenAI
from duckduckgo_search import DDGS
import numpy as np
from sklearn.neighbors import NearestNeighbors

load_dotenv()


def call_llm(user_prompt, system_prompt=None, model="glm-4-flashx-250414", output_format: Literal["yaml", "json", "text"] = "text"):
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    messages = [{"role": "user", "content": user_prompt}]
    if system_prompt:
        messages.insert(0, {"role": "system", "content": system_prompt})
    completion = client.chat.completions.create(model=model, messages=messages)
    res = completion.choices[0].message.content
    if output_format == "text":
        return res
    if output_format == "yaml":
        res = res.strip().removeprefix("```yaml").removesuffix("```").strip()
        return yaml.safe_load(res)
    elif output_format == "json":
        res = res.strip().removeprefix("```json").removesuffix("```").strip()
        return json.loads(res)
    raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_format}")


def search_web_duckduckgo(query):
    results = DDGS().text(query, max_results=5)
    # Convert results to a string
    results_str = "\n\n".join([f"Title: {r['title']}\nURL: {r['href']}\nSnippet: {r['body']}" for r in results])
    return results_str


def search_web_brave(query):

    url = f"https://api.search.brave.com/res/v1/web/search?q={query}"
    api_key = os.getenv("BRAVE_SEARCH_API_KEY")

    headers = {"accept": "application/json", "Accept-Encoding": "gzip", "x-subscription-token": api_key}

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        results = data['web']['results']
        results_str = "\n\n".join([f"Title: {r['title']}\nURL: {r['url']}\nDescription: {r['description']}" for r in results])
        return results_str
    else:
        print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")


def get_embedding(text):
    client = OpenAI(base_url=os.getenv("EMBEDDING_BASE_URL"), api_key=os.getenv("EMBEDDING_API_KEY"))

    response = client.embeddings.create(model=os.getenv("EMBEDDING_MODEL_NAME"), input=text)

    # ä»å“åº”ä¸­æå– embedding å‘é‡
    embedding = response.data[0].embedding

    # è½¬æ¢ä¸º numpy array ç”¨äºä¸€è‡´æ€§
    return np.array(embedding, dtype=np.float32)


def get_similarity(text1, text2):
    emb1 = get_embedding(text1)
    emb2 = get_embedding(text2)
    return np.dot(emb1, emb2)


# ==================== Memory åŠŸèƒ½ ====================


class ConversationMemory:
    """å¯¹è¯è®°å¿†ç®¡ç†ç±»"""

    def __init__(self, max_messages=10):
        self.messages = []
        self.max_messages = max_messages
        self.vector_index = None
        self.vector_items = []

    def add_message(self, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°è®°å¿†"""
        message = {"id": str(uuid.uuid4()), "role": role, "content": content, "timestamp": datetime.now().isoformat()}
        self.messages.append(message)

        # ä¿æŒæ¶ˆæ¯æ•°é‡åœ¨é™åˆ¶å†…
        if len(self.messages) > self.max_messages:
            self.messages.pop(0)

    def get_recent_messages(self, count=5):
        """è·å–æœ€è¿‘çš„æ¶ˆæ¯"""
        return self.messages[-count:] if len(self.messages) >= count else self.messages

    def get_context(self):
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡"""
        return [{"role": msg["role"], "content": msg["content"]} for msg in self.messages]

    def create_vector_index(self):
        """åˆ›å»ºå‘é‡ç´¢å¼•"""
        if not self.vector_items:
            return None

        embeddings = np.array([item["embedding"] for item in self.vector_items], dtype=np.float32)
        index = NearestNeighbors(n_neighbors=10, metric="cosine").fit(embeddings)

        return index

    def add_to_vector_store(self, conversation_pair, embedding):
        """æ·»åŠ å¯¹è¯å¯¹åˆ°å‘é‡å­˜å‚¨"""
        item = {"conversation": conversation_pair, "embedding": embedding}
        self.vector_items.append(item)
        self.vector_index = self.create_vector_index()

    def search_similar_conversations(self, query, k=1):
        """æœç´¢ç›¸ä¼¼å¯¹è¯"""
        if not self.vector_index or not self.vector_items:
            return []

        query_embedding = get_embedding(query)
        query_embedding = np.array([query_embedding], dtype=np.float32)

        # ä½¿ç”¨k_neighborsæ–¹æ³•ï¼Œé™åˆ¶è¿”å›çš„é‚»å±…æ•°é‡
        k = min(k, len(self.vector_items))
        distances, indices = self.vector_index.kneighbors(query_embedding, n_neighbors=k)

        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.vector_items):
                results.append({"conversation": self.vector_items[idx]["conversation"], "distance": distances[0][i]})

        return results


# ==================== RAG åŠŸèƒ½ ====================


class RAGSystem:
    """æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ"""

    def __init__(self):
        self.documents = []
        self.embeddings = None
        self.index = None

    def add_documents(self, texts: List[str]):
        """æ·»åŠ æ–‡æ¡£"""
        self.documents.extend(texts)

    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200):
        """å°†æ–‡æœ¬åˆ†å—"""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - overlap
        return chunks

    def create_embeddings(self):
        """ä¸ºæ‰€æœ‰æ–‡æ¡£åˆ›å»ºåµŒå…¥"""
        if not self.documents:
            return

        all_embeddings = []
        for doc in self.documents:
            embedding = get_embedding(doc)
            all_embeddings.append(embedding)

        self.embeddings = np.array(all_embeddings, dtype=np.float32)

    def build_index(self):
        """æ„å»ºæœç´¢ç´¢å¼•"""
        if self.embeddings is None:
            self.create_embeddings()

        if self.embeddings is None or len(self.embeddings) == 0:
            return

        self.index = NearestNeighbors(n_neighbors=10, metric="cosine").fit(self.embeddings)

    def retrieve(self, query: str, k: int = 3):
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if self.index is None:
            self.build_index()

        if self.index is None:
            return []

        query_embedding = get_embedding(query)
        query_embedding = np.array([query_embedding], dtype=np.float32)

        # ä½¿ç”¨k_neighborsæ–¹æ³•ï¼Œé™åˆ¶è¿”å›çš„é‚»å±…æ•°é‡
        k = min(k, len(self.documents))
        distances, indices = self.index.kneighbors(query_embedding, n_neighbors=k)

        results = []
        for i, idx in enumerate(indices[0]):
            if idx < len(self.documents):
                results.append({"document": self.documents[idx], "distance": distances[0][i], "index": idx})

        return results

    def generate_answer(self, query: str, retrieved_docs: List[Dict]):
        """åŸºäºæ£€ç´¢çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"""
        context = "\n\n".join([doc["document"] for doc in retrieved_docs])

        prompt = f"""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

ç­”æ¡ˆï¼š
"""

        return call_llm(prompt)


# ==================== MCP Tool åŠŸèƒ½ ====================


class MCPToolManager:
    """MCPå·¥å…·ç®¡ç†å™¨"""

    def __init__(self):
        self.tools = {}
        self.mcp_enabled = False

    def register_tool(self, name: str, description: str, function: callable, input_schema: Dict = None):
        """æ³¨å†Œå·¥å…·"""
        self.tools[name] = {"description": description, "function": function, "input_schema": input_schema or {}}

    def get_available_tools(self):
        """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
        return [{"name": name, "description": tool["description"], "input_schema": tool["input_schema"]} for name, tool in self.tools.items()]

    def call_tool(self, tool_name: str, arguments: Dict):
        """è°ƒç”¨å·¥å…·"""
        if tool_name not in self.tools:
            raise ValueError(f"å·¥å…· '{tool_name}' ä¸å­˜åœ¨")

        tool = self.tools[tool_name]
        return tool["function"](**arguments)

    def enable_mcp(self, server_script_path: str = None):
        """å¯ç”¨MCPæ”¯æŒ"""
        self.mcp_enabled = True
        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„MCPå®¢æˆ·ç«¯åˆå§‹åŒ–ä»£ç 
        print("MCPæ”¯æŒå·²å¯ç”¨")


# ==================== ReAct åŠŸèƒ½ ====================


class ReActAgent:
    """ReAct (Reasoning + Action) ä»£ç†"""

    def __init__(self, tools: MCPToolManager = None):
        self.tools = tools or MCPToolManager()
        self.thoughts = []
        self.actions = []
        self.observations = []

    def think(self, query: str):
        """æ€è€ƒé˜¶æ®µ"""
        context = self._build_context()

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªReActä»£ç†ï¼Œéœ€è¦è§£å†³ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

å¯ç”¨å·¥å…·ï¼š
{self._format_tools()}

å†å²è®°å½•ï¼š
{context}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡Œæ€è€ƒï¼š
```yaml
reasoning: |
  <ä½ çš„æ¨ç†è¿‡ç¨‹>
action: <è¦æ‰§è¡Œçš„åŠ¨ä½œåç§°>
action_input: <åŠ¨ä½œçš„è¾“å…¥å‚æ•°>
```

è¯·åŸºäºæ¨ç†é€‰æ‹©æœ€åˆé€‚çš„åŠ¨ä½œã€‚
"""

        response = call_llm(prompt, output_format="yaml")
        return response

    def act(self, action_name: str, action_input: Dict):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        try:
            result = self.tools.call_tool(action_name, action_input)
            return {"success": True, "result": result}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def observe(self, action_result: Dict):
        """è§‚å¯Ÿç»“æœ"""
        return f"åŠ¨ä½œæ‰§è¡Œç»“æœï¼š{action_result}"

    def solve(self, query: str, max_iterations: int = 5):
        """è§£å†³å®Œæ•´é—®é¢˜"""
        for i in range(max_iterations):
            # æ€è€ƒ
            thought = self.think(query)
            self.thoughts.append(thought)

            # æ‰§è¡ŒåŠ¨ä½œ
            action_result = self.act(thought["action"], thought["action_input"])
            self.actions.append(action_result)

            # è§‚å¯Ÿ
            observation = self.observe(action_result)
            self.observations.append(observation)

            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if action_result.get("success") and "final_answer" in action_result.get("result", ""):
                return action_result["result"]

        return "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé—®é¢˜æœªå®Œå…¨è§£å†³"

    def _build_context(self):
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context_parts = []

        for i, (thought, action, obs) in enumerate(zip(self.thoughts, self.actions, self.observations)):
            context_parts.append(f"æ­¥éª¤ {i+1}:")
            context_parts.append(f"  æ€è€ƒ: {thought.get('reasoning', '')}")
            context_parts.append(f"  åŠ¨ä½œ: {thought.get('action', '')}")
            context_parts.append(f"  ç»“æœ: {obs}")

        return "\n".join(context_parts)

    def _format_tools(self):
        """æ ¼å¼åŒ–å·¥å…·åˆ—è¡¨"""
        tools = self.tools.get_available_tools()
        formatted = []
        for tool in tools:
            formatted.append(f"- {tool['name']}: {tool['description']}")
        return "\n".join(formatted)


# ==================== TAO åŠŸèƒ½ ====================


class TAOAgent:
    """TAO (Thought + Action + Observation) ä»£ç†"""

    def __init__(self, tools: MCPToolManager = None):
        self.tools = tools or MCPToolManager()
        self.thoughts = []
        self.actions = []
        self.observations = []

    def think(self, query: str):
        """æ€è€ƒé˜¶æ®µ"""
        context = self._build_context()

        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªTAOä»£ç†ï¼Œéœ€è¦è§£å†³ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

å¯ç”¨å·¥å…·ï¼š
{self._format_tools()}

å†å²è®°å½•ï¼š
{context}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡Œæ€è€ƒï¼š
```yaml
thinking: |
  <ä½ çš„æ€è€ƒè¿‡ç¨‹>
action: <è¦æ‰§è¡Œçš„åŠ¨ä½œåç§°>
action_input: <åŠ¨ä½œçš„è¾“å…¥å‚æ•°>
is_final: <æ˜¯å¦ä¸ºæœ€ç»ˆç­”æ¡ˆ>
```

è¯·åŸºäºæ€è€ƒé€‰æ‹©æœ€åˆé€‚çš„åŠ¨ä½œã€‚
"""

        response = call_llm(prompt, output_format="yaml")
        return response

    def act(self, action_name: str, action_input: Dict):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        try:
            if action_name == "answer":
                return {"success": True, "result": action_input}
            else:
                result = self.tools.call_tool(action_name, action_input)
                return {"success": True, "result": result}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def observe(self, action_result: Dict):
        """è§‚å¯Ÿç»“æœ"""
        prompt = f"""
åˆ†æä»¥ä¸‹åŠ¨ä½œç»“æœå¹¶æä¾›è§‚å¯Ÿï¼š

åŠ¨ä½œç»“æœï¼š{action_result}

è¯·æä¾›å®¢è§‚çš„è§‚å¯Ÿï¼Œä¸è¦åšå†³ç­–ï¼Œåªæè¿°ä½ çœ‹åˆ°çš„å†…å®¹ã€‚
"""

        observation = call_llm(prompt)
        return observation

    def solve(self, query: str, max_iterations: int = 5):
        """è§£å†³å®Œæ•´é—®é¢˜"""
        for i in range(max_iterations):
            # æ€è€ƒ
            thought = self.think(query)
            self.thoughts.append(thought)

            # æ‰§è¡ŒåŠ¨ä½œ
            action_result = self.act(thought["action"], thought["action_input"])
            self.actions.append(action_result)

            # è§‚å¯Ÿ
            observation = self.observe(action_result)
            self.observations.append(observation)

            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if thought.get("is_final", False):
                return action_result.get("result", "å®Œæˆ")

        return "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé—®é¢˜æœªå®Œå…¨è§£å†³"

    def _build_context(self):
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context_parts = []

        for i, (thought, action, obs) in enumerate(zip(self.thoughts, self.actions, self.observations)):
            context_parts.append(f"æ­¥éª¤ {i+1}:")
            context_parts.append(f"  æ€è€ƒ: {thought.get('thinking', '')}")
            context_parts.append(f"  åŠ¨ä½œ: {thought.get('action', '')}")
            context_parts.append(f"  è§‚å¯Ÿ: {obs}")

        return "\n".join(context_parts)

    def _format_tools(self):
        """æ ¼å¼åŒ–å·¥å…·åˆ—è¡¨"""
        tools = self.tools.get_available_tools()
        formatted = []
        for tool in tools:
            formatted.append(f"- {tool['name']}: {tool['description']}")
        return "\n".join(formatted)


# ==================== ToT (Tree of Thoughts) åŠŸèƒ½ ====================


class ToTNode:
    """ToTèŠ‚ç‚¹"""

    def __init__(self, thought: str, parent=None):
        self.thought = thought
        self.parent = parent
        self.children = []
        self.value = None
        self.status = "pending"  # pending, evaluated, expanded

    def add_child(self, thought: str):
        """æ·»åŠ å­èŠ‚ç‚¹"""
        child = ToTNode(thought, self)
        self.children.append(child)
        return child

    def evaluate(self, evaluator_func):
        """è¯„ä¼°èŠ‚ç‚¹"""
        self.value = evaluator_func(self.thought)
        self.status = "evaluated"
        return self.value


class ToTAgent:
    """Tree of Thoughts ä»£ç†"""

    def __init__(self, evaluator_func=None):
        self.evaluator_func = evaluator_func or self._default_evaluator
        self.root = None
        self.max_depth = 3
        self.max_breadth = 5

    def solve(self, query: str):
        """ä½¿ç”¨ToTè§£å†³é—®é¢˜"""
        self.root = ToTNode(f"å¼€å§‹è§£å†³: {query}")

        # ç”Ÿæˆåˆå§‹æƒ³æ³•
        initial_thoughts = self._generate_thoughts(query, self.root)
        for thought in initial_thoughts:
            self.root.add_child(thought)

        # æ‰©å±•å’Œè¯„ä¼°æ ‘
        self._expand_tree(self.root, depth=0)

        # æ‰¾åˆ°æœ€ä½³è·¯å¾„
        best_path = self._find_best_path(self.root)

        return self._extract_solution(best_path)

    def _generate_thoughts(self, context: str, parent_node: ToTNode):
        """ç”Ÿæˆæƒ³æ³•"""
        prompt = f"""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆ3-5ä¸ªä¸åŒçš„æ€è€ƒæ–¹å‘ï¼š

ä¸Šä¸‹æ–‡ï¼š{context}
å½“å‰æ€è€ƒï¼š{parent_node.thought}

è¯·ç”Ÿæˆä¸åŒçš„æ€è€ƒæ–¹å‘ï¼Œæ¯ä¸ªæ–¹å‘åº”è¯¥æ˜¯ä¸€ä¸ªå…·ä½“çš„æ­¥éª¤æˆ–æƒ³æ³•ã€‚
"""

        response = call_llm(prompt)
        thoughts = [line.strip() for line in response.split('\n') if line.strip()]
        return thoughts[: self.max_breadth]

    def _expand_tree(self, node: ToTNode, depth: int):
        """æ‰©å±•æ ‘"""
        if depth >= self.max_depth:
            return

        # è¯„ä¼°å½“å‰èŠ‚ç‚¹
        node.evaluate(self.evaluator_func)

        # å¦‚æœèŠ‚ç‚¹æœ‰ä»·å€¼ï¼Œç»§ç»­æ‰©å±•
        if node.value and node.value > 0.5:
            thoughts = self._generate_thoughts(node.thought, node)
            for thought in thoughts:
                child = node.add_child(thought)
                self._expand_tree(child, depth + 1)

    def _find_best_path(self, node: ToTNode):
        """æ‰¾åˆ°æœ€ä½³è·¯å¾„"""
        if not node.children:
            return [node]

        best_child = max(node.children, key=lambda x: x.value or 0)
        best_path = self._find_best_path(best_child)
        return [node] + best_path

    def _extract_solution(self, path: List[ToTNode]):
        """æå–è§£å†³æ–¹æ¡ˆ"""
        solution_parts = [node.thought for node in path]
        return "\n".join(solution_parts)

    def _default_evaluator(self, thought: str):
        """é»˜è®¤è¯„ä¼°å‡½æ•°"""
        prompt = f"""
è¯„ä¼°ä»¥ä¸‹æ€è€ƒçš„è´¨é‡ï¼ˆ0-1åˆ†ï¼‰ï¼š

æ€è€ƒï¼š{thought}

è¯·åªè¿”å›ä¸€ä¸ª0åˆ°1ä¹‹é—´çš„æ•°å­—ï¼Œè¡¨ç¤ºè¿™ä¸ªæ€è€ƒçš„è´¨é‡ã€‚
"""

        try:
            response = call_llm(prompt)
            return float(response.strip())
        except:
            return 0.5


# ==================== CoT (Chain of Thoughts) åŠŸèƒ½ ====================


class CoTAgent:
    """Chain of Thoughts ä»£ç†"""

    def __init__(self):
        self.thoughts = []

    def solve(self, query: str, max_steps: int = 5):
        """ä½¿ç”¨CoTè§£å†³é—®é¢˜"""
        current_context = query

        for step in range(max_steps):
            # ç”Ÿæˆä¸‹ä¸€æ­¥æ€è€ƒ
            thought = self._generate_thought(current_context, step + 1)
            self.thoughts.append(thought)

            # æ›´æ–°ä¸Šä¸‹æ–‡
            current_context = f"{current_context}\n\næ­¥éª¤ {step + 1}: {thought}"

            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if "æœ€ç»ˆç­”æ¡ˆ" in thought or step == max_steps - 1:
                break

        return self._extract_final_answer()

    def _generate_thought(self, context: str, step: int):
        """ç”Ÿæˆæ€è€ƒæ­¥éª¤"""
        prompt = f"""
ä½ æ­£åœ¨ä½¿ç”¨Chain of Thoughtsæ–¹æ³•è§£å†³é—®é¢˜ã€‚

å½“å‰é—®é¢˜ï¼š{context}

è¿™æ˜¯ç¬¬ {step} æ­¥æ€è€ƒã€‚è¯·æä¾›ä½ çš„æ¨ç†è¿‡ç¨‹ï¼Œå¹¶è¯´æ˜ä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆã€‚
å¦‚æœè¿™æ˜¯æœ€åä¸€æ­¥ï¼Œè¯·æä¾›æœ€ç»ˆç­”æ¡ˆã€‚
"""

        return call_llm(prompt)

    def _extract_final_answer(self):
        """æå–æœ€ç»ˆç­”æ¡ˆ"""
        if not self.thoughts:
            return "æ²¡æœ‰ç”Ÿæˆä»»ä½•æ€è€ƒ"

        # å°è¯•ä»æœ€åä¸€ä¸ªæ€è€ƒä¸­æå–ç­”æ¡ˆ
        last_thought = self.thoughts[-1]

        # ç®€å•çš„ç­”æ¡ˆæå–é€»è¾‘
        if "æœ€ç»ˆç­”æ¡ˆ" in last_thought:
            # æå–æœ€ç»ˆç­”æ¡ˆéƒ¨åˆ†
            answer_start = last_thought.find("æœ€ç»ˆç­”æ¡ˆ")
            if answer_start != -1:
                return last_thought[answer_start:]

        return last_thought


# ==================== HITL (Human in the Loop) åŠŸèƒ½ ====================


class HITLAgent:
    """Human in the Loop ä»£ç†"""

    def __init__(self, auto_mode=False):
        self.auto_mode = auto_mode
        self.history = []

    def process_with_human(self, query: str, context: str = ""):
        """å¤„ç†éœ€è¦äººå·¥å¹²é¢„çš„æŸ¥è¯¢"""
        # åˆ†ææ˜¯å¦éœ€è¦äººå·¥å¹²é¢„
        needs_human = self._analyze_human_needed(query, context)

        if needs_human and not self.auto_mode:
            # è¯·æ±‚äººå·¥è¾“å…¥
            human_input = self._get_human_input(query, context)
            self.history.append({"query": query, "context": context, "human_input": human_input, "timestamp": datetime.now().isoformat()})
            return human_input
        else:
            # è‡ªåŠ¨å¤„ç†
            result = self._auto_process(query, context)
            self.history.append({"query": query, "context": context, "auto_result": result, "timestamp": datetime.now().isoformat()})
            return result

    def _analyze_human_needed(self, query: str, context: str):
        """åˆ†ææ˜¯å¦éœ€è¦äººå·¥å¹²é¢„"""
        prompt = f"""
åˆ†æä»¥ä¸‹æŸ¥è¯¢æ˜¯å¦éœ€è¦äººå·¥å¹²é¢„ï¼š

æŸ¥è¯¢ï¼š{query}
ä¸Šä¸‹æ–‡ï¼š{context}

è¯·åˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥å¹²é¢„ï¼Œè¿”å› "yes" æˆ– "no"ã€‚
éœ€è¦äººå·¥å¹²é¢„çš„æƒ…å†µåŒ…æ‹¬ï¼š
- éœ€è¦ä¸»è§‚åˆ¤æ–­
- æ¶‰åŠæ•æ„Ÿä¿¡æ¯
- éœ€è¦åˆ›é€ æ€§æ€ç»´
- éœ€è¦ä¸“ä¸šçŸ¥è¯†
"""

        response = call_llm(prompt).lower().strip()
        return "yes" in response

    def _get_human_input(self, query: str, context: str):
        """è·å–äººå·¥è¾“å…¥"""
        print(f"\nğŸ¤– éœ€è¦äººå·¥å¹²é¢„:")
        print(f"æŸ¥è¯¢: {query}")
        if context:
            print(f"ä¸Šä¸‹æ–‡: {context}")
        print("è¯·è¾“å…¥ä½ çš„å›ç­”:")

        return input("> ")

    def _auto_process(self, query: str, context: str):
        """è‡ªåŠ¨å¤„ç†"""
        prompt = f"""
è‡ªåŠ¨å¤„ç†ä»¥ä¸‹æŸ¥è¯¢ï¼š

æŸ¥è¯¢ï¼š{query}
ä¸Šä¸‹æ–‡ï¼š{context}

è¯·æä¾›åˆé€‚çš„å›ç­”ã€‚
"""

        return call_llm(prompt)

    def get_history(self):
        """è·å–å†å²è®°å½•"""
        return self.history


# ==================== Supervisor Swarm åŠŸèƒ½ ====================


class SupervisorAgent:
    """ç›‘ç£ä»£ç†"""

    def __init__(self, name: str, criteria: List[str]):
        self.name = name
        self.criteria = criteria
        self.approvals = 0
        self.rejections = 0

    def evaluate(self, result: str, context: str = ""):
        """è¯„ä¼°ç»“æœ"""
        prompt = f"""
ä½ æ˜¯ç›‘ç£ä»£ç† {self.name}ï¼Œè´Ÿè´£è¯„ä¼°ç»“æœè´¨é‡ã€‚

è¯„ä¼°æ ‡å‡†ï¼š{', '.join(self.criteria)}

ç»“æœï¼š{result}
ä¸Šä¸‹æ–‡ï¼š{context}

è¯·è¯„ä¼°è¿™ä¸ªç»“æœæ˜¯å¦ç¬¦åˆæ ‡å‡†ï¼Œè¿”å› "approve" æˆ– "reject" ä»¥åŠåŸå› ã€‚
"""

        response = call_llm(prompt).lower()

        if "approve" in response:
            self.approvals += 1
            return {"decision": "approve", "reason": response}
        else:
            self.rejections += 1
            return {"decision": "reject", "reason": response}


class SupervisorSwarm:
    """ç›‘ç£è€…ç¾¤ä½“"""

    def __init__(self):
        self.supervisors = []
        self.consensus_threshold = 0.7

    def add_supervisor(self, name: str, criteria: List[str]):
        """æ·»åŠ ç›‘ç£è€…"""
        supervisor = SupervisorAgent(name, criteria)
        self.supervisors.append(supervisor)

    def evaluate_result(self, result: str, context: str = ""):
        """ç¾¤ä½“è¯„ä¼°ç»“æœ"""
        if not self.supervisors:
            return {"decision": "approve", "reason": "æ²¡æœ‰ç›‘ç£è€…"}

        evaluations = []
        for supervisor in self.supervisors:
            evaluation = supervisor.evaluate(result, context)
            evaluations.append(evaluation)

        # è®¡ç®—æ‰¹å‡†ç‡
        approvals = sum(1 for eval in evaluations if eval["decision"] == "approve")
        approval_rate = approvals / len(evaluations)

        # æ”¶é›†æ‰€æœ‰åŸå› 
        reasons = [eval["reason"] for eval in evaluations]

        if approval_rate >= self.consensus_threshold:
            return {"decision": "approve", "approval_rate": approval_rate, "reasons": reasons}
        else:
            return {"decision": "reject", "approval_rate": approval_rate, "reasons": reasons}

    def get_supervisor_stats(self):
        """è·å–ç›‘ç£è€…ç»Ÿè®¡ä¿¡æ¯"""
        stats = []
        for supervisor in self.supervisors:
            total = supervisor.approvals + supervisor.rejections
            approval_rate = supervisor.approvals / total if total > 0 else 0
            stats.append(
                {"name": supervisor.name, "approvals": supervisor.approvals, "rejections": supervisor.rejections, "approval_rate": approval_rate}
            )
        return stats


# ==================== å·¥å…·å‡½æ•° ====================


def create_memory_system(max_messages=10) -> ConversationMemory:
    """åˆ›å»ºè®°å¿†ç³»ç»Ÿ"""
    return ConversationMemory(max_messages)


def create_rag_system() -> RAGSystem:
    """åˆ›å»ºRAGç³»ç»Ÿ"""
    return RAGSystem()


def create_tool_manager() -> MCPToolManager:
    """åˆ›å»ºå·¥å…·ç®¡ç†å™¨"""
    return MCPToolManager()


def create_react_agent(tools=None) -> ReActAgent:
    """åˆ›å»ºReActä»£ç†"""
    return ReActAgent(tools)


def create_tao_agent(tools=None) -> TAOAgent:
    """åˆ›å»ºTAOä»£ç†"""
    return TAOAgent(tools)


def create_tot_agent(evaluator_func=None) -> ToTAgent:
    """åˆ›å»ºToTä»£ç†"""
    return ToTAgent(evaluator_func)


def create_cot_agent() -> CoTAgent:
    """åˆ›å»ºCoTä»£ç†"""
    return CoTAgent()


def create_hitl_agent(auto_mode=False) -> HITLAgent:
    """åˆ›å»ºHITLä»£ç†"""
    return HITLAgent(auto_mode)


def create_supervisor_swarm() -> SupervisorSwarm:
    """åˆ›å»ºç›‘ç£è€…ç¾¤ä½“"""
    return SupervisorSwarm()


if __name__ == "__main__":
    print("## æµ‹è¯• call_llm")
    prompt = "ç”¨å‡ å¥è¯è§£é‡Šä¸€ä¸‹ç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"## æç¤ºè¯: {prompt}")
    response = call_llm(prompt)
    print(f"## å“åº”: {response}")

    # print("## æµ‹è¯• search_web")
    # query = "è°è·å¾—äº†2024å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–ï¼Ÿ"
    # print(f"## æŸ¥è¯¢: {query}")
    # results = search_web_duckduckgo(query)
    # print(f"## ç»“æœ: {results}")

    print("## æµ‹è¯• Memory åŠŸèƒ½")
    memory = create_memory_system()
    memory.add_message("user", "ä½ å¥½")
    memory.add_message("assistant", "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ")
    print(f"## è®°å¿†æ¶ˆæ¯æ•°: {len(memory.messages)}")

    print("## æµ‹è¯• RAG åŠŸèƒ½")
    rag = create_rag_system()
    rag.add_documents(["äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"])
    rag.build_index()
    results = rag.retrieve("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
    print(f"## RAGæ£€ç´¢ç»“æœæ•°: {len(results)}")

    print("## æµ‹è¯• CoT åŠŸèƒ½")
    cot_agent = create_cot_agent()
    result = cot_agent.solve("è®¡ç®— 15 + 27 çš„ç»“æœ")
    print(f"## CoTç»“æœ: {result}")

    print("## æµ‹è¯•å‘é‡æœç´¢åŠŸèƒ½")
    # åˆ›å»ºä¸€äº›æµ‹è¯•å‘é‡
    test_vectors = np.random.rand(10, 5)  # 10ä¸ª5ç»´å‘é‡
    query_vector = np.random.rand(5)  # æŸ¥è¯¢å‘é‡

    print("ä½¿ç”¨scikit-learnå®ç°")
    nn = NearestNeighbors(n_neighbors=3, metric="cosine").fit(test_vectors)
    distances, indices = nn.kneighbors([query_vector])
    print(f"sklearnç»“æœ - è·ç¦»: {distances[0]}, ç´¢å¼•: {indices[0]}")
