@InProceedings{RESCAL,
  author =    {Maximilian Nickel and Volker Tresp and Hans-Peter Kriegel},
  title =     {A Three-Way Model for Collective Learning on Multi-Relational Data},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  series =    {ICML '11},
  year =      {2011},
  editor =    {Lise Getoor and Tobias Scheffer},
  location =  {Bellevue, Washington, USA},
  isbn =      {978-1-4503-0619-5},
  month =     {June},
  publisher = {ACM},
  address =   {New York, NY, USA},
  pages=      {809--816},
  url = {https://icml.cc/Conferences/2011/papers/438_icmlpaper.pdf},
}
@inproceedings{TransE,
  author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Translating Embeddings for Modeling Multi-relational Data},
  url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf},
  volume = {26},
  year = {2013}
}
@article{TransH,
  title={Knowledge Graph Embedding by Translating on Hyperplanes},
  volume={28},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/8870},
  DOI={10.1609/aaai.v28i1.8870},
  abstractNote={ &lt;p&gt; We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up. &lt;/p&gt; },
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
  year={2014},
  month={Jun.}
}
@article{TransR,
  title={Learning Entity and Relation Embeddings for Knowledge Graph Completion},
  volume={29},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/9491},
  DOI={10.1609/aaai.v29i1.9491},
  abstractNote={ &lt;p&gt; Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. &lt;/p&gt; },
  number={1},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
  year={2015},
  month={Feb.}
}
@inproceedings{DistMult,
  author = {Yang, Bishan and Yih, Scott Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
  title = {Embedding Entities and Relations for Learning and Inference in Knowledge Bases},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR) 2015},
  year = {2015},
  month = {May},
  abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2% vs. 54.7% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as BornInCity(a,b) ^ City In Country (b,c) => Nationality(a,c). We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics, and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining horn rules that involve compositional reasoning.},
  url = {https://www.microsoft.com/en-us/research/publication/embedding-entities-and-relations-for-learning-and-inference-in-knowledge-bases/},
  edition = {Proceedings of the International Conference on Learning Representations (ICLR) 2015},
}
@inproceedings{TransD,
  title = "Knowledge Graph Embedding via Dynamic Mapping Matrix",
  author = "Ji, Guoliang  and
    He, Shizhu  and
    Xu, Liheng  and
    Liu, Kang  and
    Zhao, Jun",
  editor = "Zong, Chengqing  and
    Strube, Michael",
  booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
  month = jul,
  year = "2015",
  address = "Beijing, China",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P15-1067",
  doi = "10.3115/v1/P15-1067",
  pages = "687--696",
}
@inproceedings{HolE,
  Author = {Nickel, Maximilian and Rosasco, Lorenzo and Poggio, Tomaso},
  Book-Group-Author = {AAAI},
  Title = {Holographic Embeddings of Knowledge Graphs},
  Booktitle = {THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE},
  Series = {AAAI Conference on Artificial Intelligence},
  Year = {2016},
  Pages = {1955-1961},
  Note = {30th Association-for-the-Advancement-of-Artificial-Intelligence (AAAI)
     Conference on Artificial Intelligence, Phoenix, AZ, FEB 12-17, 2016},
  Organization = {Assoc Advancement Artificial Intelligence},
  ISSN = {2159-5399},
  EISSN = {2374-3468},
  Unique-ID = {WOS:000485474201139},
  url = "https://ojs.aaai.org/index.php/AAAI/article/view/10314",
}
@inproceedings{ComplEx,
  title={Complex embeddings for simple link prediction},
  author={Trouillon, Th{\'e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, {\'E}ric and Bouchard, Guillaume},
  booktitle={International conference on machine learning},
  pages={2071--2080},
  year={2016},
  organization={PMLR},
  url={https://proceedings.mlr.press/v48/trouillon16.html}
}
@InProceedings{ANALOGY,
  title = 	 {Analogical Inference for Multi-relational Embeddings},
  author =       {Hanxiao Liu and Yuexin Wu and Yiming Yang},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2168--2178},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/liu17d/liu17d.pdf},
  url = 	 {https://proceedings.mlr.press/v70/liu17d.html},
  abstract = 	 {Large-scale multi-relational embedding refers to the task of learning the latent representations for entities and relations in large knowledge graphs. An effective and scalable solution for this problem is crucial for the true success of knowledge-based inference in a broad range of applications. This paper proposes a novel framework for optimizing the latent representations with respect to the <em>analogical</em> properties of the embedded entities and relations. By formulating the objective function in a differentiable fashion, our model enjoys both its theoretical power and computational scalability, and significantly outperformed a large number of representative baseline methods on benchmark datasets. Furthermore, the model offers an elegant unification of several well-known methods in multi-relational embedding, which can be proven to be special instantiations of our framework.}
}
@inproceedings{SimplE,
  author = {Kazemi, Seyed Mehran and Poole, David},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {SimplE Embedding for Link Prediction in Knowledge Graphs},
  url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/b2ab001909a8a6f04b51920306046ce5-Paper.pdf},
  volume = {31},
  year = {2018}
}
@inproceedings{R-GCN,
  title={Modeling relational data with graph convolutional networks},
  author={Schlichtkrull, Michael and Kipf, Thomas N and Bloem, Peter and Van Den Berg, Rianne and Titov, Ivan and Welling, Max},
  booktitle={The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3--7, 2018, Proceedings 15},
  pages={593--607},
  year={2018},
  organization={Springer},
  url={https://arxiv.org/abs/1703.06103}
}
@inproceedings{RotatE,
  title={RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space},
  author={Zhiqing Sun and Zhi-Hong Deng and Jian-Yun Nie and Jian Tang},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=HkgEQnRqYQ},
}
@inproceedings{CompGCN,
  title={Composition-based Multi-Relational Graph Convolutional Networks},
  author={Shikhar Vashishth and Soumya Sanyal and Vikram Nitin and Partha Talukdar},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=BylA_C4tPr}
}
@misc{ConvE,
  title={Convolutional 2D Knowledge Graph Embeddings}, 
  author={Tim Dettmers and Pasquale Minervini and Pontus Stenetorp and Sebastian Riedel},
  year={2018},
  eprint={1707.01476},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@inproceedings{FB15k-237,
  title={Observed versus latent features for knowledge base and text inference},
  author={Toutanova, Kristina and Chen, Danqi},
  booktitle={Proceedings of the 3rd workshop on continuous vector space models and their compositionality},
  pages={57--66},
  year={2015}
}