Metadata-Version: 2.4
Name: kynex
Version: 0.1.9
Summary: A Python package to get data from different llm's
Home-page: https://www.aegletek.com/
Author: Aegletek
Author-email: coe@aegletek.com
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Developers
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: google-generativeai
Requires-Dist: langchain
Requires-Dist: groq
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ðŸ”· Kynex

**Kynex** is a modular Python framework that simplifies connecting to multiple LLM providers such as **Google Gemini** and **Groq** via a unified, flexible interface. Whether you're building AI workflows, chatbots, or prompt-based applications, Kynex makes it seamless to integrate LLMs dynamically based on user input.

---

## ðŸš€ Features

-  Multi-LLM support**: Easily switch between different LLMs
-  API key flexibility**: Dynamically accept LLM type, model name, and API key at runtime
-  LangChain prompt integration**: Use `PromptTemplate` for advanced prompt formatting
-  Simple base architecture**: Extend to any new LLM provider with a custom class


---

## ðŸ“¦ Installation

```bash
pip install kynex


Example Usage:

Create it:

Create a Python file and add the following:

from kynex.kynexa import Kynex

request = {
    "prompt": "your_prompt.",
    "model_name": "your_model_name",
    "api_key": "your_api_key",
    "llm_type": "type_llm"  # e.g: "gemini"
}

response = Kynex.get_llm_response(
    prompt=request["prompt"],
    model_name=request["model_name"],
    api_key=request["api_key"],
    llm_type=request.get("llm_type")
)

print(response)
