{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixasonics: An Image Sonification Toolbox for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixasonics is a library for interactive audiovisual image analysis and exploration, through image sonification. That is, it is using real-time audio and visualization to listen to image data: to map between image features and acoustic parameters. This can be handy when you need to work with a large number of images, image stacks, or hyper-spectral images (involving many color channels) where visualization becomes limiting, challenging, and potentially overwhelming.\n",
    "\n",
    "With pixasonics, you can launch a little web application (running in a Jupyter notebook), where you can load images, probe their data with various feature extraction methods, and map the extracted features to parameters of synths, devices that make sound. You can do all this in real-time, using a visual interface, you can remote-control the interface programmatically, record sound real-time, or non-real-time, with a custom script.\n",
    "\n",
    "You can also run multiple instances of the app, or use it \"headless mode\" (without its visual interface), perhaps in a command-line script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are in a hurry..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ;pip install pixasonics\n",
    "\n",
    "# quick workflow with a simple example\n",
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import MeanChannelValue\n",
    "from pixasonics.synths import Theremin\n",
    "\n",
    "# create a new app\n",
    "app = App() # by default 500x500 pixels\n",
    "\n",
    "# load an image from file\n",
    "app.load_image_file(\"images/test.jpg\")\n",
    "\n",
    "# create a Feature that will report the mean value of the red channel\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "# attach the feature to the app\n",
    "app.attach(mean_red) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)\n",
    "\n",
    "# create a Theremin, a simple sine wave synth that we will use to sonify the mean pixel value\n",
    "theremin = Theremin(name=\"MySine\")\n",
    "# attach the Theremin to the app\n",
    "app.attach(theremin) # this adds it to the pipeline, so that its audio output is patched into the global audio graph\n",
    "\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\") # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "# attach the Mapper to the app\n",
    "app.attach(red2freq) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolbox Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixasonics is mainly designed to run in a Jupyter Notebook environment. (It does also work in command line scripts.)\n",
    "\n",
    "At the center of pixasonics is the `App` class. This represents a template pipeline where all your image data, feature extractors, synths and mappers will live. The App also comes with a graphical user interface (UI). You can do a lot with a single `App` instance, but nothing stops you from spawning different `App`s with bespoke setups.\n",
    "\n",
    "When you have your app, you load an image (either from a file, or from a numpy array) which will be displayed in the `App` canvas. Note that _currently_ your image data height and width dimensions (the first two) will be downsampled to the `App`'s `image_size` creation argument, which is a tuple of `(500, 500)` pixels by default. (This will be improved later, stay tuned!)\n",
    "\n",
    "Then you can explore the image data with a Probe (represented by the yellow rectangle on the canvas) using your mouse or trackpad. The Probe is your \"stethoscope\" on the image, and more technically, it is the sub-matrix of the Probe that is passed to all `Feature` objects in the pipeline.\n",
    "\n",
    "Speaking of which, you can extract visual features using the `Feature` base class, or any of its convenience abstractions (e.g. `MeanChannelValue`). All basic statistical reductions are supported, such as `\"mean\"`, `\"median\"`, `\"min\"`, `\"max\"`, `\"sum\"`, `\"std\"` (standard deviation) and `\"var\"` (variance), but you can also make your own custom feature extractors by inheriting from the `Feature` base class (stay tuned for a K-means clustering example in the Advanced Use Cases section!). `Feature` objects also come with a UI that shows their current values and global/running min and max. There can be any number of different `Feature`s attached to the app, and all of them will get the same Probe matrix as input.\n",
    "\n",
    "Image features are to be mapped to synthesis parameters, that is, to the settings of sound-making gadgets. (This technique is called \"Parameter Mapping Sonification\" in the literature.) All `Synth`s (and audio) in pixasonics are based on the fantastic [signalflow library](https://signalflow.dev/). For now, there are 5 `Synth` classes that you can use (and many more are on the way): `Theremin`, `Oscillator`, `FilteredNoise`, and `SimpleFM`. Each `Synth` comes with a UI, where you can tweak the parameters (or see them being modulated by `Mapper`s) in real-time.\n",
    "\n",
    "What connects the output of a `Feature` and the input parameter of a Synth is a `Mapper` object. There can be multiple `Mapper`s reading from the same `Feature` buffer and a `Synth` can have multiple `Mapper`s modulating its different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `App` class is at the core of the pixasonics workflow. The `App` is where you load your image data, where you move the probe, the `App` connects to the real-time audio server, and it represents the pipeline of `Feature`s connected to `Synth`s with `Mapper`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase the `App` and its functionality, let's create a basic scene, using an image from the [CELLULAR open dataset](https://zenodo.org/records/8315423) and map the mean red channel value to the frequency of a Theremin (a simple sine wave generator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import MeanChannelValue\n",
    "from pixasonics.synths import Theremin\n",
    "\n",
    "# create a new app\n",
    "app = App() # by default 500x500 pixels\n",
    "\n",
    "# load an image from file\n",
    "app.load_image_file(\"images/cellular_dataset/merged_8bit/Timepoint_001_220518-ST_C03_s1.jpg\")\n",
    "\n",
    "# create a Feature that will report the mean value of the red channel\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "# attach the feature to the app\n",
    "app.attach(mean_red) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)\n",
    "\n",
    "# create a Theremin, a simple sine wave synth that we will use to sonify the mean pixel value\n",
    "theremin = Theremin()\n",
    "# attach the Theremin to the app\n",
    "app.attach(theremin) # this adds it to the pipeline, so that its audio output is patched into the global audio graph\n",
    "\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\") # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "# attach the Mapper to the app\n",
    "app.attach(red2freq) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things just happened. The app created (or connected to) the global audio graph and the UI popped up: a canvas with the image on the left, and various settings panes on the right. The size of the canvas is determined by the `image_size` argument at the creation of the `App` object and cannot be changed afterwards. It is a tuple corresponding to `(height, width)`, and it is `(500, 500)` by default. All images that you load into the app will be __resized__ to this height and width! (This might be changed/improved in the future.) This means, that, at least currently, the app does not respect the aspect ratio of your input image, it will stretch or shrink it to whatever `image_size` your `App` has. Luckily the image we loaded is 2048x2048 with a square aspect ratio, so the default `image_size` was fine.\n",
    "\n",
    "Now let's look to the right where the various settings panes live. These can be opened and closed, one at a time. Here is a brief summary of what you can find in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on \"Audio Settings\" on the top right to open this pane. Here you can control the global audio settings of the app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Audio switch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an audio switch at the top left, and a volume slider next to it. To have any sound produced by the app, you need to turn on audio, either on the UI, or by evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.audio = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no sound from the app, huh? This is because you (probably) haven't interacted with the canvas yet. Try clicking on a few of those _Drosophila_ S2 blood cells. Then try to click and drag to hear a continuous change in frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Volume Slider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to the audio switch there is the Master Volume slider where you can set the loudness of the app's sound output in decibels (dB). You can either control it via the UI or the `master_volume` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_volume = -24 # check the slider after running this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time recording to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the audio switch and the master slider, there is a pair of widgets that let you record the real-time output of the app. Leave the file name at the default `\"recording.wav\"`, hit the Record button, click and drag around the image, then click on the Record button again to stop recording.\n",
    "\n",
    "Let's listen to what you've done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "display(Audio(\"recording.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global recording is meant to be a quick-and-easy way to record your results. If you want to be more precise, (or faster than real-time) there are methods to render your results in non-real-time, which we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the recording section you find the Master Envelope, which controls how fast the sound fades in and out when you interact with the canvas. It is a traditional Attack-Decay-Sustain-Release (ADSR) curve that is applied to the volume of the global audio output. Here is how it works in a nutshell:\n",
    "- Attack: the time for the sound to fade in (in seconds),\n",
    "- Decay: the time to fade to the Sustain level (in seconds),\n",
    "- Sustain: the level (amplitude, between 0 and 1) to sustain indefinitely, until we release the mouse button (or deactivate the Probe),\n",
    "- Release: the time for the sound to fade out after we release the mouse button (or deactivate the Probe).\n",
    "\n",
    "There are some number fields to set these parameters, and a little drawing that visualizes the proportions of the different time segments. On the bottom, there is also a \"Duration\" value that shows the total duration of the envelope (that is Attack time + Decay time + Release time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a bit slower envelope to illustrate how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_envelope.attack = 0.5\n",
    "app.master_envelope.release = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set a more percussive envelope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_envelope.attack = 0.01\n",
    "app.master_envelope.decay = 0.01\n",
    "app.master_envelope.sustain = 0.1\n",
    "app.master_envelope.release = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's reset it to a more neutral default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_envelope.attack = 0.1\n",
    "app.master_envelope.decay = 0.01\n",
    "app.master_envelope.sustain = 1\n",
    "app.master_envelope.release = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future Envelopes will have more functionality in pixasonics, but for now there is only a global Master Envelope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick technical detail while we are discussing Audio Settings. Currently, there are no controls setting the sample rate and buffer size of the audio graph (sorry!), because there are some unresolved issues about these involving the Signalflow library. For now you can get the currently used values from the `App` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.sample_rate, app.output_buffer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's close the Audio Settings pane and move on to Display Settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, we have started to stray a bit far from the cell where our app UI lives... Luckily, we can get a synced copy down here if we evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaah, much better. Let's open the Display Settings. As the name suggests, this is where you can find settings related to how the image is displayed in the app canvas. These will __only affect the display__ though, the underlying image data (that may be HDR, or have many color channels or image layers) will not be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the top of the pane you can find two checkboxes: \"Normalize display\" and \"Global normalization\". More often than not we need to normalize images to understand (or even, simply, to see!) the image content better. Traditionally, the normalization is performed individually on all color channels (Red, Green, and Blue in \"normal\" images). Keeping it channel-wise can help remove imbalances between the channels, like the green tint here. Check \"Normalize display\" in the UI, or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that the background became much less green-tinted. Let's try also checking \"Global normalization\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display_global = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now the gren tint is back. It makes sense, because when normalization is set to global, the algorithm will scale all pixels according to the minimum and maximum pixel value apparent in __any__ of the channels. Since in the original image the green tint was the result of generally much higher green pixel values, applying global normalization brought the tint back. Let's turn it off for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display_global = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Offset and Layer Offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, under the checkboxes we have two (currently disabled) sliders, labelled \"Channel Offset\" and \"Layer Offset\". We will use these when we read numpy arrays instead of single image files (stay tuned!). For now, let's move on to the Probe Settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probe Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the Display Settings pane, and open the Probe Settings. This is where you set everything related to the Probe (remember, our image \"stethoscope\") and how you interact with it. The Probe is represented by a rectangle drawn over the image. When the Probe is active, the rectangle is red, and when the Probe is inactive, the rectangle is yellow. (Also, when the Probe is active, the `App` will evaluate all attached mappings in the pipeline and unmute audio for all attached synths.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Probe Width and Height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The width and height of the Probe can vary from 1x1 pixel up to the app's `image_size`. Try moving the sliders and observe how the shape of the yellow rectangle on the canvas changes. There is not really a \"right\" setting for this, it is highly dependent on the content of your image, the area of pixels you want to send to feature extraction, and generally, the kind of sonification you want to create. At an extreme, you can create horizontal or vertical scan lines for the whole image like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal scanning (a vertical scan line)\n",
    "app.probe_width = 1\n",
    "app.probe_height = app.image_size[0] # remember that image size is given as (height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical scanning (a horizontal scan line)\n",
    "app.probe_width = app.image_size[1]\n",
    "app.probe_height = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, you probably want to set the Probe dimensions to something smaller to fit the content of the image. Let's set it to fit that nice, bright cell in the middle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_width = 25\n",
    "app.probe_height = 25\n",
    "app.probe_x = 233\n",
    "app.probe_y = 252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you guessed it: we can programmatically move the probe around using the `probe_x` and `probe_y` properties of our `App`. Setting the dimensions and the position of the probe like this will be the basis of scripting custom paths and render them non-real-time (more on that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now something fun: interaction. So far we have used the \"Hold\" mode, that is, sound will be activated (and all existing mappers evaluated in the pipeline) while the mouse button is held down. This mode is comfortable when you want to quickly inspect (and listen to) the various parts of your image.\n",
    "\n",
    "But sometimes you may want to activate the Probe, and then leave it on while you experiment with, let's say, changing synthesis parameters. Or you just want to free your mouse to use it somewhere else without stopping the sound output. This is what \"Toggle\" mode is for. You can change it on the UI by clicking on the \"Toggle\" button, or programmatically like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.interaction_mode = \"toggle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can double-click on the probe to activate it. Double-click again to deactivate it. It could be sometimes comfortable to activate the Probe, and just click on various parts in the image (here, on different cells) to get an abrupt comparison of their sounding mappings. \n",
    "\n",
    "Try to activate the toggle and then click on the different cells to find out which one produces the highest pitch! Since we mapped the mean red channel value to the theremin frequency, and since in this image red fluorescent protein indicates the level of autophagy going on in the cell, we can intuitively find the cell with the most active autophagy by simple clicking around and listening. (Technically, if we would extract the _maximum_ red value under the probe we would get more precise results for this task, but let's give this a pass for now.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is a checkbox for having the Probe __always__ follow your mouse, even when you don't hold the mouse button. Beyond this being simply comfortable for your hand if you are making sound from the image continuously, it can also be handy when you don't want sound output at the moment, just want to read the value of a Feature. Let's check \"Probe follows idle mouse\" on the UI, or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_follows_idle_mouse = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you hover your mouse over the app canvas, you can see that the Probe constantly follows it, and you can see indicators of the Probe's horizontal and vertical coordinates labelled \"Probe X\" and \"Probe Y\", respectively, and controlled by the `probe_x` and `probe_y` properties (as shown above).\n",
    "\n",
    "Let's move on to the Features pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Features pane is where all your `Feature` objects (that you attached to the app!) will show up. (As of now, in pixasonics, the term \"feature\" always refers to a _visual_ feature, as currently audio feature extraction and mapping to auditory features is not supported.) \n",
    "\n",
    "Right now you should see a card labelled \"MeanRed\" there, our Feature that reports the mean red pixel value of the Probe's slice of the image. Since we just checked the option for the Probe to follow our idle (hovering) mouse, let's move it around a bit (without necessarily making any sound), and look at the values we get on the card.\n",
    "\n",
    "To avoid scrolling back-and-forth too much, let's have another copy of our app UI here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every feature gets the slice of the image data under the Probe and reduces it into one or more values. Every time you move the probe with your mouse, all the features within the `App` are sent a new Probe matrix, and they update their feature buffers (the container where they store their most-recent values). This example feature that we named \"MeanRed\" measures the mean value of the Probe matrix in the red color channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the card you can see that, next to the name, there is a small identifier, which is uniquely generated for every different object. It is not very important now, but it might help you later identifying various objects in your pipeline.\n",
    "Underneath this header you see the \"Number of Features:\" box that shows you how many values does this `Feature` report. Now it's just 1, since we are only filtering for the first (red) channel. Before we discuss what's behind this in more detail, let's just move on and look at the rest of the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the card is occupied by 3 number fields, showing the minimum, maximum and current (or most-recent) value of the feature. When a `Feature` is attached to the `App`, it actually gets to look at the whole image _once_, so it can calculate things, like global minimum and maximum values. You probably noticed that as you move the Probe around, the Min and Max fields don't change. That is because it calculated the global minimum and maximum value in the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting to running min and max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping track of the range of values will come especially handy when setting up mappings. In many cases it is helpful to get global min/max values, but sometimes we might need running min/max values of the Probe matrix only. If you hover your mouse over the Reset button in the bottom left corner of the Feature card, you will see the tooltip appear: _\"Reset to the running min and max\"_. You can click on that or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_red.reset_minmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at the time of resetting you got the same value as min and max. But when you move the Probe around, you see that the values start to move away from each other, as the running min and max values update. This behavior can be useful when you want to create a dynamic mapping and compare two visual objects relative to each other. It can also be convenient to compare parts of the image only visually, without sonification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach to and detach from the App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the other button that says \"Detach\"? Click on it, or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.detach(mean_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, our Feature is gone! Don't worry, the `mean_red` variable still holds our `Feature` object, it's just no longer included in the `App`'s pipeline. That is, whenever the Probe moves, this detached `Feature` won't get the updated Probe matrix anymore, and its values (its feature buffer) will remain the same. Just to prove that it's still there, here is how you can pull up its UI separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_red.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go, our MeanRed feature is still functional. It is just outside the `App`'s pipeline. But we can attach it back by saying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.attach(mean_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to the `App` UI again, you can see that now the card labelled MeanRed re-appeared in the Features pane. And by the way, our copy of the feature UI still works, it is in sync with the one in the `App` UI. Try to move the Probe and you'll see the numbers update in both places.\n",
    "\n",
    "This mechanism of \"attaching\" and \"detaching\" objects to and from the main app is a pattern that is the same for `Feature`s, `Synth`s, and `Mapper`s. When you want to include something in the `App`, which is to say, you want to include it in the pipeline of Probe --> Features --> Mappers --> Synths, you _attach_ it to the App. When you don't need it anymore (even just for a while), you can _detach_ it from the App. The object will still \"live\", but it won't get updates from the `App` anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another `Feature` to see the mean pixel values across all three color channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pix = MeanChannelValue(name=\"MeanPix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we created `mean_red`? It was like this:\n",
    "```python\n",
    "# create a Feature that will report the mean value of the red channel\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only difference is that this time we did not specify `filter_channels` (so it defaults to `None`). By default all `Feature` objects process the entire Probe matrix which is actually a 4-dimensional matrix in the shape of:\n",
    "\n",
    "__(Height, Width, Channels, Layers)__\n",
    "\n",
    "_In fact, our entire image data is kept in that shape._ So when we specified `filter_channels=0`, we selected the 0th (red) channel in the 2nd, Channel dimension. Don't worry if this is a bit over your head now, the TLDR is that `Feature` objects can actually filter the Probe matrix in any way you need.\n",
    "\n",
    "Now let's attach our new `Feature` to our `App`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.attach(mean_pix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Features pane on our app UI has two cards now: MeanRed and MeanPix. We can also query the currently attached `Feature`s like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have probably noticed, MeanPix reports 3 values, which are the mean pixel values across the 3 channels of the Probe matrix. We'll get into more details about `Feature`s in a bit, but for now let's move on to `Synth`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Synth`s are the tools to make sound with in pixasonics. As mentioned at the top, they are all abstractions (technically, `Patch`es) based on the awesome [signalflow library](https://signalflow.dev/).\n",
    "\n",
    "First let's have a look at a simple Theremin synth, outside the `App`. Here is how you make one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_theremin = Theremin(name=\"SimpleTheremin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull up its UI like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_theremin.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see we got a similar card here: a header with the name of the synth and its unique ID, a main section with all its parameters and corresponding sliders, as well as Reset and Detach buttons at the bottom of the card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing there though that turns the synth on or off. This is normally done by our `App` as a result of our interaction with the Probe. Under the hood, the `App` creates (or connects to) the global audio graph in SignalFlow, which is the `AudioGraph` object. This way we can use `App.graph` to reference this global audio graph and play the `output` Node of our synth patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = app.graph\n",
    "graph.play(simple_theremin.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Parameters and Resetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should hear a static sine tone. Try moving the sliders and listen to how the sound changes to get an understanding of the parameters. When you are done, you can hit the Reset button to reset all parameters to their defaults. These defaults are not set in stone, they are decided when we create the synth object. If we spell it out, we actually said this:\n",
    "```python\n",
    "simple_theremin = Theremin(frequency=440, amplitude=0.5, panning=0, name=\"SimpleTheremin\")\n",
    "```\n",
    "\n",
    "Now let's stop playing the synth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.stop(simple_theremin.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping to parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pixasonics we currently focus on Parameter Mapping Sonification. This is probably the most common sonification technique, where data features get mapped to input parameters of a synthesizer. While this might have some perceptual issues when mappings get complex, there is a reason why this technique is so popular: it is very simple to set up and it is easily explainable.\n",
    "There is a way to define your custom mapping scheme too, stay tuned for that (or jump ahead to the \"Advanced Use Cases\" section).\n",
    "For now, let's focus on how we can express quantities in the image (such as the level of red channel values) as changes in the parameters of synths, such as the frequency.\n",
    "\n",
    "Let's make a new instance of our app UI again for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach and detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the Synths pane. Just like the Features pane, this is where all the attached `Synth`s will have their UIs. Similarly to `Feature`s, you can detach `Synth`s by clicking on the Detach button on their card, or by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.detach(theremin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means our Theremin is not included in the `App`'s audio graph anymore, so when we activate the probe, there is no sound (since there is no `Synth` currently attached to the `App`).\n",
    "\n",
    "We can attach the Theremin back by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.attach(theremin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving sliders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to activate the Probe, listen to the sound and watch the frequency slider of the Theremin. (Remember, we are still in \"Toggle\" interaction mode, so you have to double-click to activate the Probe.)\n",
    "\n",
    "The slider of the Theremin seems to move on its own as we move the Probe. (Don't worry that the slider updates are not constant, this is a deliberate optimization to avoid choking the Jupyter server with slider updates. The sliders will update once you haven't moved the Probe for about 50 milliseconds.) Have you observed any relationship between the image data and the frequency of the Theremin? If you need a hint, open the Mappers pane on the app UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our very first setup of this scene, we created and attached this `Mapper` like so:\n",
    "```python\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\") # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "# attach the Mapper to the app\n",
    "app.attach(red2freq) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Mapper`s take two positional arguments as the `Feature` object we want to map _from_ and the `Synth` parameter (or a list of `Synth` parameters!) we want to map _to_. The notation `theremin[\"frequency\"]` will tell the `Mapper` to map to the frequency parameter of the theremin `Synth` object. The actual object that is returned is a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theremin[\"frequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranges at the input and output sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of info in here that we don't need to worry about right now. The most important for us now are the two top ones, the 'min' and 'max'.\n",
    "\n",
    "What a `Mapper` does, is mapping a value (or an array of values) from a given input range to a given output range. It can do this either linearly (where `exponent=1`) or exponentially (like `exponent=2` for a cubic curve).\n",
    "\n",
    "But we haven't specified any input or output range when we created our `red2freq` mapper object. How can it still know what red values to map to what frequency value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is knowing that, on one hand, `Feature` objects report their global (or running) min and max values (remember?!), and, similarly, that all `Synth` parameter dictionaries will contain curated (sensible) min and max values. If no input and output ranges are specified upon creation, the `Mapper` object will just look up the corresponding min and max values on both the feature-side and the synth-side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having automatic ranges that update live with the running min/max of a `Feature` object can be handy for experimentation, but sometimes we do want to specify something more concrete. Before we get there, it is perhaps helpful to look at the spelled-out verison of how we created our `red2freq` object:\n",
    "```python\n",
    "red2freq = Mapper(\n",
    "    obj_in=mean_red, \n",
    "    obj_out=theremin[\"frequency\"],\n",
    "    in_low=None,\n",
    "    in_high=None,\n",
    "    out_low=None,\n",
    "    out_high=None,\n",
    "    exponent=2, \n",
    "    clamp=True,\n",
    "    name=\"Red2Freq\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify an input range (on the `Feature` side) with the `in_low` and `out_low` arguments, while the output range (on the `Synth` parameter side) with the `out_low` and `out_high` arguments. They all default to `None`, which will mean that the `Mapper` will look up the current min/max reported by the `Feature` and the default min and max values of the `Synth` param (that were decided when we created the `Synth`).\n",
    "\n",
    "We can also retrieve these values like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq.in_low, red2freq.in_high, red2freq.out_low, red2freq.out_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though our MeanRed `Feature` reports singular scalar values, the data is in a Numpy array of shape `(1, 1)`. This technical detail will be more important when we want to map a vector of features to a multichannel synth, but let's ignore that case for now.\n",
    "\n",
    "What is important is that we can now override the ranges like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq.in_low = 150\n",
    "red2freq.in_high = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's listen to what has changed in the mapping.\n",
    "\n",
    "You probably notice that for most cells the sound remains at its minimum pitch, while for the really bright ones it suddenly chirps up to a really high whistle. Let's go back to using the automatic ranges by setting the `in_low` and `in_high` props to `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq.in_low = None\n",
    "red2freq.in_high = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a listen!\n",
    "\n",
    "We are back to the kind of mapping we started out with. It is striking how little change resulted in such a drastic change in the sound pattern, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by the way, in the previous setting, why did all the cells that were not so bright get the same static low hum? That is because by default we clamp (limit) the mapped values to `out_low` and `out_high`. You probably want clamping to be on for most of the time, but if you know what you are doing, consider turning it off (by specifying `clamp=False`) and then any input value that is below `in_low` or above `in_high` will get proportionally mapped to below `out_low` or `out_high`. But be careful, this can quickly explode values, especially when you use exponential scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last important parameter of a `Mapper` is the exponent to use for the mapping. Think about it as the mapping \"curve\". To get a sense of how `exponent` influences the mapping curve, consider this interactive plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixasonics.ui import ExponentPlot\n",
    "exponent_plot = ExponentPlot()\n",
    "exponent_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For linear mapping we can use (the default) `exponent=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_plot.exponent = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we might want an exponential or a logarithmic curve for our mapping. In the cells below you can find some examples, but feel free to explore the curve by moving the Exponent slider!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_plot.exponent = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_plot.exponent = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_plot.exponent = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent_plot.exponent = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we were mapping to frequencies (measured in Hz) we actually get preceptually more \"linear\" feel in the mapping if the curve is exponential, that is why we set `exponent=2` in our `red2freq` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq.exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have covered the basics of pixasonics, how you can set up an `App` with a pipeline that involves various `Feature`s, `Synth`s and `Mapper`s. Additionally, there are a few \"breakout\" doors designed to integrate pixasonics in your existing workflow or to speed/scale up your sonification sessions:\n",
    "- __Loading Numpy arrays__: An `App` lets you load any matrix data (up to 4 dimensions) as an image to sonify. If you have any specific preprocessing, you can set it up to output Numpy matrices which you can then load into the `App`. Using Numpy arrays also lets you load image sequences or hyper-spectral images (there is no conceptual restriction of the number of color channels or image layers used). By the way, if you don't want to worry about Numpy arrays, you can also directly load HDR images from files.\n",
    "- __Non-real-time rendering__: Instead of having to move the Probe in real-time, perhaps for a longer recording, you can script a \"timeline\" and render it non-real-time. You can also reuse a script to render the same scan pattern on many images.\n",
    "- __Headless mode__: While the `App` class is meant to help with interactive audiovisual exploration, you can totally skip its entire graphical user interface, and control it using its properties. You should also use headless mode if you are outside of a Jupyter Notebook environment, and using Pixasonics in a script.\n",
    "- __Remote control via OSC__: Since all of the `App` settings can be controlled via properties, while having the UI automatically update, it is possible to fully \"remote-control\" an `App` via the [Open Sound Control (OSC)](https://en.wikipedia.org/wiki/Open_Sound_Control) protocol.\n",
    "- __Multichannel `Synth`s__: Providing a list instead of a number for any of a `Synth`s arguments will make it multichannel, which can be used to sonify `Feature`s that have more than one number. And don't worry if the number of features do not match the number of `Synth` channels: in this case `Mapper`s will dynamically resample the feature vector to fit the number of channels.\n",
    "- __`Feature` base class and custom `Feature`s__: While there are lots of convenient abstractions for simple `Feature`s (e.g., `MeanChannelValue`, `MedianRowValue`, etc), these are all just configs for the `Feature` base class, and if you learn how it works, you can intuitively fit the `Feature` to whatever slice of the image you need to focus on, using any of the \"built-in\" (Numpy-based) reducing methods. But you can also create your completely custom `Feature` processors (let's say one that fits a K-means model on the image) by inheriting from the `Feature` base class and overriding two of its methods.\n",
    "- __Custom `Synth`s from a SignalFlow `Patch`__: Since all audio processing in Pixasonics is based on the [SignalFlow](https://signalflow.dev/) library, it is also possible to create custom `Synth`s (that are fully compatible with the rest of Pixasonics) from a SignalFlow [`Patch`](https://signalflow.dev/patch/).\n",
    "- __Multi-target mapping and custom `Mapper`s__: Sometimes you might want to link several `Synth` parameters (on the same or different `Synth`s) to the same `Feature`. In this case, you can create a single `Mapper` with a list of targets, while controlling the output ranges and exponents separately. Just like with `Feature`s and `Synth`s, it is also possible to create custom `Mapper`s.\n",
    "- __Multiple `App`s in the same session__: You can also set up different `App`s with different pipelines (or even images) and use them simultaneously in the same Notebook. For scientists, this can help testing the same sonifications on different images (or sequences), or different sonification setups on the same image data. For creatives, this will let you create different interactive instruments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into the advanced examples, let's import everything we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import Feature, MeanChannelValue\n",
    "from pixasonics.synths import Theremin, Oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading HDR images and Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDR images from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixasonics internally uses the [Pillow](https://pillow.readthedocs.io/en/stable/) library, which allows reading from a wide range of image formats and supports high bit rates! For an example, let's create an `App`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create app\n",
    "app = App()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...then pass the path to a High Dynamic Range (HDR) image. Pixasonics happens to come with both the 8-bit and 16-bit versions of its example images (from the [CELLULAR open dataset](https://zenodo.org/records/8315423)), so you can start experimenting right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load HDR images\n",
    "img_path = \"images/cellular_dataset/single_channel_16bit/Timepoint_005_220518-ST_C03_s1_w1.TIF\"\n",
    "app.load_image_file(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, something is wrong, the image is (almost completely) black! No, no, let me explain.\n",
    "\n",
    "When you read a HDR image (in fact, any image), the `App` creates its own  image to display. This is so that the displayed image can be decoupled from the underlying data, and to make sure it is always 8-bit (which is a requirement for displaying it in a Jupyter Notebook — or a web browser). You can access this displayed image at any time from the `image_displayed` property. We can use the `display` function from IPython and the `Image` module from Pillow to create a static display of `app.image_displayed` in our Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image.fromarray(app.image_displayed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, but this is still quite dark. The reason for this is that in the input image the data did not span the full 16-bit range. This can often be the case with scientific imaging, where microscopes will record their raw values without stretching their ranges (i.e. normalizing them) to the available headroom. Now let's activate normalization in our `App` and have another snapshot of its `image_displayed`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display = True\n",
    "display(Image.fromarray(app.image_displayed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that both inside our `App`'s UI and in the new snapshot the displayed image is normalized, so we can see those _Drosophila_ S2 cells much better. An important takeaway here is that nothing actually happened to the 16-bit image data that we read from the file (and which we can access using the `image` property, here \"`app.image`\"), the `App` merely re-created its displayed image. The previous image was so dark because the 8-bit displayed image was created only using the fact that the input image was 16-bit, so when creating its displayed image the `App` scaled the full 16-bit range (65,536 steps) into the 8-bit range (256 steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is even better than reading HDR image files is the ability to load image data directly from a NumPy array! This opens up a lot of possibilities, such as:\n",
    "- implementing any kind of preprocessing before loading the data into a Pixasonics `App`,\n",
    "- procedurally creating image _volumes_, images with an arbitrary number of color channels or layers.\n",
    "\n",
    "As briefly hinted above, internally images have 4 dimensions in an `App`: __Height, Width, Channels, Layers__. Any of these dimensions can have any size your computer can handle. Let's see a few examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to read from a numpy array, you have to make sure that the array you pass to the `load_image_data` method has up-to 4 dimensions. An example of a 2D (only Height and Width) image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image as a numpy array\n",
    "img_path = \"images/cellular_dataset/merged_8bit/Timepoint_005_220518-ST_C03_s1.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img = np.array(img)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's read two images (that capture red and green fluorescent proteins) and stack them on the third (Channels) dimension, then load our (Height x Width x 2)-shaped image into the `App`. (Don't worry, it will automatically add an empty 3rd channel for its displayed image!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two HDR images and load as numpy array\n",
    "img_path = \"images/cellular_dataset/single_channel_16bit/Timepoint_005_220518-ST_C03_s1_w2.TIF\"\n",
    "img_path2 = \"images/cellular_dataset/single_channel_16bit/Timepoint_005_220518-ST_C03_s1_w1.TIF\"\n",
    "img = Image.open(img_path)\n",
    "img2 = Image.open(img_path2)\n",
    "img = np.array(img)\n",
    "img2 = np.array(img2)\n",
    "img = np.stack([img, img2], axis=-1)\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the fun stuff: stacking image sequences in the Channel and Layer dimensions. So far in all our examples the Channel Offset and Layer Offset sliders (under Display Settings) were always disabled. This is because for the Channel Offset to be enabled you need to load an image that has at least 4 (one more than 3, corresponding to RGB) color channels. In our dataset at hand we don't really need this, but just to showcase how you could \"peek through\" a hyperspectral image (with, for example, 128 color channels), let's read a time-lapse of 5 red channel images and stack them along the Channel dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all red-ch images into arrays and concatenate them in the channel dimension\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "img_files = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs = []\n",
    "for img_file in img_files:\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    img = Image.open(img_path)\n",
    "    img = np.array(img)\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the channel dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, now the Channel Offset slider got enabled (since we have 5 color channels in our image) and we can look through the image channels using a visualization of overlaying 3 consecutive channels and displaying them as RGB. Even when not working with hyperspectral images, this could be used to visualize camera drift (or cell migration), among other things. When `app.display_channel_offset` is 0, we look at an RGB visualization of channels [0, 1, 2], if the Channel Offset is 1, we look at channels [1, 2, 3] and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our current dataset it may make more sense to stack the images on the Layer dimension instead. We can do this just like before, the trick is only that we add an empty dimension to the 2D images, so their shape becomes (Height, Width, 1) before stacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all red-ch images into arrays and concatenate them in the layer dimension\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "img_files = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs = []\n",
    "for img_file in img_files:\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    img = Image.open(img_path)\n",
    "    img = np.array(img)[..., None] # add a new dimension for channels\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Layer Offset slider got enabled so we can conveniently flip through the time-lapse with it. Depending on your machine, and your `App`'s `image_size` (that is 500x500 by default) this could be a bit laggy, since every time the Layer (or Channel) offset is changed a new displayed image is rendered by the `App`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's stack the red and green component images on the Channel dimension while stacking the time steps on the Layer dimension, so our final shape is (Height, Width, 2, 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine red and green channels and all layers\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs = []\n",
    "for img_red, img_green in zip(imgs_red, imgs_green):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img = np.stack([img_red, img_green], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippet above is so handy that we'll re-use it in the following examples as well.\n",
    "\n",
    "Lastly, to use all images in the dataset, let's add \"brightfield\" image to the 3rd (blue) channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use all images in the folder\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs_blue = [f for f in img_files if f.endswith(\"w3.TIF\")] # only blue channel images\n",
    "imgs = []\n",
    "for img_red, img_green, img_blue in zip(imgs_red, imgs_green, imgs_blue):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_path_blue = os.path.join(img_folder, img_blue)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_blue = Image.open(img_path_blue)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img_blue = np.array(img_blue)\n",
    "    img = np.stack([img_red, img_green, img_blue], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may have created a strong blue tint since the number range of the bright-field image is so different. We can undo that tint by enabling global normalization instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display_global = True\n",
    "display(Image.fromarray(app.image_displayed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-real-time rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we looked at how it is possible to make quick real-time recordings using the Record button (or the `recording` boolean property). It is also possible to render sonifications Non-Real-Time (NRT). We can do this by creating a \"timeline\", where we script the size and position of the `App`'s Probe with respect to time values. A timeline is expected to have the following structure:\n",
    "- At the top-level the timeline is just a `List`. \n",
    "    - Each list element should be a `Tuple` of \n",
    "        - a `float`, which denotes the time in seconds,\n",
    "        - and a `Dict`, that contains \"settings\" for that time point, where keys are the names of properties. Currently only the Probe-related properties are supported: `\"probe_width\"`, `\"probe_height\"`, `\"probe_x\"` and `\"probe_y\"`.\n",
    "\n",
    "Let's see a toy example! First let's create another `App`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_2 = App()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...then load all color channels and time steps of the image like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine red and green channels and all layers\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs = []\n",
    "for img_red, img_green in zip(imgs_red, imgs_green):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img = np.stack([img_red, img_green], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app_2.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optionally enable normalization for a clear display:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_2.normalize_display = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...then create a minimal sonification setup (mapping the mean read pixel value under the Probe to the frequency of a sine wave):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example minimal setup\n",
    "\n",
    "# create a feature that will report the mean value of the red channel\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "# attach the feature to the app\n",
    "app_2.attach(mean_red)\n",
    "\n",
    "# create a Theremin, a simple sine wave synth that we will use to sonify the mean pixel value\n",
    "theremin = Theremin()\n",
    "# attach the Theremin to the app\n",
    "app_2.attach(theremin)\n",
    "\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq = Mapper(\n",
    "    mean_red, \n",
    "    theremin[\"frequency\"], \n",
    "    in_low=200,\n",
    "    in_high=500,\n",
    "    exponent=2, # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "    name=\"Red2Freq\")\n",
    "# attach the Mapper to the app\n",
    "app_2.attach(red2freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendering a timeline is essentially scripting Probe movements (and size) through time. In the example below we specify only two timepoints: the start (at 0 s) and the end (specified in the `duration` variable). This example show how to do a \"horizontal scan\", where the Probe is set to become a vertical line (width being 1 pixel and height being the image size) and placed on the left edge of the image (since the Probe height matches the image height the value of `probe_y` does not matter). In the second timepoint we specify that the Probe should be at the right edge of the image.\n",
    "\n",
    "Property values that are not named will repeat their last specified values. Values between adjacent timepoints linearly interpolate. This example will render a *horizontal_scan.wav* file with a duration of 5 seconds. We can use `display` and `Audio` from IPython to embed the result in the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: horizontal scan\n",
    "duration = 5\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 1, # horizontal scanning (a vertical scan line)\n",
    "        \"probe_height\": 500,\n",
    "        \"probe_x\": 0, # start at the left edge of the image\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_x\": 499 # move the probe to the right edge of the image\n",
    "    })\n",
    "]\n",
    "\n",
    "target_filename = \"horizontal_scan.wav\"\n",
    "\n",
    "app_2.render_timeline_to_file(my_timeline, target_filename)\n",
    "\n",
    "display(Audio(target_filename)) # display the resulting audio file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Ignore the \"*Warning: buffer overrun?*\" message from SignalFlow, this is spurious, and will get changed in future versions.)\n",
    "\n",
    "Sounds squeaky! It did not take long for this short file, but you may want to render files much longer. In that case, it is useful to know about the `fps` property of the `App`, which specifies how many times a second the whole sonification pipeline is evaluated. By default `App.fps == 60`, which means all `Feature`s and `Mapper`s are evaluated 60 times a second. When rendering long sonificaitons, you may get away with a perceptually identical result with much less rendering time if you temporarily reduce the `fps`, like in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: long horizontal scan, so we reduce app fps to optimize rendering\n",
    "app_2.fps = 10 # reduce fps to 10 for longer duration rendering\n",
    "duration = 60 # 60 seconds of audio\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 1, # horizontal scanning (a vertical scan line)\n",
    "        \"probe_height\": 500,\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_x\": 499\n",
    "    })\n",
    "]\n",
    "\n",
    "target_filename = \"long_horizontal_scan.wav\"\n",
    "\n",
    "app_2.render_timeline_to_file(my_timeline, target_filename)\n",
    "\n",
    "display(Audio(target_filename))\n",
    "app_2.fps = 60 # restore fps to 60 for interactive use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess how a vertical scan (with a horizontal scan line) setup would look like? Take a look below: it is almost the same as our first example, except that we set the *height* of the Probe to 1 pixel.\n",
    "\n",
    "(By the way, nothing stops you from changing the size of the Probe between timepoints!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: vertical scan\n",
    "duration = 5\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 500,\n",
    "        \"probe_height\": 1, # vertical scanning (a horizontal scan line)\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_y\": 499\n",
    "    })\n",
    "]\n",
    "\n",
    "target_filename = \"vertical_scan.wav\"\n",
    "\n",
    "app_2.render_timeline_to_file(my_timeline, target_filename)\n",
    "\n",
    "display(Audio(target_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headless mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want the real-time sonification pipeline, but don't need the graphical interface? Maybe you want to build your own UI, or just live code? Then headless mode is for you!\n",
    "\n",
    "Since it is possible to control `App`s via properties, you can initialize them with `headless=True` to skip rendering the built-in UI entirely. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3 = App(headless=True) # create a new app in headless mode (no UI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, there is no UI popping up. But our `App` is still ready. Let's load an image into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.load_image_file(\"images/test.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and then add a minimal sonification pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_red_3 = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "app_3.attach(mean_red_3)\n",
    "theremin_3 = Theremin(name=\"MySine\")\n",
    "app_3.attach(theremin_3)\n",
    "red2freq_3 = Mapper(mean_red_3, theremin_3[\"frequency\"], exponent=2, name=\"Red2Freq\")\n",
    "app_3.attach(red2freq_3)\n",
    "app_3.features, app_3.synths, app_3.mappers # check that the app has the expected features, synths and mappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon checking the `features`, `synths`, and `mappers` properties, we see that the pipeline is there. Similarly, we can check the current size and position of the Probe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.probe_width, app_3.probe_height, app_3.probe_x, app_3.probe_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move it away from the top left corner:\n",
    "\n",
    "(feel free to evaluate the cell above after you evaluated the cell below to check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.probe_x, app_3.probe_y = 200, 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's enable audio in the `App`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.audio = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always access the global audio graph from any `App`'s `graph` property. This will return the `AudioGraph` singleton object from SignalFlow. Let's check the `status` of the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.graph.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output is silent. Now let's unmute the audio (equivalent to activating the Probe on the visual display) and then check the graph `status` again. (You should also hear a low, 60Hz sine tone if your speakers are good.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.unmuted = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move the Probe again, and listen if the sound changes. (Be careful with your speaker levels!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.probe_x, app_3.probe_y = 50, 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Auch!) It changed indeed, now let's mute the `App` (equivalent to deactivating the Probe) again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.unmuted = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a silly little script snippet to move the Probe real-time while it is unmuted. Since we loaded *test.jpg* up top, which is a simple gradient, you should hear an upwards glide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a little loop to unmute the probe, then move it around then turn it off\n",
    "import time\n",
    "app_3.unmuted = True\n",
    "app_3.probe_x, app_3.probe_y = 250, 0\n",
    "while app_3.probe_y < 350:\n",
    "    app_3.probe_y += 1\n",
    "    time.sleep(0.01)\n",
    "app_3.unmuted = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's swap the image and then try evaluating the snippet above again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_3.load_image_file(\"images/cellular_dataset/merged_8bit/Timepoint_001_220518-ST_C03_s1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully that gives you an idea about headless mode. While it is an optional feature when you are working in a Notebook, it is crucial when you are working in an offline script. Here is an example script snippet that gets all image files in a folder and then renders the same horizontal scanning sonification path on all images in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a for loop where for each image in the folder we load a headless app and render a timeline in nrt mode\n",
    "img_folder = \"images/cellular_dataset/merged_8bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "\n",
    "# example: horizontal scan\n",
    "duration = 5\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 1,\n",
    "        \"probe_height\": 500,\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_x\": 499\n",
    "    })\n",
    "]\n",
    "_app = App(headless=True, nrt=True) # create an App\n",
    "# only need to create processor objects once and attach them to the App\n",
    "_mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\") # we will attach it later, when there is an image loaded\n",
    "_theremin = Theremin(name=\"MySine\")\n",
    "_red2freq = Mapper(_mean_red, _theremin[\"frequency\"], exponent=2, name=\"Red2Freq\")\n",
    "# these we can already attach\n",
    "_app.attach(_theremin)\n",
    "_app.attach(_red2freq)\n",
    "# loop over all images in the folder, load the image, and render the timeline\n",
    "for img_file in img_files:\n",
    "    print(f\"Processing {img_file}\")\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    _app.load_image_file(img_path)\n",
    "    _app.attach(_mean_red) # attach the feature to the app when there is some image loaded. it will only happen the first time, and will be ignored after\n",
    "    target_filename = img_file.replace(\".jpg\", \".wav\")\n",
    "    _app.render_timeline_to_file(my_timeline, target_filename)\n",
    "    print(f\"Saved {target_filename}\")\n",
    "    display(Audio(target_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above process rendered wave files via the `render_timeline_to_file` method. You might render sonifications as part of a larger workflow (maybe training a neural net to couple sound with images?), and in that case you may want to render sonifications to NumPy arrays instead via the `render_timeline_to_array` method. Here we still go for displaying these arrays as sound buffers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing but render np.arrays instead\n",
    "img_folder = \"images/cellular_dataset/merged_8bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "\n",
    "# example: horizontal scan\n",
    "duration = 5\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 1,\n",
    "        \"probe_height\": 500,\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_x\": 499\n",
    "    })\n",
    "]\n",
    "_app = App(headless=True, nrt=True) # create an App\n",
    "# only need to create processor objects once and attach them to the App\n",
    "_mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\") # we will attach it later, when there is an image loaded\n",
    "_theremin = Theremin(name=\"MySine\")\n",
    "_red2freq = Mapper(_mean_red, _theremin[\"frequency\"], exponent=2, name=\"Red2Freq\")\n",
    "# these we can already attach\n",
    "_app.attach(_theremin)\n",
    "_app.attach(_red2freq)\n",
    "# loop over all images in the folder, load the image, and render the timeline\n",
    "for img_file in img_files:\n",
    "    print(f\"Processing {img_file}\")\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    _app.load_image_file(img_path)\n",
    "    _app.attach(_mean_red) # attach the feature to the app when there is some image loaded. it will only happen the first time, and will be ignored after\n",
    "    buf = _app.render_timeline_to_array(my_timeline)\n",
    "    display(Audio(buf, rate=_app.sample_rate, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote control via OSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `App`s can be controlled via their properties, you could also include your sonification pipeline(s) in a broader setup via Open Sound Control (OSC). The following is a minimal example to help you get started.\n",
    "\n",
    "First, you need to install the `python-osc` package (uncomment and evaluate the line below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-osc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the components we need from `python-osc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythonosc import dispatcher, osc_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is yet another instance of our toy sonification setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy setup...\n",
    "app_4 = App()\n",
    "app_4.load_image_file(\"images/test.jpg\")\n",
    "mean_red_4 = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "app_4.attach(mean_red_4)\n",
    "theremin_4 = Theremin(name=\"MySine\")\n",
    "app_4.attach(theremin_4)\n",
    "red2freq_4 = Mapper(mean_red_4, theremin_4[\"frequency\"], exponent=2, name=\"Red2Freq\")\n",
    "app_4.attach(red2freq_4)\n",
    "app_4.audio = True # already start the audio graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to define all the helper functions (\"the API\") through which our OSC messages will be routed (by a `Dispatcher`) to `App` properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create helper functions\n",
    "def handle_unmuted(address, value):\n",
    "    print(f\"Unmuted: {value}\")\n",
    "    app_4.unmuted = value\n",
    "def handle_probe_x(address, value):\n",
    "    print(f\"Probe X: {value}\")\n",
    "    app_4.probe_x = value\n",
    "def handle_probe_y(address, value):\n",
    "    print(f\"Probe Y: {value}\")\n",
    "    app_4.probe_y = value\n",
    "def handle_probe_xy(address, x, y):\n",
    "    print(f\"Probe XY: {x}, {y}\")\n",
    "    app_4.probe_x = x\n",
    "    app_4.probe_y = y\n",
    "def handle_probe_width(address, value):\n",
    "    print(f\"Probe Width: {value}\")\n",
    "    app_4.probe_width = value\n",
    "def handle_probe_height(address, value):\n",
    "    print(f\"Probe Height: {value}\")\n",
    "    app_4.probe_height = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the `Dispatcher` and set up the map between OSC addresses and these helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dispatcher and map addresses to functions\n",
    "dispatcher = dispatcher.Dispatcher()\n",
    "dispatcher.map(\"/unmuted\", handle_unmuted)\n",
    "dispatcher.map(\"/probe/x\", handle_probe_x)\n",
    "dispatcher.map(\"/probe/y\", handle_probe_y)\n",
    "dispatcher.map(\"/probe/xy\", handle_probe_xy)\n",
    "dispatcher.map(\"/probe/w\", handle_probe_width)\n",
    "dispatcher.map(\"/probe/h\", handle_probe_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the OSC server. Beware, this cell is blocking, and will run until you interrupt it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a server (will hang until keyboard interrupt!)\n",
    "ip = \"127.0.0.1\"\n",
    "port = 12345\n",
    "server = osc_server.ThreadingOSCUDPServer(\n",
    "    (ip, port), dispatcher)\n",
    "print(\"Serving on {}\".format(server.server_address))\n",
    "server.serve_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other side, you could set up another app to *send* OSC messages to control our sonification `App`. Here is a simple exampe in Max:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/osc_example_max_patch.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the compressed patcher you see above, in case you want to give it a try. You need to copy the cell below, then, in Max go to *File* > *New From Clipboard*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<pre><code>\n",
    "----------begin_max5_patcher----------\n",
    "973.3oc0X0zbaBCD8r8uBUtzC0IAILXSu0+AY50NYxHiUsUFPhAIbbZl7eu5\n",
    "CHAmXCJFhiyEX75EV81c0aehGGOxaAeKQ3A9I3OfQidb7nQFSZCip98HuL71\n",
    "jTrv3lWBOKivjdSr+mjrUZrKHRv0E7EDvZBc05mcHGKSVSYqtsfjHsAJZV3k\n",
    "9S.APys34leDeoO3lpmgtz7J4Kt6Bz75WDqLixRIRyx.VYbQ4hEojlVTtwKk\n",
    "094uieB5RiuHswmFOVeYhi3Vjpd3h1PUjuAH1av.C3P9Mfkf9OS3C80tT+hJ\n",
    "vYDIo3VBCWAE+8jFhaMMrKnqsZMIeHmXWgddfaNBjyH2qVBuof+C.zgrQr4F\n",
    "J9xP0Uz9qwA96Ebn2G3nplxiCekYKboxNEYtEZQTSr3XQL.9NJhnCVDm.7Vf\n",
    "YqFzhYdAImvVBtJWuE9p0NjMr4An+aSGMgL5yqusalp6oKksA0PneShpfVIp\n",
    "l8UgnZZb3GHQU3WLhp5rgKDUnfyahpJrzahJj+WEhp6cHaTSTgZinBN+7jnJ\n",
    "mlHs6nAEpbIQ.jbfbMA7q77uK.zL7Jxsl8qUOcJkQR3kLYyZydxNASMYmJR.\n",
    "aNJX1ARN8jaa.yIY7MDK6cKUdq9RDpAsMZ5AfV3G.scv.BXIe0pTBnjkUJIK\n",
    "aAzHjsD5vrJ3zOhYU6XMmSssfpFqga2e4xbgd2eJOAmtlKj.HR0H2RVwN5tZ\n",
    "PNbZ6b.uGwJGCpr0xNWsU725V1W035H+MDNDjYG+bpNnt6tWFtKycTaUsyuS\n",
    "F8pYTaenMjNeW0zgsA04mcP8N01bGPGxAwUyFBsUCJ391EJMDwsovvG1fyUy\n",
    "1dX3E8oJc7EYDcWsf0zOubyIBHIOeCNszXSk21ygAaMETn+pQ64E3nZScpYR\n",
    "OxPGZ96uIYbIAjvYxBdJ.CtltEK3LZhPqAqSRL60fJ0mG5vistw9upXWejLz\n",
    "KmHq+yivRYQIs6Nh3lhDOlCTztdC8xv3ZiVfSw1dWgeTegenav+4MP8A7l0l\n",
    "4T.u5y5Zvm19tYDAurHo9UUMOE7BBWRDRJCKobVCezJkzNs2jtqAZ9oJPybH\n",
    "PyGf3D4PbzqEXOiSnCwIX.vyzSTbBPmpNA8WfsaHMHQx2kHAGhH4XW2PDGXG\n",
    "wIZ.hCxEBn5za+hjKajPCQGNxkhDZPvjK8cvgfsCdx3ugtrqM9MAxNIDmmug\n",
    "THpb1DCkJf63lQtFMldpYx1eZFw5UP1Pq82LL2CWnTFHUxBJKLqKusQVEMdY\n",
    "bkjZlRDg9oGqQmJjFEFLkv.QN1BDiPjwOM9+MkkxtC\n",
    "-----------end_max5_patcher-----------\n",
    "</code></pre>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multichannel Synths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most exciting features of Pixasonics is the ability to work with multichannel `Synth`s. Every parameter of a `Synth` can be defined as a list of values, instead of a single number. In this case the `Synth` will grow additional channels to synthesize all parameters, and mix its output to stereo (2 channels).\n",
    "\n",
    "Multichannel `Synth`s can be useful to sonify a list of `Feature`s as a distribution of a parameter. For example pixel intensities could be mapped to individual frequencies in an oscillator bank.\n",
    "\n",
    "Note that multichannel expansion in Pixasonics does not behave identically to how it does in SignalFlow. While in the latter, having a 2-channel parameter and a 11-channel parameter would mean repeating the first, 2-channel parameter to meet the channel count of the other one (\"tiling\", repeating signal [1, 2] to become [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1]), while in Pixasonics `Synth`s the parameter with less channels will *interpolate* its channels (signal [1, 2] becomes [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2]).\n",
    "\n",
    "Additionally, `Mapper`s can dynamically project between a `Feature` vector and the number of channels in a `Synth`, so even if the length of the `Feature` vector changes over time (which we will demonstrate in a bit) there is no need to redo the mapping.\n",
    "\n",
    "Let's see how all this works in action. First we create an `App`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_4 = App()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load our usual image sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine red and green channels and all layers\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs = []\n",
    "for img_red, img_green in zip(imgs_red, imgs_green):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img = np.stack([img_red, img_green], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app_4.load_image_data(img) # load as numpy array\n",
    "app_4.normalize_display = True # normalize the displayed image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a pair of `Feature`s. The first one will reduce the Probe rectangle into a *column* (vertical line) by taking the average value of every row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean row value (will change the number of features depending of the Probe height)\n",
    "mean_row = Feature(\n",
    "    name=\"Mean Row\",\n",
    "    target_dim=0, # mean value of each row (for horizontal scanning)\n",
    ")\n",
    "app_4.attach(mean_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second `Feature` will reduce the rectangle into a single *row* (horizontal line) by averaging columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean col value (will change the number of features depending of the Probe width)\n",
    "mean_col = Feature(\n",
    "    name=\"Mean Col\",\n",
    "    target_dim=1, # mean value of each col (for horizontal scanning)\n",
    ")\n",
    "app_4.attach(mean_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to try these out. Check their indicated number of features. Now go to the Probe Settings pane and change the Probe dimensions. Then check the `Feature`s again.\n",
    "\n",
    "You probably noticed that the number of reported features changes according to the Probe width of Height. Now let's create those oscillator banks we will use to listen to these changing feature vectors. We will use the `Oscillator` built-in `Synth`, and create two 16-channel instances, where the frequencies will be initialized with lists (of 16 elements), instead of single values.\n",
    "\n",
    "Here is one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 16-voice oscillator bank of sawtooth waves, spread across the stereo field\n",
    "num_voices = 16\n",
    "freqs = [440 for i in range(num_voices)]\n",
    "osc_bank = Oscillator(frequency=freqs, waveform=\"saw\", name=\"OSCBank\") # test multichannel\n",
    "app_4.attach(osc_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the other one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 16-voice oscillator bank of sawtooth waves, spread across the stereo field\n",
    "num_voices = 16\n",
    "freqs = [440 for i in range(num_voices)]\n",
    "osc_bank_2 = Oscillator(frequency=freqs, waveform=\"saw\", name=\"OSCBank2\") # test multichannel\n",
    "app_4.attach(osc_bank_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to look at the UI cards of these `Synth`s. Instead of sliders, they now have text fields, indicating the current parameter values. Note that the *panning* parameter automatically created a spread (between -1 and 1, corresponding to hard-left and hard-right) populating the `Synth` channels in the stereo field from left to right.\n",
    "\n",
    "Now the fun part: the mapping. Let's map both the Mean Row and Mean Col features to corresponding oscillator banks, so that the distribution of frequencies reflect the distribution of mean pixel values. Actually, we don't have to do anything tricky, we can use the `Mapper`s just like we did before, and they will automatically project between the number of features and the number of `Synth` channels.\n",
    "\n",
    "Here is a `Mapper` mapping the Mean Row `Feature` vector to the frequencies of `osc_bank`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping where each row value will be mapped to a different channel of the oscillator bank\n",
    "row2freq = Mapper(mean_row, osc_bank[\"frequency\"], exponent=2, out_high=1000, name=\"Row2Freq\")\n",
    "app_4.attach(row2freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and another `Mapper` bridging the Mean Col `Feature` and `osc_bank_2`'s frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping where each col value will be mapped to a different channel of the oscillator bank\n",
    "col2freq = Mapper(mean_col, osc_bank_2[\"frequency\"], exponent=2, out_high=1000, name=\"Col2Freq\")\n",
    "app_4.attach(col2freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to enable audio processing for the `App`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_4.audio = True # enable audio output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out! Sounds wild, doesn't it? You can use this setup to \"listen through\" all time steps (layers) in the image, to, for example, find cells that are hidden on the first time step. Can you find one like that? Try smaller Probe sizes so you can better focus on an area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Feature base class and custom Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_5 = App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine red and green channels and all layers\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs = []\n",
    "for img_red, img_green in zip(imgs_red, imgs_green):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img = np.stack([img_red, img_green], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app_5.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults\n",
    "base_feature = Feature(\n",
    "    filter_rows=None,\n",
    "    filter_columns=None,\n",
    "    filter_channels=None,\n",
    "    filter_layers=None,\n",
    "    target_dim=2,\n",
    "    reduce_method=\"mean\",\n",
    "    name=\"Feature\"\n",
    ")\n",
    "app_5.attach(base_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pixel value\n",
    "mean_pix = Feature(name=\"Mean\") # defaults to mean pixel value, channel dimension is kept\n",
    "app_5.attach(mean_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median pixel value\n",
    "median_pix = Feature(name=\"Median\", reduce_method=\"median\")\n",
    "app_5.attach(median_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean row value (will change the number of features depending of the Probe height)\n",
    "mean_row = Feature(\n",
    "    name=\"Mean Row\",\n",
    "    target_dim=0, # mean value of each row (for horizontal scanning)\n",
    ")\n",
    "app_5.attach(mean_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean col value (will change the number of features depending of the Probe width)\n",
    "mean_col = Feature(\n",
    "    name=\"Mean Col\",\n",
    "    target_dim=1, # mean value of each col (for horizontal scanning)\n",
    ")\n",
    "app_5.attach(mean_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median, filter for the red (first) channel only\n",
    "median_red = Feature(\n",
    "    name=\"Median Red\",\n",
    "    target_dim=2, # channel dim\n",
    "    filter_channels=0, # only use the first channel (red)\n",
    "    reduce_method=\"median\"\n",
    ")\n",
    "app_5.attach(median_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_6 = App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine red and green channels and all layers\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = sorted(os.listdir(img_folder))\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs = []\n",
    "for img_red, img_green in zip(imgs_red, imgs_green):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img = np.stack([img_red, img_green], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app_6.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom feature that will return random values\n",
    "class MyRandomFeature(Feature):\n",
    "    def __init__(self, name=\"MyRandomFeature\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def process_image(self, mat):\n",
    "        return np.random.rand(*mat.shape)\n",
    "    \n",
    "    def compute(self, mat):\n",
    "        num_features = mat.shape[self.target_dim]\n",
    "        return np.random.rand(num_features)\n",
    "    \n",
    "my_random_feature = MyRandomFeature()\n",
    "app_6.attach(my_random_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_6.detach(my_random_feature)\n",
    "my_random_feature = MyRandomFeature()\n",
    "# change target dim to height (0)\n",
    "my_random_feature.target_dim = 0\n",
    "app_6.attach(my_random_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom feature that will fit a KMeans model to the image and return the histogram of cluster assignments\n",
    "from sklearn.cluster import KMeans # pip install scikit-learn if you don't have it\n",
    "\n",
    "class KMeansFeature(Feature):\n",
    "    def __init__(self, n_clusters=3, name=\"KMeansFeature\"):\n",
    "        super().__init__(name=name)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.kmeans = None\n",
    "        self._original_shape = None\n",
    "\n",
    "    def _reshape_for_kmeans(self, mat):\n",
    "        \"\"\"Helper to reshape 4D matrix to 2D for KMeans\"\"\"\n",
    "        mat_reshaped = np.moveaxis(mat, self.target_dim, 0)\n",
    "        return mat_reshaped.reshape(mat_reshaped.shape[0], -1)\n",
    "\n",
    "    def process_image(self, mat):\n",
    "        self._original_shape = mat.shape\n",
    "        features = self._reshape_for_kmeans(mat)\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters).fit(features.T)\n",
    "\n",
    "        # Get cluster assignments and reshape back to original dimensions\n",
    "        labels = self.kmeans.predict(features.T)\n",
    "        other_dims = [s for i, s in enumerate(mat.shape) if i != self.target_dim]\n",
    "        self.transformed_image = np.expand_dims(\n",
    "            labels.reshape(*other_dims), \n",
    "            axis=self.target_dim\n",
    "        )\n",
    "        return self.transformed_image\n",
    "    \n",
    "    def compute(self, mat):\n",
    "        if self.kmeans is None:\n",
    "            raise ValueError(\"KMeans model has not been fitted. Call process_image first.\")\n",
    "        features = self._reshape_for_kmeans(mat)\n",
    "        labels = self.kmeans.predict(features.T)\n",
    "        # Compute histogram of cluster assignments\n",
    "        hist, _ = np.histogram(labels, bins=range(self.n_clusters + 1))\n",
    "        return hist.astype(float) / hist.sum() # normalize to sum to 1\n",
    "\n",
    "kmeans_feature = KMeansFeature(n_clusters=10)\n",
    "app_6.attach(kmeans_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a multichannel Theremin that has its frequencies in a harmonic series\n",
    "fundamental_freq = 110\n",
    "num_harmonics = kmeans_feature.n_clusters\n",
    "freqs = fundamental_freq * np.arange(1, num_harmonics + 1)\n",
    "print(\"Frequencies:\",freqs)\n",
    "osc = Theremin(frequency=freqs, name=\"KMeansOsc\")\n",
    "app_6.attach(osc)\n",
    "\n",
    "# create a Mapper that will map the KMeans cluster histogram to the amplitude of the Theremin \n",
    "k2amp = Mapper(kmeans_feature, osc[\"amplitude\"], exponent=1, name=\"K2Amp\")\n",
    "app_6.attach(k2amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the KMeans clusters\n",
    "import matplotlib.pyplot as plt # pip install matplotlib seaborn if you don't have them\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a colormap with distinct colors for each cluster\n",
    "n_clusters = kmeans_feature.n_clusters\n",
    "colors = sns.color_palette(\"husl\", n_colors=n_clusters)\n",
    "colormap = {i: colors[i] for i in range(n_clusters)}\n",
    "\n",
    "# Get the cluster assignments from transformed_image\n",
    "cluster_image = kmeans_feature.transformed_image[:, :, 0, 0]\n",
    "print(cluster_image.shape)\n",
    "\n",
    "# Create RGB image where each cluster gets a unique color\n",
    "rgb_image = np.zeros((*cluster_image.shape, 3))\n",
    "for cluster_id, color in colormap.items():\n",
    "    mask = cluster_image == cluster_id\n",
    "    rgb_image[mask] = color\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(app_6.image_displayed)  # Show the currently displayed image\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('K-means Clusters')\n",
    "plt.imshow(rgb_image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom synths using the Synth base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "from pixasonics.synths import Synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_7 = App()\n",
    "app_7.audio = True\n",
    "app_7.interaction_mode = \"toggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the SignalFlow patch as a subclass of sf.Patch (usual procedure)\n",
    "class SimpleAMPatch(sf.Patch):\n",
    "    def __init__(self, carr_freq=440, mod_freq=1, mod_depth=0.25):\n",
    "        super().__init__()\n",
    "        # define params\n",
    "        carr_freq = self.add_input(\"carrier_freq\", carr_freq)\n",
    "        mod_freq = self.add_input(\"mod_freq\", mod_freq)\n",
    "        mod_depth = self.add_input(\"mod_depth\", mod_depth)\n",
    "        # build the patch for Amplitude Modulation\n",
    "        modulator = (sf.SineOscillator(mod_freq) * mod_depth) + (1 - mod_depth)\n",
    "        carrier = sf.SineOscillator(carr_freq)\n",
    "        out = carrier * modulator\n",
    "        self.set_output(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PatchSpec out of the Patch - we will use this to create a Synth\n",
    "am_patch_spec = SimpleAMPatch().to_spec()\n",
    "am_patch_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-off version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict for params (for UI sliders)\n",
    "am_params = {\n",
    "    \"carrier_freq\": {\n",
    "        \"min\": 20,\n",
    "        \"max\": 8000,\n",
    "        \"unit\": \"Hz\",\n",
    "        \"scale\": \"log\"\n",
    "    },\n",
    "    \"mod_freq\": {\n",
    "        \"min\": 0.1,\n",
    "        \"max\": 100,\n",
    "        \"unit\": \"Hz\",\n",
    "        \"scale\": \"log\"\n",
    "    },\n",
    "    \"mod_depth\": {\n",
    "        \"min\": 0,\n",
    "        \"max\": 1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_am_synth = Synth(am_patch_spec, am_params, name=\"SimpleAMSynth\", add_amplitude=True, add_panning=True)\n",
    "app_7.attach(simple_am_synth)\n",
    "simple_am_synth.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-term version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAM(Synth):\n",
    "    def __init__(\n",
    "            self,\n",
    "            carrier_frequency=440,\n",
    "            modulator_frequency=1,\n",
    "            modulator_depth=0.25,\n",
    "            name=\"SimpleAM\"\n",
    "    ):\n",
    "        _spec = SimpleAMPatch(\n",
    "                carr_freq=carrier_frequency,\n",
    "                mod_freq=modulator_frequency,\n",
    "                mod_depth=modulator_depth\n",
    "        ).to_spec()\n",
    "        _params = {\n",
    "            \"carrier_freq\": {\n",
    "                \"min\": 20,\n",
    "                \"max\": 8000,\n",
    "                \"unit\": \"Hz\",\n",
    "                \"scale\": \"log\"\n",
    "            },\n",
    "            \"mod_freq\": {\n",
    "                \"min\": 0.1,\n",
    "                \"max\": 100,\n",
    "                \"unit\": \"Hz\",\n",
    "                \"scale\": \"log\"\n",
    "            },\n",
    "            \"mod_depth\": {\n",
    "                \"min\": 0,\n",
    "                \"max\": 1,\n",
    "            }\n",
    "        }\n",
    "        # call the parent constructor\n",
    "        super().__init__(_spec, params_dict=_params, name=name, add_amplitude=True, add_panning=True)\n",
    "\n",
    "def __repr__(self): # not really necessary, but nice to have\n",
    "    return f\"SimpleAM {self.id}: {self.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_am = SimpleAM()\n",
    "app_7.attach(simple_am)\n",
    "simple_am.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-target mapping and custom Mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixasonics.synths import Theremin, FilteredNoise\n",
    "from pixasonics.features import MeanChannelValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-target mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_8 = App()\n",
    "app_8.load_image_file(\"images/test.jpg\")\n",
    "mean_red_8 = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "app_8.attach(mean_red_8)\n",
    "theremin_8 = Theremin()\n",
    "app_8.attach(theremin_8)\n",
    "filtered_noise_8 = FilteredNoise()\n",
    "app_8.attach(filtered_noise_8)\n",
    "red2freqs = Mapper(mean_red_8, [theremin_8[\"frequency\"], filtered_noise_8[\"cutoff\"]], name=\"Red2Freqs\") # a list of targets!\n",
    "app_8.attach(red2freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freqs.in_low, red2freqs.in_high, red2freqs.out_low, red2freqs.out_high, red2freqs.exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is possible to set the ranges and exponent for each target separately\n",
    "red2freqs.out_low = [60, 50]\n",
    "red2freqs.out_high = [4000, 5000]\n",
    "red2freqs.exponent = [1, 2]\n",
    "\n",
    "red2freqs.out_low, red2freqs.out_high, red2freqs.exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_9 = App()\n",
    "app_9.load_image_file(\"images/test.jpg\")\n",
    "mean_pix_9 = MeanChannelValue()\n",
    "app_9.attach(mean_pix_9)\n",
    "simple_am_9 = SimpleAM() # from the custom synth example above\n",
    "app_9.attach(simple_am_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB2AM(Mapper):\n",
    "    def __init__(self, source, target_am, name=\"RGB2AM\"):\n",
    "        # here we already define a multitarget mapping, expecting the SimpleAM parameters\n",
    "        super().__init__(source, [target_am[\"carrier_freq\"], target_am[\"mod_freq\"], target_am[\"mod_depth\"]], name=name)\n",
    "    \n",
    "    def map(self, in_data):\n",
    "        r, g, b = in_data # this is going to be the current values from a MeanChannelValue on an RGB image\n",
    "        # map red to carrier frequency\n",
    "        carr_freq = np.interp(r, [0, 255], [60, 1000])\n",
    "        # map green to modulator frequency\n",
    "        mod_freq = np.interp(g, [0, 255], [0.1, 100])\n",
    "        # map blue to modulator depth\n",
    "        mod_depth = np.interp(b, [0, 2], [0, 1])\n",
    "        # return them as a list (as expected by the multitarget mapping we set up above)\n",
    "        return [carr_freq, mod_freq, mod_depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2am = RGB2AM(mean_pix_9, simple_am_9)\n",
    "app_9.attach(rgb2am)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Apps and the AppRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixasonics.core import AppRegistry\n",
    "AppRegistry._apps # list of all currently running Apps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixasonics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
