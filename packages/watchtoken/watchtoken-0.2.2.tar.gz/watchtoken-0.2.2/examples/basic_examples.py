"""
Podstawowe przyk≈Çady u≈ºycia biblioteki WatchToken.
"""

from watchtoken import TokenCounter, FileLogger


def basic_usage():
    """Podstawowe u≈ºycie - liczenie token√≥w i sprawdzanie limit√≥w."""
    print("=== Podstawowe u≈ºycie ===")
    
    # Inicjalizacja TokenCounter
    tc = TokenCounter(model="gpt-4-turbo", limit=100)
    
    # Przyk≈Çadowy tekst
    text = "Napisz streszczenie ksiƒÖ≈ºki 'Pan Tadeusz' Adama Mickiewicza."
    
    # Liczenie token√≥w
    tokens = tc.count(text)
    print(f"Tekst: '{text}'")
    print(f"Liczba token√≥w: {tokens}")
    
    # Sprawdzenie czy przekracza limit
    if tc.is_over(text):
        print("‚ö†Ô∏è Tekst przekracza limit!")
    else:
        print("‚úÖ Tekst mie≈õci siƒô w limicie")
    
    # Sprawdzenie pozosta≈Çych token√≥w
    remaining = tc.get_remaining_tokens(text)
    print(f"Pozosta≈Çe tokeny: {remaining}")


def cost_estimation():
    """Przyk≈Çad estymacji koszt√≥w."""
    print("\n=== Estymacja koszt√≥w ===")
    
    tc = TokenCounter("gpt-4-turbo")
    
    prompts = [
        "Hello world!",
        "Napisz artyku≈Ç o sztucznej inteligencji (500 s≈Ç√≥w).",
        "Przet≈Çumacz nastƒôpujƒÖcy tekst na jƒôzyk angielski: 'Dzie≈Ñ dobry, jak siƒô masz?'"
    ]
    
    for prompt in prompts:
        tokens = tc.count(prompt)
        
        # Estymacja kosztu dla r√≥≈ºnych d≈Çugo≈õci odpowiedzi
        cost_short = tc.estimate_cost(prompt, output_tokens=50)
        cost_medium = tc.estimate_cost(prompt, output_tokens=200)
        cost_long = tc.estimate_cost(prompt, output_tokens=500)
        
        print(f"\nPrompt: '{prompt[:50]}...'")
        print(f"Input tokens: {tokens}")
        print(f"Koszt (50 tokens output): ${cost_short:.6f}")
        print(f"Koszt (200 tokens output): ${cost_medium:.6f}")
        print(f"Koszt (500 tokens output): ${cost_long:.6f}")


def model_comparison():
    """Por√≥wnanie r√≥≈ºnych modeli."""
    print("\n=== Por√≥wnanie modeli ===")
    
    models = ["gpt-3.5-turbo", "gpt-4-turbo", "claude-3-sonnet", "gemini-pro"]
    text = "Analyze the impact of artificial intelligence on modern education."
    
    print(f"Tekst do analizy: '{text}'")
    print(f"{'Model':<20} {'Tokens':<8} {'Koszt (100 out)':<15} {'Limit':<10}")
    print("-" * 60)
    
    for model in models:
        try:
            tc = TokenCounter(model)
            tokens = tc.count(text)
            cost = tc.estimate_cost(text, output_tokens=100)
            info = tc.get_model_info()
            
            print(f"{model:<20} {tokens:<8} ${cost:<14.6f} {info['context_length']:<10}")
        except Exception as e:
            print(f"{model:<20} Error: {e}")


def callback_example():
    """Przyk≈Çad u≈ºycia callback√≥w."""
    print("\n=== Callbacki przy przekroczeniu limitu ===")
    
    def on_limit_exceeded(tokens, limit, model):
        print(f"üö® ALERT: Model {model} przekroczy≈Ç limit!")
        print(f"   U≈ºyto: {tokens} token√≥w")
        print(f"   Limit: {limit} token√≥w")
        print(f"   Przekroczenie: {tokens - limit} token√≥w")
    
    # TokenCounter z ma≈Çym limitem i callbackiem
    tc = TokenCounter(
        model="gpt-3.5-turbo", 
        limit=10,
        on_limit_exceeded=on_limit_exceeded
    )
    
    # Ten tekst przekroczy limit
    long_text = """
    Sztuczna inteligencja to dziedzina informatyki, kt√≥ra zajmuje siƒô tworzeniem 
    system√≥w zdolnych do wykonywania zada≈Ñ wymagajƒÖcych inteligencji, gdy sƒÖ 
    wykonywane przez ludzi. Obejmuje to uczenie maszynowe, przetwarzanie jƒôzyka 
    naturalnego, rozpoznawanie obraz√≥w i wiele innych technologii.
    """
    
    print("Sprawdzanie d≈Çugiego tekstu...")
    tc.check_limit(long_text.strip())


def logging_example():
    """Przyk≈Çad logowania u≈ºycia token√≥w."""
    print("\n=== Logowanie u≈ºycia ===")
    
    # Utworzenie loggera do pliku
    logger = FileLogger("token_usage.log")
    
    # TokenCounter z automatycznym logowaniem
    tc = TokenCounter("gpt-4-turbo", logger=logger, auto_log=True)
    
    # Przyk≈Çadowe u≈ºycie (zostanie automatycznie zalogowane)
    prompts = [
        "Translate: Hello world",
        "Summarize: The quick brown fox jumps over the lazy dog",
        "Explain: quantum computing"
    ]
    
    print("Przetwarzanie prompt√≥w z logowaniem...")
    for prompt in prompts:
        tokens = tc.count(prompt)
        cost = tc.estimate_cost(prompt, output_tokens=30)
        print(f"  '{prompt}' -> {tokens} tokens, ${cost:.6f}")
    
    print("‚úÖ Wszystkie operacje zosta≈Çy zalogowane do 'token_usage.log'")


def batch_processing():
    """Przyk≈Çad przetwarzania wsadowego."""
    print("\n=== Przetwarzanie wsadowe ===")
    
    from watchtoken.utils import calculate_batch_cost
    
    texts = [
        "What is machine learning?",
        "Explain neural networks in simple terms",
        "How does natural language processing work?",
        "What are the applications of AI in healthcare?",
        "Describe the future of artificial intelligence"
    ]
    
    # Obliczenie kosztu dla ca≈Çej paczki
    batch_info = calculate_batch_cost(
        texts=texts,
        model="gpt-4-turbo", 
        output_tokens_per_text=150
    )
    
    print(f"Analiza paczki {batch_info['total_texts']} tekst√≥w:")
    print(f"  Model: {batch_info['model']}")
    print(f"  Ca≈Çkowite tokeny input: {batch_info['total_input_tokens']}")
    print(f"  Ca≈Çkowite tokeny output: {batch_info['total_output_tokens']}")
    print(f"  Ca≈Çkowity koszt: ${batch_info['total_cost']:.6f}")
    print(f"  ≈öredni koszt na tekst: ${batch_info['average_cost_per_text']:.6f}")
    
    print("\nSzczeg√≥≈Çy dla ka≈ºdego tekstu:")
    for i, text_info in enumerate(batch_info['text_breakdown']):
        print(f"  {i+1}. '{text_info['text']}' -> {text_info['input_tokens']} tokens, ${text_info['cost']:.6f}")


def model_info_example():
    """Przyk≈Çad pobierania informacji o modelach."""
    print("\n=== Informacje o modelach ===")
    
    from watchtoken.utils import get_model_summary
    
    # Pobranie podsumowania wszystkich modeli
    models_summary = get_model_summary()
    
    print(f"{'Model':<20} {'Provider':<12} {'Context':<8} {'Input$/1K':<10} {'Output$/1K':<10}")
    print("-" * 70)
    
    for model in models_summary:
        print(
            f"{model['name']:<20} "
            f"{model['provider']:<12} "
            f"{model['context_length']:<8} "
            f"${model['input_cost_per_1k_tokens']:<9.3f} "
            f"${model['output_cost_per_1k_tokens']:<9.3f}"
        )


if __name__ == "__main__":
    print("üïê WatchToken - Przyk≈Çady u≈ºycia\n")
    
    try:
        basic_usage()
        cost_estimation()
        model_comparison() 
        callback_example()
        logging_example()
        batch_processing()
        model_info_example()
        
        print("\n‚úÖ Wszystkie przyk≈Çady wykonane pomy≈õlnie!")
        
    except Exception as e:
        print(f"\n‚ùå WystƒÖpi≈Ç b≈ÇƒÖd: {e}")
        print("Upewnij siƒô, ≈ºe zainstalowa≈Çe≈õ wszystkie wymagane zale≈ºno≈õci:")
        print("pip install tiktoken")
