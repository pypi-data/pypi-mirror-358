{% raw %}name: '{% endraw %}{{ project_name }}{% raw %}'
version: '0.0.1'
config-version: 2

require-dbt-version: [">=1.6.0", "<2.0.0"]

profile: 'snowplow_autogen'

dispatch:
  - macro_namespace: dbt
    search_order: ['snowplow_utils', 'dbt']

model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]
docs-paths: ["docs"]

target-path: "target"
clean-targets:
  - "target"
  - "dbt_packages"
  - "logs"
  
models:
  {% endraw %}{{ project_name }}{% raw %}:
    +materialized: table
    +file_format:  "{{ 'delta' if target.type not in ['spark'] else 'iceberg'}}"
    +incremental_strategy: "{{ None if target.type not in ['spark'] else 'merge' }}"
    +bind: false
    base:
      +tags: "base"
      manifest:
        +schema: "snowplow_manifest"
      scratch:
        +schema: "scratch"
        +tags: "scratch"
    filtered_events:
      +schema: "derived"
      +tags: "{% endraw %}{{ project_name }}{% raw %}_incremental"
      scratch:
        +schema: "scratch"
        +tags: "scratch"
    daily_aggregates:
      +schema: "derived"
      +tags: "{% endraw %}{{ project_name }}{% raw %}_incremental"
      scratch:
        +schema: "scratch"
        +tags: "scratch"
      manifest:
        +schema: "snowplow_manifest"
    attributes:
      +schema: "derived"
      +tags: "{% endraw %}{{ project_name }}{% raw %}_incremental"

vars:
  {% endraw %}{{ project_name }}{% raw %}:

    # OPERATION & LOGIC
    
    # filtered_events
    snowplow__start_date: '2025-01-01' # date from where it starts looking for events based on both load and derived_tstamp
    snowplow__app_id: [] # already gets applied in base_events_this_run
    snowplow__backfill_limit_days: 1 # limit backfill increments for the filtered_events_table
    
    # daily_aggregates
    snowplow__late_event_lookback_days: 5 # the number of days to allow for late arriving data to be reprocessed fully in the daily aggregate table
    snowplow__min_late_events_to_process: 1 # the number of total daily events that have been skipped in previous runs, if it falls within the late_event_lookback_days, if the treshold is reached, those events will be processed in the daily aggregate model
    
    snowplow__allow_refresh: false # if true, the snowplow_incremental_manifest will be dropped when running with a --full-refresh flag

    # WAREHOUSE & TRACKER
    
    snowplow__dev_target_name: dev
    snowplow__databricks_catalog: "hive_metastore"
    # snowplow__atomic_schema: 'atomic' # Only set if not using 'atomic' schema for Snowplow events data
    # snowplow__database: # Only set if not using target.database for Snowplow events data -- WILL BE IGNORED FOR DATABRICKS
    # snowplow__events_table: "events" # Only set if not using 'events' table for Snowplow events data
    snowplow__events: "{{ source('atomic', 'events') }}" # for dev use only

    # Entity configuration
    snowplow__entity_key: '{% endraw %}{{ entity_key }}{% raw %}' # The entity key to use for aggregations

on-run-start:
  - "{{ snowplow_utils.snowplow_delete_from_manifest(var('models_to_remove',[])) }}"

on-run-end:
  - "{{ snowplow_utils.snowplow_incremental_post_hook_t(package_name='{% endraw %}{{ project_name }}{% raw %}', incremental_manifest_table_name='{% endraw %}{{ project_name }}{% raw %}_incremental_manifest', base_events_this_run_table_name='{% endraw %}{{ project_name }}{% raw %}_base_events_this_run') }}"
  - "{{ snowplow_utils.grant_usage_on_schemas_built_into(var('snowplow__grant_schemas', true)) }}"
{% endraw %}
