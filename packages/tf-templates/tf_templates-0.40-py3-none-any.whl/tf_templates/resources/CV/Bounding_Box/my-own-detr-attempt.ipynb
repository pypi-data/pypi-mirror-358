{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 23902,
     "sourceType": "datasetVersion",
     "datasetId": 18276
    }
   ],
   "dockerImageVersionId": 31040,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os\nimport torch\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms.v2 import functional as F\nfrom torchvision.ops import box_convert\nfrom einops import rearrange",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:35.147807Z",
     "iopub.execute_input": "2025-05-28T01:34:35.148247Z",
     "iopub.status.idle": "2025-05-28T01:34:44.692276Z",
     "shell.execute_reply.started": "2025-05-28T01:34:35.148224Z",
     "shell.execute_reply": "2025-05-28T01:34:44.691621Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.693442Z",
     "iopub.execute_input": "2025-05-28T01:34:44.693837Z",
     "iopub.status.idle": "2025-05-28T01:34:44.756035Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.693809Z",
     "shell.execute_reply": "2025-05-28T01:34:44.755281Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# DataLoader",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader\nfrom torchvision.transforms import v2",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.757055Z",
     "iopub.execute_input": "2025-05-28T01:34:44.757313Z",
     "iopub.status.idle": "2025-05-28T01:34:44.794246Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.757296Z",
     "shell.execute_reply": "2025-05-28T01:34:44.793531Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# from datasets import load_dataset\n# train_dataset = load_dataset(import os\nimport torch\nimport torchvision.transforms.v2\nfrom torch.utils.data.dataset import Dataset\nimport xml.etree.ElementTree as ET\nfrom torchvision import tv_tensors\nfrom torchvision.io import read_image\n\n\ndef load_images_and_anns(im_sets, label2idx, ann_fname, split):\n    r\"\"\"\n    Method to get the xml files and for each file\n    get all the objects and their ground truth detection\n    information for the dataset\n    :param im_sets: Sets of images to consider\n    :param label2idx: Class Name to index mapping for dataset\n    :param ann_fname: txt file containing image names{trainval.txt/test.txt}\n    :param split: train/test\n    :return:\n    \"\"\"\n    im_infos = []\n    for im_set in im_sets:\n        im_names = []\n        # Fetch all image names in txt file for this imageset\n        for line in open(os.path.join(\n                im_set, 'ImageSets', 'Main', '{}.txt'.format(ann_fname))):\n            im_names.append(line.strip())\n\n        # Set annotation and image path\n        ann_dir = os.path.join(im_set, 'Annotations')\n        im_dir = os.path.join(im_set, 'JPEGImages')\n\n        for im_name in im_names:\n            ann_file = os.path.join(ann_dir, '{}.xml'.format(im_name))\n            im_info = {}\n            ann_info = ET.parse(ann_file)\n            root = ann_info.getroot()\n            size = root.find('size')\n            width = int(size.find('width').text)\n            height = int(size.find('height').text)\n            im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]\n            im_info['filename'] = os.path.join(\n                im_dir, '{}.jpg'.format(im_info['img_id'])\n            )\n            im_info['width'] = width\n            im_info['height'] = height\n            detections = []\n            for obj in ann_info.findall('object'):\n                det = {}\n                label = label2idx[obj.find('name').text]\n                difficult = int(obj.find('difficult').text)\n                bbox_info = obj.find('bndbox')\n                bbox = [\n                    int(bbox_info.find('xmin').text) - 1,\n                    int(bbox_info.find('ymin').text) - 1,\n                    int(bbox_info.find('xmax').text) - 1,\n                    int(bbox_info.find('ymax').text) - 1\n                ]\n                det['label'] = label\n                det['bbox'] = bbox\n                det['difficult'] = difficult\n                detections.append(det)\n            im_info['detections'] = detections\n            # Because we are using 25 as num_queries,\n            # so we ignore all images in VOC with greater\n            # than 25 target objects.\n            # This is okay, since this just means we are\n            # ignoring a small number of images(15 to be precise)\n            if len(detections) <= 25:\n                im_infos.append(im_info)\n    print('Total {} images found'.format(len(im_infos)))\n    return im_infos\n\n\nclass VOCDataset(Dataset):\n    def __init__(self, split, im_sets, im_size=640):\n        self.split = split\n\n        # Imagesets for this dataset instance (VOC2007/VOC2007+VOC2012/VOC2007-test)\n        self.im_sets = im_sets\n        self.fname = 'trainval' if self.split == 'train' else 'test'\n        self.im_size = im_size\n        self.im_mean = [123.0, 117.0, 104.0]\n        self.imagenet_mean = [0.485, 0.456, 0.406]\n        self.imagenet_std = [0.229, 0.224, 0.225]\n\n        # Train and test transformations\n        self.transforms = {\n            'train': torchvision.transforms.v2.Compose([\n                torchvision.transforms.v2.RandomHorizontalFlip(p=0.5),\n                torchvision.transforms.v2.RandomZoomOut(fill=self.im_mean),\n                torchvision.transforms.v2.RandomIoUCrop(),\n                torchvision.transforms.v2.RandomPhotometricDistort(),\n                torchvision.transforms.v2.Resize(size=(self.im_size, self.im_size)),\n                torchvision.transforms.v2.SanitizeBoundingBoxes(\n                    labels_getter=lambda transform_input:\n                    (transform_input[1][\"labels\"], transform_input[1][\"difficult\"])),\n                torchvision.transforms.v2.ToPureTensor(),\n                torchvision.transforms.v2.ToDtype(torch.float32, scale=True),\n                torchvision.transforms.v2.Normalize(mean=self.imagenet_mean,\n                                                    std=self.imagenet_std)\n\n            ]),\n            'test': torchvision.transforms.v2.Compose([\n                torchvision.transforms.v2.Resize(size=(self.im_size, self.im_size)),\n                torchvision.transforms.v2.ToPureTensor(),\n                torchvision.transforms.v2.ToDtype(torch.float32, scale=True),\n                torchvision.transforms.v2.Normalize(mean=self.imagenet_mean,\n                                                    std=self.imagenet_std)\n            ]),\n        }\n\n        classes = [\n            'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'\n        ]\n        classes = sorted(classes)\n        # We need to add background class as well with 0 index\n        classes = ['background'] + classes\n\n        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n        print(self.idx2label)\n        self.images_info = load_images_and_anns(self.im_sets,\n                                                self.label2idx,\n                                                self.fname,\n                                                self.split)\n\n    def __len__(self):\n        return len(self.images_info)\n\n    def __getitem__(self, index):\n        im_info = self.images_info[index]\n        im = read_image(im_info['filename'])\n\n        # Get annotations for this image\n        targets = {}\n        targets['boxes'] = tv_tensors.BoundingBoxes(\n            [detection['bbox'] for detection in im_info['detections']],\n            format='XYXY', canvas_size=im.shape[-2:])\n        targets['labels'] = torch.as_tensor(\n            [detection['label'] for detection in im_info['detections']])\n        targets['difficult'] = torch.as_tensor(\n            [detection['difficult']for detection in im_info['detections']])\n\n        # Transform the image and targets\n        transformed_info = self.transforms[\"test\"](im, targets)\n        im_tensor, targets = transformed_info\n\n        h, w = im_tensor.shape[-2:]\n\n        # Boxes returned are in x1y1x2y2 format normalized from 0-1\n        wh_tensor = torch.as_tensor([[w, h, w, h]]).expand_as(targets['boxes'])\n        return im_tensor, targets, im_info['filename']\n# test_dataset = load_dataset(\"detection-datasets/coco\", split='val', streaming=True)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.795946Z",
     "iopub.execute_input": "2025-05-28T01:34:44.796181Z",
     "iopub.status.idle": "2025-05-28T01:34:44.815170Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.796163Z",
     "shell.execute_reply": "2025-05-28T01:34:44.814467Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# dataset",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.816026Z",
     "iopub.execute_input": "2025-05-28T01:34:44.816279Z",
     "iopub.status.idle": "2025-05-28T01:34:44.832652Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.816257Z",
     "shell.execute_reply": "2025-05-28T01:34:44.832043Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nimport torch\nimport torchvision.transforms.v2\nfrom torch.utils.data.dataset import Dataset\nimport xml.etree.ElementTree as ET\nfrom torchvision import tv_tensors\nfrom torchvision.io import read_image\n\n\ndef load_images_and_anns(im_sets, label2idx, ann_fname, split):\n    r\"\"\"\n    Method to get the xml files and for each file\n    get all the objects and their ground truth detection\n    information for the dataset\n    :param im_sets: Sets of images to consider\n    :param label2idx: Class Name to index mapping for dataset\n    :param ann_fname: txt file containing image names{trainval.txt/test.txt}\n    :param split: train/test\n    :return:\n    \"\"\"\n    im_infos = []\n    for im_set in im_sets:\n        im_names = []\n        # Fetch all image names in txt file for this imageset\n        for line in open(os.path.join(\n                im_set, 'ImageSets', 'Main', '{}.txt'.format(ann_fname))):\n            im_names.append(line.strip())\n\n        # Set annotation and image path\n        ann_dir = os.path.join(im_set, 'Annotations')\n        im_dir = os.path.join(im_set, 'JPEGImages')\n\n        for im_name in im_names:\n            ann_file = os.path.join(ann_dir, '{}.xml'.format(im_name))\n            im_info = {}\n            ann_info = ET.parse(ann_file)\n            root = ann_info.getroot()\n            size = root.find('size')\n            width = int(size.find('width').text)\n            height = int(size.find('height').text)\n            im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]\n            im_info['filename'] = os.path.join(\n                im_dir, '{}.jpg'.format(im_info['img_id'])\n            )\n            im_info['width'] = width\n            im_info['height'] = height\n            detections = []\n            for obj in ann_info.findall('object'):\n                det = {}\n                label = label2idx[obj.find('name').text]\n                difficult = int(obj.find('difficult').text)\n                bbox_info = obj.find('bndbox')\n                bbox = [\n                    int(bbox_info.find('xmin').text) - 1,\n                    int(bbox_info.find('ymin').text) - 1,\n                    int(bbox_info.find('xmax').text) - 1,\n                    int(bbox_info.find('ymax').text) - 1\n                ]\n                det['label'] = label\n                det['bbox'] = bbox\n                det['difficult'] = difficult\n                detections.append(det)\n            im_info['detections'] = detections\n            # Because we are using 25 as num_queries,\n            # so we ignore all images in VOC with greater\n            # than 25 target objects.\n            # This is okay, since this just means we are\n            # ignoring a small number of images(15 to be precise)\n            if len(detections) <= 25:\n                im_infos.append(im_info)\n    print('Total {} images found'.format(len(im_infos)))\n    return im_infos\n\n\nclass VOCDataset(Dataset):\n    def __init__(self, split, im_sets, im_size=640):\n        self.split = split\n\n        # Imagesets for this dataset instance (VOC2007/VOC2007+VOC2012/VOC2007-test)\n        self.im_sets = im_sets\n        self.fname = 'trainval' if self.split == 'train' else 'test'\n        self.im_size = im_size\n        self.im_mean = [123.0, 117.0, 104.0]\n        self.imagenet_mean = [0.485, 0.456, 0.406]\n        self.imagenet_std = [0.229, 0.224, 0.225]\n\n        # Train and test transformations\n        self.transforms = {\n            'train': torchvision.transforms.v2.Compose([\n                torchvision.transforms.v2.RandomHorizontalFlip(p=0.5),\n                torchvision.transforms.v2.RandomZoomOut(fill=self.im_mean),\n                torchvision.transforms.v2.RandomIoUCrop(),\n                torchvision.transforms.v2.RandomPhotometricDistort(),\n                torchvision.transforms.v2.Resize(size=(self.im_size, self.im_size)),\n                torchvision.transforms.v2.SanitizeBoundingBoxes(\n                    labels_getter=lambda transform_input:\n                    (transform_input[1][\"labels\"], transform_input[1][\"difficult\"])),\n                torchvision.transforms.v2.ToPureTensor(),\n                torchvision.transforms.v2.ToDtype(torch.float32, scale=True),\n                torchvision.transforms.v2.Normalize(mean=self.imagenet_mean,\n                                                    std=self.imagenet_std)\n\n            ]),\n            'test': torchvision.transforms.v2.Compose([\n                torchvision.transforms.v2.Resize(size=(self.im_size, self.im_size)),\n                torchvision.transforms.v2.ToPureTensor(),\n                torchvision.transforms.v2.ToDtype(torch.float32, scale=True),\n                torchvision.transforms.v2.Normalize(mean=self.imagenet_mean,\n                                                    std=self.imagenet_std)\n            ]),\n        }\n\n        classes = [\n            'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'\n        ]\n        classes = sorted(classes)\n        # We need to add background class as well with 0 index\n        classes = classes + ['background']\n\n        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n        print(self.idx2label)\n        self.images_info = load_images_and_anns(self.im_sets,\n                                                self.label2idx,\n                                                self.fname,\n                                                self.split)\n\n    def __len__(self):\n        return len(self.images_info)\n\n    def __getitem__(self, index):\n        im_info = self.images_info[index]\n        im = read_image(im_info['filename'])\n\n        # Get annotations for this image\n        targets = {}\n        targets['boxes'] = tv_tensors.BoundingBoxes(\n            [detection['bbox'] for detection in im_info['detections']],\n            format='XYXY', canvas_size=im.shape[-2:])\n        targets['labels'] = torch.as_tensor(\n            [detection['label'] for detection in im_info['detections']])\n        targets['difficult'] = torch.as_tensor(\n            [detection['difficult']for detection in im_info['detections']])\n\n        # Transform the image and targets\n        transformed_info = self.transforms[\"test\"](im, targets)\n        im_tensor, targets = transformed_info\n\n        h, w = im_tensor.shape[-2:]\n\n        # Boxes returned are in x1y1x2y2 format normalized from 0-1\n        wh_tensor = torch.as_tensor([[w, h, w, h]]).expand_as(targets['boxes'])\n        targets[\"boxes\"] = targets[\"boxes\"] / wh_tensor\n        return im_tensor, targets, im_info['filename']",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.833431Z",
     "iopub.execute_input": "2025-05-28T01:34:44.833652Z",
     "iopub.status.idle": "2025-05-28T01:34:44.851905Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.833635Z",
     "shell.execute_reply": "2025-05-28T01:34:44.851133Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "transforms = v2.Compose([\n    # v2.RGB(),\n    # v2.ToImage(),  # Convert to tensor, only needed if you had a PIL image\n    # v2.PILToTensor(),\n    # v2.ToDtype(torch.uint8, scale=True),  # optional, most input are already uint8 at this point\n    # ...\n    # v2.Resize(size=(320, 320), antialias=True),  # Or Resize(antialias=True)\n    # ...\n    v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.852581Z",
     "iopub.execute_input": "2025-05-28T01:34:44.852779Z",
     "iopub.status.idle": "2025-05-28T01:34:44.871403Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.852763Z",
     "shell.execute_reply": "2025-05-28T01:34:44.870722Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# def preprocess_img(batch):\n#     img = batch['image']\n#     img = transforms(img)\n#     w = batch['width']\n#     h = batch['height']\n#     obj = batch['objects']\n#     boxes_xywh = torch.tensor(obj['bbox'])\n#     boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')\n#     boxes_xyxy[:, [0, 2]] = boxes_xyxy[:, [0, 2]] / w\n#     boxes_xyxy[:, [1, 3]] = boxes_xyxy[:, [1, 3]] / h\n#     boxes_xyxy = torch.clamp(boxes_xyxy, 0, 1)\n#     batch['new_image'] = img\n#     batch['boxes_xyxy'] = boxes_xyxy\n#     batch['labels'] = torch.tensor(obj[\"category\"])\n#     obj_mask = torch.zeros(320, 320)\n#     for obx in boxes_xyxy:\n#         x1, y1, x2, y2 = tuple(torch.round(obx * 320).to(torch.int32))\n#         # print(x1, y1, x2, y2)\n#         obj_mask[x1:x2, y1:y2] = 1\n#     batch['mask'] = obj_mask\n#     batch['image'] = -1\n#     return batch",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.872169Z",
     "iopub.execute_input": "2025-05-28T01:34:44.872508Z",
     "iopub.status.idle": "2025-05-28T01:34:44.887574Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.872478Z",
     "shell.execute_reply": "2025-05-28T01:34:44.886889Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# import random\n\n# # train_ds = dataset['train'].shuffle().select(range(0, 20000))\n# train_ds = list(train_dataset.take(10000))\n# # test_ds = dataset['val'].shuffle().select(range(0, 1000))\n# test_ds = list(test_dataset.take(1000))\n# len(train_ds), len(test_ds)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.888311Z",
     "iopub.execute_input": "2025-05-28T01:34:44.888691Z",
     "iopub.status.idle": "2025-05-28T01:34:44.901677Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.888673Z",
     "shell.execute_reply": "2025-05-28T01:34:44.901099Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from torchvision import tv_tensors\ndef preprocess_ds(batch):\n    img = batch[0]\n    w = img.shape[1]\n    h = img.shape[2]\n    obj = batch[1]\n    boxes_xyxy = obj['boxes'].to(torch.float32)\n    output = {}\n    output['new_image'] = img\n    output['boxes_xyxy'] = tv_tensors.BoundingBoxes(\n        boxes_xyxy,\n        format=\"XYXY\", canvas_size=(1, 1)\n    )\n    output['labels'] = obj[\"labels\"]\n    obj_mask = torch.zeros(320, 320)\n    for obx in boxes_xyxy:\n        x1, y1, x2, y2 = tuple(torch.round(obx * 320).to(torch.int32))\n        # print(x1, y1, x2, y2)\n        obj_mask[x1:x2, y1:y2] = 1\n    output['mask'] = obj_mask\n    return output",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.904179Z",
     "iopub.execute_input": "2025-05-28T01:34:44.904384Z",
     "iopub.status.idle": "2025-05-28T01:34:44.915555Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.904370Z",
     "shell.execute_reply": "2025-05-28T01:34:44.914949Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport argparse\nimport os\nimport numpy as np\nimport yaml\nimport random\nfrom tqdm import tqdm\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.optim.lr_scheduler import MultiStepLR\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.backends.mps.is_available():\n    device = torch.device('mps')\n    print('Using mps')\n\n\ndef collate_function(data):\n    return tuple(zip(*data))\n\n\nvoc = VOCDataset('train',\n                 im_sets=[\"/kaggle/input/pascal-voc-2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007\"],\n                 im_size=320)\ntrain_ds = []\nfor i in tqdm(range(len(voc))):\n    train_ds.append(preprocess_ds(voc[i]))\nvoc2 = VOCDataset('test',\n                 im_sets=[\"/kaggle/input/pascal-voc-2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007\"],\n                 im_size=320)\ntest_ds = []\nfor i in tqdm(range(len(voc2))):\n    test_ds.append(preprocess_ds(voc2[i]))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:34:44.916244Z",
     "iopub.execute_input": "2025-05-28T01:34:44.916515Z",
     "iopub.status.idle": "2025-05-28T01:38:53.028594Z",
     "shell.execute_reply.started": "2025-05-28T01:34:44.916500Z",
     "shell.execute_reply": "2025-05-28T01:38:53.027822Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_ds[0]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.029714Z",
     "iopub.execute_input": "2025-05-28T01:38:53.029970Z",
     "iopub.status.idle": "2025-05-28T01:38:53.056709Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.029941Z",
     "shell.execute_reply": "2025-05-28T01:38:53.055877Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "len(train_ds), len(test_ds)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.057542Z",
     "iopub.execute_input": "2025-05-28T01:38:53.057795Z",
     "iopub.status.idle": "2025-05-28T01:38:53.062533Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.057755Z",
     "shell.execute_reply": "2025-05-28T01:38:53.061842Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_augs = v2.Compose([\n    v2.RandomRotation(30),\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.ClampBoundingBoxes(),\n])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.063154Z",
     "iopub.execute_input": "2025-05-28T01:38:53.063373Z",
     "iopub.status.idle": "2025-05-28T01:38:53.077956Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.063358Z",
     "shell.execute_reply": "2025-05-28T01:38:53.077326Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Random Resize\nr_resize_list = []\nfor i in range(-2, 3):\n    for j in range(-2, 3):\n        r_resize_list.append(v2.Resize((320 + i * 32, 320 + j * 32)))\nlen(r_resize_list)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.078617Z",
     "iopub.execute_input": "2025-05-28T01:38:53.078803Z",
     "iopub.status.idle": "2025-05-28T01:38:53.096482Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.078789Z",
     "shell.execute_reply": "2025-05-28T01:38:53.095833Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "r_resize_list[12]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.097364Z",
     "iopub.execute_input": "2025-05-28T01:38:53.097758Z",
     "iopub.status.idle": "2025-05-28T01:38:53.108819Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.097742Z",
     "shell.execute_reply": "2025-05-28T01:38:53.108297Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "r_resize_list[1].size",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.109579Z",
     "iopub.execute_input": "2025-05-28T01:38:53.109833Z",
     "iopub.status.idle": "2025-05-28T01:38:53.122652Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.109812Z",
     "shell.execute_reply": "2025-05-28T01:38:53.122108Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import random\n\ndef collate_fn(in_batch):\n    images = []\n    targets = []\n    # if random.random() < 0.1:\n    #     idx = random.randint(0, 24)\n    # else:\n    #     idx = 12\n    for batch in in_batch:\n        img = batch['new_image']\n        lbl = batch['labels']\n        bbox = batch['boxes_xyxy']\n        mask = batch['mask']\n        img, bbox = train_augs(img, bbox)\n        # img, bbox = r_resize_list[idx](img, bbox)\n        # bbox[:, [0, 2]] = bbox[:, [0, 2]] / r_resize_list[idx].size[0]\n        # bbox[:, [1, 3]] = bbox[:, [1, 3]] / r_resize_list[idx].size[1]\n        images.append(img)\n        temp = {\n            \"labels\": lbl,\n            \"boxes\": bbox,\n            \"mask\": mask\n        }\n        targets.append(temp)\n    return torch.stack(images), targets",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.123357Z",
     "iopub.execute_input": "2025-05-28T01:38:53.124004Z",
     "iopub.status.idle": "2025-05-28T01:38:53.137137Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.123987Z",
     "shell.execute_reply": "2025-05-28T01:38:53.136448Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_loader = DataLoader(\n    train_ds,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn,\n)\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn,\n)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.137830Z",
     "iopub.execute_input": "2025-05-28T01:38:53.138051Z",
     "iopub.status.idle": "2025-05-28T01:38:53.154989Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.138031Z",
     "shell.execute_reply": "2025-05-28T01:38:53.154451Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\nfor batch in train_loader:\n    print(batch[0].shape)\n    print(batch[1][0])\n    break",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:53.155578Z",
     "iopub.execute_input": "2025-05-28T01:38:53.155733Z",
     "iopub.status.idle": "2025-05-28T01:38:55.322134Z",
     "shell.execute_reply.started": "2025-05-28T01:38:53.155721Z",
     "shell.execute_reply": "2025-05-28T01:38:55.321398Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import cProfile",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.323551Z",
     "iopub.execute_input": "2025-05-28T01:38:55.323803Z",
     "iopub.status.idle": "2025-05-28T01:38:55.328030Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.323776Z",
     "shell.execute_reply": "2025-05-28T01:38:55.327267Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# cProfile.run(\n# \"\"\"\n# for batch in train_loader:\n#     print(batch[0].shape)\n#     print(batch[1][0])\n#     break\n# \"\"\"\n# , sort='tottime')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.328880Z",
     "iopub.execute_input": "2025-05-28T01:38:55.329132Z",
     "iopub.status.idle": "2025-05-28T01:38:55.342516Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.329115Z",
     "shell.execute_reply": "2025-05-28T01:38:55.341974Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Naive DETR Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.343067Z",
     "iopub.execute_input": "2025-05-28T01:38:55.343218Z",
     "iopub.status.idle": "2025-05-28T01:38:55.358692Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.343206Z",
     "shell.execute_reply": "2025-05-28T01:38:55.358070Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class ResNetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.path1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=stride)\n        self.path2 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1),\n            nn.GELU(),\n            nn.GroupNorm(1, out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride),\n        )\n\n    def forward(self, x):\n        x1 = self.path1(x)\n        x2 = self.path2(x)\n        return x1 + x2",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.359638Z",
     "iopub.execute_input": "2025-05-28T01:38:55.359900Z",
     "iopub.status.idle": "2025-05-28T01:38:55.371526Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.359878Z",
     "shell.execute_reply": "2025-05-28T01:38:55.370832Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class SimpleBackBone(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.resnet1 = ResNetBlock(in_channels, 16, 2)\n        self.resnet2 = ResNetBlock(16, 32, 2)\n        self.resnet3 = ResNetBlock(32, 64, 2)\n        self.resnet4 = ResNetBlock(64, out_channels, 2)\n        self.whole_skip = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=16) # monkeyed yes\n\n    def forward(self, x):\n        x1 = self.resnet1(x)\n        x1 = self.resnet2(x1)\n        x1 = self.resnet3(x1)\n        x1 = self.resnet4(x1)\n        x = x1 + self.whole_skip(x)\n        return x",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.372224Z",
     "iopub.execute_input": "2025-05-28T01:38:55.372481Z",
     "iopub.status.idle": "2025-05-28T01:38:55.385255Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.372460Z",
     "shell.execute_reply": "2025-05-28T01:38:55.384573Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class BBoxHead(nn.Module):\n    # Just alot of MLP layers\n    # Adding Residual because I think that's why it's hard to train\n    def __init__(self, in_shape, out_shape, middle_dim=256, num_layers=4, act_fn=nn.GELU):\n        super().__init__()\n        self.in_proj = nn.Linear(in_shape, middle_dim)\n        self.out_proj = nn.Linear(middle_dim, out_shape)\n        self.middle = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.Linear(middle_dim, middle_dim),\n                    act_fn(),\n                )\n             for i in range(num_layers - 2)\n            ]\n        )\n        self.act_fn = act_fn\n\n    def forward(self, x):\n        x = self.act_fn()(self.in_proj(x))\n        for layer in self.middle:\n            x = layer(x) + x\n        return nn.Sigmoid()(self.out_proj(x)) # bounding box is scaled between 0 and 1",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.385970Z",
     "iopub.execute_input": "2025-05-28T01:38:55.386394Z",
     "iopub.status.idle": "2025-05-28T01:38:55.403709Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.386377Z",
     "shell.execute_reply": "2025-05-28T01:38:55.402988Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torchvision.ops as ops\n\nclass DeformConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = ops.DeformConv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.offset = nn.Sequential(\n            nn.Conv2d(in_channels, 2 * 1 * 3 * 3, kernel_size=3, stride=1, padding=1),\n            nn.Tanh(),\n        )\n        self.offset_scale = nn.Parameter(torch.tensor(3.0))\n    def forward(self, x):\n        pred_offset = self.offset(x) * self.offset_scale\n        x = self.conv(x, pred_offset)\n        x = nn.GELU()(x)\n        return x\n\nclass Projection(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.dconv1 = DeformConvBlock(in_channels, out_channels)\n        self.dconv2 = DeformConvBlock(out_channels, out_channels)\n        self.dconv3 = DeformConvBlock(out_channels, out_channels)\n    def forward(self, x):\n        x = self.dconv1(x)\n        x = self.dconv2(x) + x\n        x = self.dconv3(x) + x\n        return x",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.404467Z",
     "iopub.execute_input": "2025-05-28T01:38:55.404786Z",
     "iopub.status.idle": "2025-05-28T01:38:55.419793Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.404749Z",
     "shell.execute_reply": "2025-05-28T01:38:55.419065Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from torchvision.models import *\n\nclass SimpleDETR(nn.Module):\n    def __init__(self, hidden_dim, ffn_dim, n_heads, num_layers, num_classes):\n        super().__init__()\n\n        # self.backbone = SimpleBackBone(3, 64)\n        self.backbone = resnet18(weights=ResNet18_Weights.DEFAULT)\n        self.backbone.avgpool = nn.Identity()\n        self.backbone.fc = nn.Identity()\n\n        self.proj = Projection(512, hidden_dim)\n\n        # self.proj = nn.Sequential(\n        #     nn.Conv2d(512, 128, kernel_size=3, padding=1, stride=1),\n        #     nn.GELU(),\n        #     nn.Conv2d(128, hidden_dim, kernel_size=1, padding=0, stride=1),\n        #     nn.GELU(),\n        # )\n        sine_embed = torch.zeros(hidden_dim, 32, 32)\n        sine_embed = sine_embed + torch.arange(32).expand(hidden_dim, 32, 32)\n        sine_embed = sine_embed + torch.arange(32).expand(hidden_dim, 32, 32).permute(0, 2, 1)\n        sine_embed = sine_embed + torch.arange(hidden_dim).expand(32, 32, hidden_dim).permute(2, 0, 1)\n        sine_embed = torch.sin(sine_embed).unsqueeze(0)\n        self.pos_embed = nn.Parameter(sine_embed) # 32 is max image width/height after backbone\n        self.pos_embed_weight = nn.Parameter(torch.tensor(0.1))\n        self.transformer = nn.Transformer(\n            hidden_dim, \n            n_heads, \n            num_layers[0], \n            num_layers[1], \n            ffn_dim, \n            activation = \"gelu\",\n            dropout=0.1,\n            batch_first=True\n        )\n\n        self.query_1 = nn.Sequential(\n            nn.Conv1d(64, hidden_dim, kernel_size=1, stride=1, padding=0),\n            nn.GELU(),\n            nn.Linear(80 * 80, 10)\n        )\n        self.query_2 = nn.Sequential(\n            nn.Conv1d(128, hidden_dim, kernel_size=1, stride=1, padding=0),\n            nn.GELU(),\n            nn.Linear(40 * 40, 10)\n        )\n        self.query_3 = nn.Sequential(\n            nn.Conv1d(256, hidden_dim, kernel_size=1, stride=1, padding=0),\n            nn.GELU(),\n            nn.Linear(20 * 20, 10)\n        )\n        self.box_query = nn.Linear(10 * 10, 10)\n        self.box_embed = nn.Parameter(torch.rand(1, 10, hidden_dim))\n        self.query_weights = nn.Parameter(torch.tensor([0.05, 0.05, 0.2, 1.0, 0.1]))\n\n        self.fc_class = nn.Linear(hidden_dim, num_classes + 1) # +1 for the <no object> class\n        self.fc_bbox = BBoxHead(hidden_dim, 4, num_layers=3)\n\n        self.impt_pts = nn.Sequential(\n            nn.Conv2d(hidden_dim, 128, kernel_size=3, padding=1, stride=1),\n            nn.GELU(),\n            nn.Conv2d(128, 2, kernel_size=1, padding=0, stride=1),\n            nn.Softmax(dim=1),\n        ) # custom auxiliary loss to predict whether a point is important\n\n        self.bbox_sizes = BBoxHead(hidden_dim, 2, num_layers=3) # custom auxiliary loss to predict size of bboxes\n        # hopefully help with small ones\n        \n    def forward(self, x):\n        batch_size = x.shape[0]\n        w = x.shape[2]\n        h = x.shape[3]\n        \n        # BACKBONE PORTION\n        # x = self.backbone(x)\n        x = self.backbone.relu(self.backbone.bn1(self.backbone.conv1(x)))\n        x = self.backbone.maxpool(x)\n        # print(x.shape)\n        # 80 by 80\n        x1 = self.backbone.layer1(x)\n        # print(x1.shape)\n        # 80 by 80\n        x2 = self.backbone.layer2(x1)\n        # print(x2.shape)\n        # 40 by 40\n        x3 = self.backbone.layer3(x2)\n        # print(x3.shape)\n        # 20 by 20\n        x = self.backbone.layer4(x3)\n        # print(x.shape)\n        # END BACKBONE PORTION\n        \n        x = x.reshape(batch_size, 512, w//32, h//32)\n        x = self.proj(x)\n        batch_size = x.shape[0]\n        hidden = x.shape[1]\n        w = x.shape[2]\n        h = x.shape[3]\n        x = x + self.pos_embed[:, :, :w, :h] * self.pos_embed_weight\n        # [batch size, hidden_dim, width, height]\n        impt_pixels = self.impt_pts(x)\n        # [batch size, 2, width, height]\n        x = x.reshape(batch_size, hidden, -1)\n        seq_len = x.shape[2]\n        # [batch size, hidden_dim, width * height]\n        # Treat it like [batch size, hidden_dim, sequence len] WILL PERMUTE LATER\n\n        box_Q = self.query_weights[3] * self.box_query(x).permute(0, 2, 1)\n        # [batch size, 100, hidden_dim]\n        box_Q = box_Q + self.query_weights[4] * self.box_embed\n        # [batch size, 100, hidden_dim]\n        # x1 = model.backbone.layer1[1](x1)\n        x1 = rearrange(x1, 'b c w h -> b c (w h)')\n        box_Q = box_Q + self.query_weights[0] * self.query_1(x1).permute(0, 2, 1)\n        # x2 = model.backbone.layer2[1](x2)\n        x2 = rearrange(x2, 'b c w h -> b c (w h)')\n        box_Q = box_Q + self.query_weights[1] * self.query_2(x2).permute(0, 2, 1)\n        # x3 = model.backbone.layer3[1](x3)\n        x3 = rearrange(x3, 'b c w h -> b c (w h)')\n        box_Q = box_Q + self.query_weights[2] * self.query_3(x3).permute(0, 2, 1)\n        # [batch size, 100, hidden_dim]\n        x = x.permute(0, 2, 1)\n        # [batch size, width * height, hidden_dim] # w * h = seq len\n        x = self.transformer(x, box_Q) # encoder input and decoder input\n        # [batch size, 100, hidden_dim] comes from box_Q aka decoder input length\n\n        class_pred = self.fc_class(x)\n        # [batch size, 100, num classes + 1]\n        box_pred = self.fc_bbox(x)\n        # [batch size, 100, 4]\n        bbox_size = self.bbox_sizes(x)\n        # [batch size, 100, 2]\n        return class_pred, box_pred, impt_pixels, bbox_size",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.424444Z",
     "iopub.execute_input": "2025-05-28T01:38:55.424639Z",
     "iopub.status.idle": "2025-05-28T01:38:55.442519Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.424624Z",
     "shell.execute_reply": "2025-05-28T01:38:55.441842Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Loss Computation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class_loss_weights = torch.ones(20 + 1).to(device)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.443267Z",
     "iopub.execute_input": "2025-05-28T01:38:55.443541Z",
     "iopub.status.idle": "2025-05-28T01:38:55.656695Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.443519Z",
     "shell.execute_reply": "2025-05-28T01:38:55.655875Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from tqdm import tqdm",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.657626Z",
     "iopub.execute_input": "2025-05-28T01:38:55.657999Z",
     "iopub.status.idle": "2025-05-28T01:38:55.661240Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.657971Z",
     "shell.execute_reply": "2025-05-28T01:38:55.660661Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# cnt = 0\n# for _, target in tqdm(train_loader):\n#     cnt += 1\n#     if cnt >= 100:\n#         break\n#     for dictionary in target:\n#         class_loss_weights[dictionary['labels']] += 1",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.662042Z",
     "iopub.execute_input": "2025-05-28T01:38:55.662674Z",
     "iopub.status.idle": "2025-05-28T01:38:55.676780Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.662650Z",
     "shell.execute_reply": "2025-05-28T01:38:55.676108Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class_loss_weights[-1] = torch.sum(class_loss_weights) / 20 ",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.677459Z",
     "iopub.execute_input": "2025-05-28T01:38:55.677743Z",
     "iopub.status.idle": "2025-05-28T01:38:55.762568Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.677727Z",
     "shell.execute_reply": "2025-05-28T01:38:55.762029Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class_loss_weights = 1 / class_loss_weights\nclass_loss_weights[-1] = class_loss_weights[-1] / 20\nclass_loss_weights = class_loss_weights / torch.sum(class_loss_weights)\nclass_loss_weights",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:55.763336Z",
     "iopub.execute_input": "2025-05-28T01:38:55.763612Z",
     "iopub.status.idle": "2025-05-28T01:38:56.023116Z",
     "shell.execute_reply.started": "2025-05-28T01:38:55.763589Z",
     "shell.execute_reply": "2025-05-28T01:38:56.022479Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from scipy.optimize import linear_sum_assignment\n# just import package to do bipartite matching\nfrom torchvision.ops import generalized_box_iou, box_iou, generalized_box_iou_loss, box_convert\n\nclass_criteria = nn.CrossEntropyLoss(reduction=\"none\", weight=None, label_smoothing=0.05)\nno_labels_target_id = 20\n\npixel_criteria = nn.CrossEntropyLoss()\n\nbbox_size_criteria = nn.SmoothL1Loss()\n\ndef match_bbox_class(\n    class_pred_in, \n    box_pred_in, \n    pred_pixels, \n    bbox_size_in,\n    target_classes_in, \n    target_bbox_in, \n    target_pixels, \n    weights=[2, 5, 0]\n):\n    # class_pred: [batch size, 100, num_classes + 1]\n    # box_pred: [batch size, 100, 4]\n    # target_classes: not batched\n    # target_bbox: not batched\n    batch_size, num_bb, num_class = class_pred_in.shape\n    # num_class already contain the +1\n\n    # Stores the total loss\n    final_loss = 0\n    # Stores total IoU\n    total_iou = 0\n\n    # Stores each type of loss\n    total_class_loss = 0\n    total_bbox_loss = 0\n    total_no_label_loss = 0\n    total_mask_loss = 0\n    total_size_loss = 0\n\n    # For-loop over batch size dimension because input can't really be batched efficiently\n    for i in range(batch_size):\n        class_pred = class_pred_in[i]\n        box_pred = box_pred_in[i]\n        bbox_size = bbox_size_in[i]\n        target_classes = target_classes_in[i]\n        target_bbox = target_bbox_in[i]\n        target_len = len(target_classes)\n        \n        # CLASS LOSS\n        class_pred = class_pred.unsqueeze(1).expand(-1, target_len, -1)\n        # class_pred: [100, target_len, num_classes + 1]\n        target_classes = target_classes.unsqueeze(0).expand(num_bb, -1) \n        # note that this is transposed relative to above to ensure the correct targets lines up\n        # and we actually get proper [i, j] pairs\n        # might be more useful to imagine the first 100 as N, and second 100 as M\n        # target_classes: [100, target_len]\n        \n        class_loss = class_criteria(class_pred.reshape(-1, num_class), target_classes.reshape(-1))\n        class_loss = class_loss.reshape(num_bb, target_len)\n        # class_loss: [100, target_len]\n    \n        # BBOX L1 LOSS\n        bbox_loss = 1 * torch.cdist(box_pred, target_bbox, p=1.0) \n        # bbox_loss = bbox_loss + 10 * torch.square(torch.cdist(box_pred, target_bbox, p=2.0))\n        # bbox_loss: [100, target_len]\n    \n        # BBOX G-IOU LOSS\n        giou_loss = 1 - generalized_box_iou(\n            box_pred, \n            target_bbox\n        )\n        # giou_loss: [100, target_len]\n        # assert torch.min(giou_loss) >= -20.0\n    \n        # Total Cost\n        total_cost = (weights[0] * class_loss + weights[1] * bbox_loss + weights[2] * giou_loss)\n        # Positive because linear_sum_assignment minimizers total weight by default\n        # Compute the bipartite matching problem and final loss\n        row_idx, col_idx = linear_sum_assignment(total_cost.detach().cpu().numpy())\n        total_cost = (weights[0] * class_loss + weights[1] * bbox_loss + weights[2] * giou_loss) / (weights[0] + weights[1] + weights[2])\n        # giou_loss = generalized_box_iou_loss(\n        #     box_pred[row_idx], \n        #     target_bbox[col_idx],\n        #     reduction=\"mean\",\n        # )\n        final_loss += (torch.mean(total_cost[row_idx, col_idx]))\n        total_class_loss += torch.mean(weights[0] * class_loss[row_idx, col_idx]) / (weights[0] + weights[1])\n        total_bbox_loss += torch.mean(weights[1] * bbox_loss[row_idx, col_idx]) / (weights[0] + weights[1])\n\n        # Force the non-matched classes to be \"no labels\" class\n        mask = torch.ones(num_bb)\n        mask[row_idx] = mask[row_idx] - 1\n        mask = mask > 0.5 # Select the indicies not matched\n        if torch.sum(mask) > 0:\n            bad_class = class_pred[mask, :, :].reshape(-1, num_class)\n            # reshape to allow loss computation\n            no_label_target = torch.ones(bad_class.shape[0], device=device, dtype=torch.long) * no_labels_target_id\n            # construct the target that forces everything to \"no label\" class\n            final_loss += class_criteria(bad_class, no_label_target).mean()\n            total_no_label_loss += class_criteria(bad_class, no_label_target).mean()\n\n        # # Pixel Loss (similar loss to Segmentation)\n        # target_mask = target_pixels[i]\n        # # [320, 320]\n        # target_mask = target_mask[::32, ::32]\n        # # print(target_mask.shape)\n        # # [10, 10]\n        # pred_mask = pred_pixels[i]\n        # # print(pred_mask.shape)\n        # # [10, 10, 2]\n        # mask_loss = pixel_criteria(pred_mask.reshape(-1, 2), target_mask.reshape(-1))\n        # final_loss += 0.02 * mask_loss\n        # total_mask_loss += 0.02 * mask_loss\n\n        # BBox sizes loss (hopefully help with smaller objects)\n        target_bbox_size = target_bbox[:, [2, 3]] - target_bbox[:, [0, 1]]\n        size_loss = bbox_size_criteria(bbox_size[row_idx].reshape(-1), target_bbox_size[col_idx].reshape(-1)) * 10\n        final_loss += size_loss\n        total_size_loss += size_loss\n        \n        # Compute IoU\n        iou_matrix = box_iou(box_pred, target_bbox)\n        total_iou += torch.mean(iou_matrix[row_idx, col_idx])\n\n        # In addition, to encourage matching, we also add all those with IoU > 0.6 or IoU < 0.3\n        iou_mask = (iou_matrix > 0.6) | (iou_matrix < 0.3)\n        final_loss += torch.mean(iou_matrix[iou_mask]) * 0.5\n    exact_losses = torch.tensor([total_class_loss, total_bbox_loss, total_no_label_loss, total_mask_loss, total_size_loss]) / batch_size\n    return final_loss, total_iou / batch_size, exact_losses",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:38:56.023896Z",
     "iopub.execute_input": "2025-05-28T01:38:56.024149Z",
     "iopub.status.idle": "2025-05-28T01:38:56.591384Z",
     "shell.execute_reply.started": "2025-05-28T01:38:56.024125Z",
     "shell.execute_reply": "2025-05-28T01:38:56.590789Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Model, Optimizers, Schedulers",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model = SimpleDETR(\n    32,\n    32,\n    2,\n    (1, 1),\n    20\n).to(device)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:50.019343Z",
     "iopub.execute_input": "2025-05-28T01:43:50.020059Z",
     "iopub.status.idle": "2025-05-28T01:43:50.300982Z",
     "shell.execute_reply.started": "2025-05-28T01:43:50.020031Z",
     "shell.execute_reply": "2025-05-28T01:43:50.300357Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "sum(p.numel() for p in model.parameters())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:50.301996Z",
     "iopub.execute_input": "2025-05-28T01:43:50.302261Z",
     "iopub.status.idle": "2025-05-28T01:43:50.307718Z",
     "shell.execute_reply.started": "2025-05-28T01:43:50.302234Z",
     "shell.execute_reply": "2025-05-28T01:43:50.307152Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# model",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:50.359786Z",
     "iopub.execute_input": "2025-05-28T01:43:50.359996Z",
     "iopub.status.idle": "2025-05-28T01:43:50.363171Z",
     "shell.execute_reply.started": "2025-05-28T01:43:50.359980Z",
     "shell.execute_reply": "2025-05-28T01:43:50.362466Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# optimiser = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\noptimiser = optim.AdamW(\n    [\n        {'params': model.backbone.parameters(), 'lr': 1e-5},\n        {'params': model.proj.parameters()},\n        {'params': model.pos_embed, 'lr': 1e-5},\n        {'params': model.pos_embed_weight},\n        {'params': model.transformer.parameters()},\n        {'params': model.query_1.parameters()},\n        {'params': model.query_2.parameters()},\n        {'params': model.query_3.parameters()},\n        {'params': model.box_query.parameters()},\n        {'params': model.box_embed},\n        {'params': model.query_weights},\n        {'params': model.fc_class.parameters()},\n        {'params': model.fc_bbox.parameters()},\n        {'params': model.impt_pts.parameters()},\n        {'params': model.bbox_sizes.parameters()},\n    ], lr=2e-4, weight_decay=1e-4\n)\n# optimiser = optim.LBFGS(model.parameters(), lr=1e-0, line_search_fn=\"strong_wolfe\", history_size=10)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimiser, T_max=10, eta_min=1e-4)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:51.340717Z",
     "iopub.execute_input": "2025-05-28T01:43:51.341436Z",
     "iopub.status.idle": "2025-05-28T01:43:51.349794Z",
     "shell.execute_reply.started": "2025-05-28T01:43:51.341387Z",
     "shell.execute_reply": "2025-05-28T01:43:51.349177Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Training and Evaluation Loops",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from tqdm import tqdm",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:51.695912Z",
     "iopub.execute_input": "2025-05-28T01:43:51.696175Z",
     "iopub.status.idle": "2025-05-28T01:43:51.699770Z",
     "shell.execute_reply.started": "2025-05-28T01:43:51.696145Z",
     "shell.execute_reply": "2025-05-28T01:43:51.699001Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from torchmetrics.detection.mean_ap import MeanAveragePrecision",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:52.394327Z",
     "iopub.execute_input": "2025-05-28T01:43:52.395028Z",
     "iopub.status.idle": "2025-05-28T01:43:52.398294Z",
     "shell.execute_reply.started": "2025-05-28T01:43:52.395006Z",
     "shell.execute_reply": "2025-05-28T01:43:52.397604Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# for batch in (pbar := tqdm(train_loader)):\n",
    "#     image, targets = batch\n",
    "#     # image = image.to(device)        \n",
    "#     all_labels = []\n",
    "#     all_boxes  = []\n",
    "#     all_masks = []\n",
    "#     for t in targets:\n",
    "#         t[\"boxes\"] = t[\"boxes\"].resources\n",
    "#         t[\"boxes\"] = torch.cat([t[\"boxes\"], t[\"boxes\"] * (1 + torch.randn(1) / 100)], dim=0)\n",
    "#         t[\"boxes\"] = torch.cat([t[\"boxes\"], t[\"boxes\"] * (1 + torch.randn(1) / 100)], dim=0)\n",
    "#         t[\"boxes\"] = torch.cat([t[\"boxes\"], t[\"boxes\"] * (1 + torch.randn(1) / 100)], dim=0)\n",
    "#         t[\"boxes\"] = torch.cat([t[\"boxes\"], t[\"boxes\"] * (1 + torch.randn(1) / 100)], dim=0)\n",
    "#         print(t[\"boxes\"])\n",
    "#         1/0"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:52.587555Z",
     "iopub.execute_input": "2025-05-28T01:43:52.587736Z",
     "iopub.status.idle": "2025-05-28T01:43:52.591075Z",
     "shell.execute_reply.started": "2025-05-28T01:43:52.587723Z",
     "shell.execute_reply": "2025-05-28T01:43:52.590540Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def train(weights=[2, 5, 1]):\n    model.train()\n    total_loss = 0.0\n    cnt = 0\n    # Stores each type of loss\n    exact_loss = torch.tensor([0, 0, 0, 0, 0]).to(torch.float)\n    for batch in (pbar := tqdm(train_loader)):\n        image, targets = batch\n        image = image.to(device)        \n        all_labels = []\n        all_boxes  = []\n        all_masks = []\n        for t in targets:\n            t[\"boxes\"] = t[\"boxes\"].data\n            # t[\"boxes\"] = torch.cat([t[\"boxes\"], t[\"boxes\"] * (1 + torch.randn(1) / 100)], dim=0)\n            # t[\"boxes\"] = torch.cat([t[\"boxes\"], t[\"boxes\"] * (1 + torch.randn(1) / 100)], dim=0)\n            # t[\"boxes\"] = torch.cat([t[\"boxes\"], t[\"boxes\"] * (1 + torch.randn(1) / 100)], dim=0)\n            # t[\"labels\"] = torch.cat([t[\"labels\"], t[\"labels\"]])\n            # t[\"labels\"] = torch.cat([t[\"labels\"], t[\"labels\"]])\n            # t[\"labels\"] = torch.cat([t[\"labels\"], t[\"labels\"]])\n            all_labels.append(t[\"labels\"].to(device))\n            all_boxes.append(t[\"boxes\"].to(device))\n            all_masks.append(t[\"mask\"].to(torch.long).to(device))\n        \n        class_pred, box_pred, mask_pred, size_pred = model(image)\n        \n        loss, iou, exact_losses = match_bbox_class(class_pred, box_pred, mask_pred, size_pred, all_labels, all_boxes, all_masks, weights)\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n        total_loss += loss.item()\n        exact_loss += exact_losses\n        cnt += 1\n        pbar.set_description(f\"Average Loss: {total_loss / cnt:6f} | Average IoU: {iou:6f}\")\n\n    exact_loss = exact_loss / cnt\n    loss_type_dict = {\n        'total_class_loss': exact_loss[0],\n        'total_bbox_loss': exact_loss[1],\n        'total_no_label_loss': exact_loss[2],\n        'total_mask_loss': exact_loss[3],\n        'total_size_loss': exact_loss[4],\n    }\n    print(loss_type_dict)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:52.754113Z",
     "iopub.execute_input": "2025-05-28T01:43:52.754285Z",
     "iopub.status.idle": "2025-05-28T01:43:52.761059Z",
     "shell.execute_reply.started": "2025-05-28T01:43:52.754272Z",
     "shell.execute_reply": "2025-05-28T01:43:52.760321Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torchvision.ops as ops\n\ndef train_map():\n    model.eval()\n    total_loss = 0.0\n    cnt = 0\n    # 1) Instantiate once, before training\n    map_metric = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"bbox\").to(device)\n    with torch.no_grad():\n        for batch in (pbar := tqdm(train_loader)):\n            image, targets = batch\n            image = image.to(device)\n            all_labels = []\n            all_boxes  = []\n            all_masks = []\n            for t in targets:\n                all_labels.append(t[\"labels\"].to(device))\n                all_boxes.append(t[\"boxes\"].to(device))\n                all_masks.append(t[\"mask\"].to(torch.long).to(device))\n            \n            class_pred, box_pred, mask_pred, size_pred = model(image)\n\n            # AP Metrics Computation\n            for i in range(image.shape[0]):\n                has_labels = (torch.argmax(class_pred[i], dim=-1) != 20)\n                image_size = (box_pred[i][:, 2] - box_pred[i][:, 0]) * (box_pred[i][:, 3] - box_pred[i][:, 1])\n                image_size = image_size * 320 * 320\n                size_mask = image_size > 4\n                final_mask = torch.logical_and(has_labels, size_mask)\n                boxes = box_pred[i][final_mask] * 320\n                scores = torch.max(nn.Softmax(dim=-1)(class_pred[i]), dim=-1)[0][final_mask]\n                labels = torch.argmax(class_pred[i], dim=-1)[final_mask]\n                idx = ops.nms(boxes, scores, 0.5)\n                preds = [{\n                    \"boxes\": boxes[idx],\n                    \"scores\": scores[idx],\n                    \"labels\": labels[idx],\n                }]\n                target = [{\n                    \"boxes\": all_boxes[i] * 320,\n                    \"labels\": all_labels[i],\n                }]\n                map_metric.update(preds, target)\n    print(\"Train Set Results:\")\n    print(map_metric.compute())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:52.945442Z",
     "iopub.execute_input": "2025-05-28T01:43:52.945640Z",
     "iopub.status.idle": "2025-05-28T01:43:52.953302Z",
     "shell.execute_reply.started": "2025-05-28T01:43:52.945626Z",
     "shell.execute_reply": "2025-05-28T01:43:52.952729Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def test(weights=[2, 5, 1]):\n    model.eval()\n    total_loss = 0.0\n    cnt = 0\n    # Stores each type of loss\n    exact_loss = torch.tensor([0, 0, 0, 0, 0]).to(torch.float)\n    with torch.no_grad():\n        for batch in (pbar := tqdm(test_loader)):\n            image, targets = batch\n            image = image.to(device)\n            all_labels = []\n            all_boxes  = []\n            all_masks = []\n            for t in targets:\n                all_labels.append(t[\"labels\"].to(device))\n                all_boxes.append(t[\"boxes\"].to(device))\n                all_masks.append(t[\"mask\"].to(torch.long).to(device))\n            \n            class_pred, box_pred, mask_pred, size_pred = model(image)\n            \n            loss, iou, exact_losses = match_bbox_class(class_pred, box_pred, mask_pred, size_pred, all_labels, all_boxes, all_masks, weights)    \n            total_loss += loss.item()\n            exact_loss += exact_losses\n            cnt += 1\n            pbar.set_description(f\"Testing: Average Loss: {total_loss / cnt:6f} | Average IoU: {iou:6f}\")\n    exact_loss = exact_loss / cnt\n    loss_type_dict = {\n        'total_class_loss': exact_loss[0],\n        'total_bbox_loss': exact_loss[1],\n        'total_no_label_loss': exact_loss[2],\n        'total_mask_loss': exact_loss[3],\n        'total_size_loss': exact_loss[4],\n    }\n    print(loss_type_dict)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:53.114959Z",
     "iopub.execute_input": "2025-05-28T01:43:53.115647Z",
     "iopub.status.idle": "2025-05-28T01:43:53.122039Z",
     "shell.execute_reply.started": "2025-05-28T01:43:53.115623Z",
     "shell.execute_reply": "2025-05-28T01:43:53.121336Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def test_map():\n    model.eval()\n    total_loss = 0.0\n    cnt = 0\n    # 1) Instantiate once, before training\n    map_metric = MeanAveragePrecision(box_format=\"xyxy\", iou_type=\"bbox\").to(device)\n    with torch.no_grad():\n        for batch in (pbar := tqdm(test_loader)):\n            image, targets = batch\n            image = image.to(device)\n            all_labels = []\n            all_boxes  = []\n            all_masks = []\n            for t in targets:\n                all_labels.append(t[\"labels\"].to(device))\n                all_boxes.append(t[\"boxes\"].to(device))\n                all_masks.append(t[\"mask\"].to(torch.long).to(device))\n            \n            class_pred, box_pred, mask_pred, size_pred = model(image)\n\n            # AP Metrics Computation\n            for i in range(image.shape[0]):\n                has_labels = (torch.argmax(class_pred[i], dim=-1) != 20)\n                image_size = (box_pred[i][:, 2] - box_pred[i][:, 0]) * (box_pred[i][:, 3] - box_pred[i][:, 1])\n                image_size = image_size * 320 * 320\n                size_mask = image_size > 4\n                final_mask = torch.logical_and(has_labels, size_mask)\n                boxes = box_pred[i][final_mask] * 320\n                scores = torch.max(nn.Softmax(dim=-1)(class_pred[i]), dim=-1)[0][final_mask]\n                labels = torch.argmax(class_pred[i], dim=-1)[final_mask]\n                idx = ops.nms(boxes, scores, 0.5)\n                preds = [{\n                    \"boxes\": boxes[idx],\n                    \"scores\": scores[idx],\n                    \"labels\": labels[idx],\n                }]\n                target = [{\n                    \"boxes\": all_boxes[i] * 320,\n                    \"labels\": all_labels[i],\n                }]\n                map_metric.update(preds, target)\n    # print(map_metric.compute())\n    results = map_metric.compute()\n    print(results)\n    return results",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:53.302596Z",
     "iopub.execute_input": "2025-05-28T01:43:53.303248Z",
     "iopub.status.idle": "2025-05-28T01:43:53.310882Z",
     "shell.execute_reply.started": "2025-05-28T01:43:53.303228Z",
     "shell.execute_reply": "2025-05-28T01:43:53.310195Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "all_results = []\nfor i in range(10):\n    train([2, 6, 0])\n    test([2, 6, 0])\n    results = test_map()\n    all_results.append(results)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:43:53.468226Z",
     "iopub.execute_input": "2025-05-28T01:43:53.468445Z",
     "execution_failed": "2025-05-28T02:19:14.027Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_map()",
   "metadata": {
    "trusted": true,
    "execution": {
     "execution_failed": "2025-05-28T02:19:14.028Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# test_map()",
   "metadata": {
    "trusted": true,
    "execution": {
     "execution_failed": "2025-05-28T02:19:14.029Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# import gc\n# gc.collect()\n# torch.cuda.empty_cache()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.250111Z",
     "iopub.status.idle": "2025-05-28T01:41:37.250325Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.250224Z",
     "shell.execute_reply": "2025-05-28T01:41:37.250234Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# cProfile.run(\"train()\", sort='tottime')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.250955Z",
     "iopub.status.idle": "2025-05-28T01:41:37.251174Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.251058Z",
     "shell.execute_reply": "2025-05-28T01:41:37.251066Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Visualise",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.253217Z",
     "iopub.status.idle": "2025-05-28T01:41:37.253947Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.253770Z",
     "shell.execute_reply": "2025-05-28T01:41:37.253786Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "with torch.no_grad():\n    for batch in (test_loader): # train_loader\n        image, targets = batch\n        image = image.to(device)\n        all_labels = []\n        all_boxes  = []\n        all_masks = []\n        for t in targets:\n            all_labels.append(t[\"labels\"].to(device))\n            all_boxes.append(t[\"boxes\"].to(device))\n            all_masks.append(t[\"mask\"].to(torch.long).to(device))\n        \n        class_pred, box_pred, _, _ = model(image)\n        break",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.255079Z",
     "iopub.status.idle": "2025-05-28T01:41:37.255666Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.255539Z",
     "shell.execute_reply": "2025-05-28T01:41:37.255552Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "all_labels[0]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.256405Z",
     "iopub.status.idle": "2025-05-28T01:41:37.256687Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.256565Z",
     "shell.execute_reply": "2025-05-28T01:41:37.256575Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# class_pred[0].shape",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.257915Z",
     "iopub.status.idle": "2025-05-28T01:41:37.258250Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.258040Z",
     "shell.execute_reply": "2025-05-28T01:41:37.258056Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class_pred[0].argmax(-1)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.259917Z",
     "iopub.status.idle": "2025-05-28T01:41:37.260626Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.260431Z",
     "shell.execute_reply": "2025-05-28T01:41:37.260449Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "box_pred[0].shape",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.261381Z",
     "iopub.status.idle": "2025-05-28T01:41:37.261640Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.261538Z",
     "shell.execute_reply": "2025-05-28T01:41:37.261547Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "box_pred[:, [0,2]] *= 320\nbox_pred[:, [1,3]] *= 320",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.262932Z",
     "iopub.status.idle": "2025-05-28T01:41:37.263264Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.263083Z",
     "shell.execute_reply": "2025-05-28T01:41:37.263097Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "torch.round(box_pred[0].cpu()).numpy()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.264271Z",
     "iopub.status.idle": "2025-05-28T01:41:37.264584Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.264397Z",
     "shell.execute_reply": "2025-05-28T01:41:37.264430Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "box_pred.shape",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.265946Z",
     "iopub.status.idle": "2025-05-28T01:41:37.266166Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.266070Z",
     "shell.execute_reply": "2025-05-28T01:41:37.266079Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "targets[0]['boxes'].cpu().numpy()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.266887Z",
     "iopub.status.idle": "2025-05-28T01:41:37.267190Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.267032Z",
     "shell.execute_reply": "2025-05-28T01:41:37.267046Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "idx2labels = {0: 'aeroplane', 1: 'bicycle', 2: 'bird', 3: 'boat', 4: 'bottle', 5: 'bus', 6: 'car', 7: 'cat', 8: 'chair', 9: 'cow', 10: 'diningtable', 11: 'dog', 12: 'horse', 13: 'motorbike', 14: 'person', 15: 'pottedplant', 16: 'sheep', 17: 'sofa', 18: 'train', 19: 'tvmonitor', 20: 'background'}",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.268345Z",
     "iopub.status.idle": "2025-05-28T01:41:37.268596Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.268485Z",
     "shell.execute_reply": "2025-05-28T01:41:37.268498Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "plt.figure(figsize=(12, 8))\nplt.imshow(image[0].cpu().permute(1, 2, 0))\nax = plt.gca()\n\nimport matplotlib.patches as patches\n\n# Example: keep only top‑K or score‑filtered indices\n# for i in keep_inds:\ncnt = 0\nfor xmin, ymin, xmax, ymax in box_pred[0].cpu().numpy():\n    width  = xmax - xmin\n    height = ymax - ymin\n    rect = patches.Rectangle(\n        (xmin, ymin), width, height,\n        linewidth=2, edgecolor='red', facecolor='none'\n    )\n    ax.add_patch(rect)\n    ax.text(xmin, ymin - 5, f\"{idx2labels[int(class_pred[0].argmax(-1)[cnt].detach().cpu().numpy())]}\",\n        bbox=dict(facecolor='red', alpha=0.5, pad=0),\n        color='white', fontsize=8)\n    cnt += 1\n\ncnt = 0\nfor xmin, ymin, xmax, ymax in targets[0]['boxes'].cpu().numpy():\n    xmin = xmin * 320\n    ymin = ymin * 320\n    xmax = xmax * 320\n    ymax = ymax * 320\n    width  = xmax - xmin\n    height = ymax - ymin\n    rect = patches.Rectangle(\n        (xmin, ymin), width, height,\n        linewidth=2, edgecolor='blue', facecolor='none'\n    )\n    ax.add_patch(rect)\n    ax.text(xmin, ymin - 5, f\"{idx2labels[int(all_labels[0][cnt].detach().cpu().numpy())]}\",\n        bbox=dict(facecolor='blue', alpha=0.5, pad=0),\n        color='white', fontsize=8)\n    cnt += 1\n\nplt.axis('off')   # optional: hide axes\nplt.show()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-28T01:41:37.269865Z",
     "iopub.status.idle": "2025-05-28T01:41:37.270119Z",
     "shell.execute_reply.started": "2025-05-28T01:41:37.269983Z",
     "shell.execute_reply": "2025-05-28T01:41:37.269998Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
