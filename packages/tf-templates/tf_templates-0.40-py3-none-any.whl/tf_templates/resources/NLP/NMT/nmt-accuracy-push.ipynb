{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":29837,"sourceType":"datasetVersion","datasetId":23320}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install muon_optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:01.793385Z","iopub.execute_input":"2025-06-10T15:00:01.794018Z","iopub.status.idle":"2025-06-10T15:00:06.257989Z","shell.execute_reply.started":"2025-06-10T15:00:01.793994Z","shell.execute_reply":"2025-06-10T15:00:06.257273Z"}},"outputs":[{"name":"stdout","text":"Collecting muon_optimizer\n  Downloading muon_optimizer-0.1.0-py3-none-any.whl.metadata (5.1 kB)\nDownloading muon_optimizer-0.1.0-py3-none-any.whl (7.1 kB)\nInstalling collected packages: muon_optimizer\nSuccessfully installed muon_optimizer-0.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:06.259430Z","iopub.execute_input":"2025-06-10T15:00:06.259695Z","iopub.status.idle":"2025-06-10T15:00:09.633151Z","shell.execute_reply.started":"2025-06-10T15:00:06.259673Z","shell.execute_reply":"2025-06-10T15:00:09.632421Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import random\nf = open(\"/kaggle/input/frenchenglish-bilingual-pairs/fra.txt\", \"r\")\nr = f.read().splitlines()\nf.close()\nrandom.shuffle(r)\nels = []\nfrs = []\nfor y in r:\n    el, fr = y.split(\"\\t\")\n    els.append(el)\n    frs.append(fr)\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers.processors import TemplateProcessing\nimport os\ndef maketokenizer(sents, of):\n    tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n    tokenizer.decoder = decoders.WordPiece()\n    trainer = trainers.WordPieceTrainer(\n        vocab_size=2048,\n        special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n    )\n    tokenizer.train_from_iterator(sents, trainer=trainer)\n    if os.path.exists(of):\n        os.remove(of)\n    tokenizer.save(of)\n    tokenizer = PreTrainedTokenizerFast(tokenizer_file=of,\n                                        unk_token=\"[UNK]\",\n                                        pad_token=\"[PAD]\",\n                                        bos_token=\"[BOS]\",\n                                        eos_token=\"[EOS]\", padding_side=\"left\")\n    tokenizer._tokenizer.post_processor = TemplateProcessing(\n        single=\"[BOS] $A [EOS]\",\n        pair=\"[BOS] $A $B [EOS]\",\n        special_tokens=[(\"[BOS]\", tokenizer.bos_token_id), (\"[EOS]\", tokenizer.eos_token_id)]\n    )\n    return tokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:09.634008Z","iopub.execute_input":"2025-06-10T15:00:09.634231Z","iopub.status.idle":"2025-06-10T15:00:15.121418Z","shell.execute_reply.started":"2025-06-10T15:00:09.634210Z","shell.execute_reply":"2025-06-10T15:00:15.120808Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"el_tokenizer = maketokenizer(els, \"el_tokenizer.json\")\nfr_tokenizer = maketokenizer(frs, \"fr_tokenizer.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:15.123408Z","iopub.execute_input":"2025-06-10T15:00:15.123783Z","iopub.status.idle":"2025-06-10T15:00:16.787850Z","shell.execute_reply.started":"2025-06-10T15:00:15.123764Z","shell.execute_reply":"2025-06-10T15:00:16.787223Z"}},"outputs":[{"name":"stdout","text":"\n\n\n\n\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:16.788620Z","iopub.execute_input":"2025-06-10T15:00:16.788861Z","iopub.status.idle":"2025-06-10T15:00:16.792768Z","shell.execute_reply.started":"2025-06-10T15:00:16.788834Z","shell.execute_reply":"2025-06-10T15:00:16.791993Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"el_t = el_tokenizer(els, return_tensors=\"pt\", max_length = 32, padding=\"max_length\", truncation=True)[\"input_ids\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:16.793453Z","iopub.execute_input":"2025-06-10T15:00:16.793669Z","iopub.status.idle":"2025-06-10T15:00:25.110881Z","shell.execute_reply.started":"2025-06-10T15:00:16.793644Z","shell.execute_reply":"2025-06-10T15:00:25.110042Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"fr_t = fr_tokenizer(frs, return_tensors=\"pt\", max_length = 33, padding=\"max_length\", truncation=True)[\"input_ids\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:25.111969Z","iopub.execute_input":"2025-06-10T15:00:25.112275Z","iopub.status.idle":"2025-06-10T15:00:33.661737Z","shell.execute_reply.started":"2025-06-10T15:00:25.112251Z","shell.execute_reply":"2025-06-10T15:00:33.661033Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"el_t.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:33.662586Z","iopub.execute_input":"2025-06-10T15:00:33.662819Z","iopub.status.idle":"2025-06-10T15:00:33.668483Z","shell.execute_reply.started":"2025-06-10T15:00:33.662801Z","shell.execute_reply":"2025-06-10T15:00:33.667773Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"torch.Size([145437, 32])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"fr_t.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:33.669385Z","iopub.execute_input":"2025-06-10T15:00:33.669855Z","iopub.status.idle":"2025-06-10T15:00:33.686013Z","shell.execute_reply.started":"2025-06-10T15:00:33.669822Z","shell.execute_reply":"2025-06-10T15:00:33.685200Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([145437, 33])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nel_train, el_test, fr_train, fr_test = train_test_split(el_t, fr_t, test_size=0.025)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:33.688842Z","iopub.execute_input":"2025-06-10T15:00:33.689036Z","iopub.status.idle":"2025-06-10T15:00:34.856169Z","shell.execute_reply.started":"2025-06-10T15:00:33.689021Z","shell.execute_reply":"2025-06-10T15:00:34.855445Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"el_train.shape, el_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:34.856967Z","iopub.execute_input":"2025-06-10T15:00:34.857378Z","iopub.status.idle":"2025-06-10T15:00:34.863414Z","shell.execute_reply.started":"2025-06-10T15:00:34.857357Z","shell.execute_reply":"2025-06-10T15:00:34.862726Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(torch.Size([141801, 32]), torch.Size([3636, 32]))"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:34.864076Z","iopub.execute_input":"2025-06-10T15:00:34.864282Z","iopub.status.idle":"2025-06-10T15:00:34.876340Z","shell.execute_reply.started":"2025-06-10T15:00:34.864263Z","shell.execute_reply":"2025-06-10T15:00:34.875718Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:34.877022Z","iopub.execute_input":"2025-06-10T15:00:34.877246Z","iopub.status.idle":"2025-06-10T15:00:34.945066Z","shell.execute_reply.started":"2025-06-10T15:00:34.877223Z","shell.execute_reply":"2025-06-10T15:00:34.944415Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:34.945826Z","iopub.execute_input":"2025-06-10T15:00:34.946130Z","iopub.status.idle":"2025-06-10T15:00:34.956804Z","shell.execute_reply.started":"2025-06-10T15:00:34.946103Z","shell.execute_reply":"2025-06-10T15:00:34.956099Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class TranslationDataset(Dataset):\n    def __init__(self, el, fr):\n        super().__init__()\n        self.el = np.array(el)\n        self.fr = np.array(fr)\n    def __len__(self):\n        return self.el.shape[0]\n    def __getitem__(self, idx):\n        return self.el[idx], self.fr[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:34.957579Z","iopub.execute_input":"2025-06-10T15:00:34.957819Z","iopub.status.idle":"2025-06-10T15:00:34.968724Z","shell.execute_reply.started":"2025-06-10T15:00:34.957795Z","shell.execute_reply":"2025-06-10T15:00:34.968072Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_ds = TranslationDataset(el_train, fr_train)\ntest_ds = TranslationDataset(el_test, fr_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:34.969295Z","iopub.execute_input":"2025-06-10T15:00:34.969454Z","iopub.status.idle":"2025-06-10T15:00:35.010684Z","shell.execute_reply.started":"2025-06-10T15:00:34.969442Z","shell.execute_reply":"2025-06-10T15:00:35.009938Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def collate_fn(batch):\n    el = []\n    fr = []\n    for el_t, fr_t in batch:\n        el.append(el_t)\n        fr.append(fr_t)\n    el = np.array(el)\n    fr = np.array(fr)\n    return torch.from_numpy(el), torch.from_numpy(fr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.011658Z","iopub.execute_input":"2025-06-10T15:00:35.011890Z","iopub.status.idle":"2025-06-10T15:00:35.016254Z","shell.execute_reply.started":"2025-06-10T15:00:35.011873Z","shell.execute_reply":"2025-06-10T15:00:35.015563Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_ds,\n    batch_size=512,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=4,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.017023Z","iopub.execute_input":"2025-06-10T15:00:35.017283Z","iopub.status.idle":"2025-06-10T15:00:35.030008Z","shell.execute_reply.started":"2025-06-10T15:00:35.017267Z","shell.execute_reply":"2025-06-10T15:00:35.029366Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"test_loader = DataLoader(\n    test_ds,\n    batch_size=1024,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=4,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.030814Z","iopub.execute_input":"2025-06-10T15:00:35.031085Z","iopub.status.idle":"2025-06-10T15:00:35.044826Z","shell.execute_reply.started":"2025-06-10T15:00:35.031063Z","shell.execute_reply":"2025-06-10T15:00:35.044130Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"%%time\nfor el, fr in train_loader:\n    print(el.shape)\n    print(fr.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.045578Z","iopub.execute_input":"2025-06-10T15:00:35.045828Z","iopub.status.idle":"2025-06-10T15:00:35.219314Z","shell.execute_reply.started":"2025-06-10T15:00:35.045809Z","shell.execute_reply":"2025-06-10T15:00:35.218477Z"}},"outputs":[{"name":"stdout","text":"torch.Size([512, 32])\ntorch.Size([512, 33])\nCPU times: user 35.3 ms, sys: 82.6 ms, total: 118 ms\nWall time: 159 ms\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def relu2(x):\n    x = F.relu(x)\n    # x = F.gelu(x)\n    x = torch.square(x)\n    return x\n\nclass ReLU2(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        x = F.relu(x)\n        x = torch.square(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.220257Z","iopub.execute_input":"2025-06-10T15:00:35.220517Z","iopub.status.idle":"2025-06-10T15:00:35.225679Z","shell.execute_reply.started":"2025-06-10T15:00:35.220483Z","shell.execute_reply":"2025-06-10T15:00:35.224967Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.0):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n\n        self.norm1 = nn.RMSNorm(d_model)\n        self.norm2 = nn.RMSNorm(d_model)\n\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            ReLU2(),\n            nn.Dropout(dropout),\n            nn.Linear(dim_feedforward, d_model),\n        )\n\n        self.alpha = nn.Parameter(torch.tensor([1.0, 1.0]) * 1.0)\n\n    def forward(self, x, mask):\n        x1 = self.norm1(x)\n        # x1 = x\n        x1, _ = self.self_attn(x1, x1, x1, attn_mask=mask)\n        x = x + x1 * self.alpha[0]\n        x1 = self.norm2(x)\n        x1 = self.ffn(x1)\n        return x + x1 * self.alpha[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.226477Z","iopub.execute_input":"2025-06-10T15:00:35.226719Z","iopub.status.idle":"2025-06-10T15:00:35.238600Z","shell.execute_reply.started":"2025-06-10T15:00:35.226699Z","shell.execute_reply":"2025-06-10T15:00:35.237875Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.0):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.norm1 = nn.RMSNorm(d_model)\n        self.norm2 = nn.RMSNorm(d_model)\n        self.norm3 = nn.RMSNorm(d_model)\n\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            ReLU2(),\n            nn.Dropout(dropout),\n            nn.Linear(dim_feedforward, d_model),\n        )\n\n        self.alpha = nn.Parameter(torch.tensor([1.0, 1.0, 1.0]) * 1.0)\n\n    def forward(self, x, memory, tgt_mask):\n        x1 = self.norm1(x)\n        x1, _ = self.self_attn(x1, x1, x1, attn_mask=tgt_mask)\n        x1 = self.dropout1(x1)\n        x = x + x1 * self.alpha[0]\n        x1 = self.norm2(x)\n        x1, _ = self.cross_attn(x1, memory, memory, attn_mask=None) # No need to mask encoder memory in NMT\n        x1 = self.dropout2(x1)\n        x = x + x1 * self.alpha[1]\n        x = self.norm3(x)\n        x = x + self.ffn(x) * self.alpha[2]\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.239454Z","iopub.execute_input":"2025-06-10T15:00:35.239681Z","iopub.status.idle":"2025-06-10T15:00:35.254348Z","shell.execute_reply.started":"2025-06-10T15:00:35.239663Z","shell.execute_reply":"2025-06-10T15:00:35.253729Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import torch.nn as nn\nimport torch, math\nfrom einops import rearrange\nimport torch.nn.functional as F\n\nclass DynamicPositionBias(nn.Module):\n    def __init__(self, dim, heads, depth):\n        super().__init__()\n\n        self.mlp = nn.ModuleList([])\n        self.mlp.append(nn.Sequential(\n            nn.Linear(1, dim),\n            nn.SiLU()\n        ))\n        for _ in range(depth - 1):\n            self.mlp.append(nn.Sequential(\n                nn.Linear(dim, dim),\n                nn.SiLU()\n            ))\n        self.mlp.append(nn.Linear(dim, heads))\n\n    def forward(self, n, device):\n        indices = (n-1) + torch.arange(n).unsqueeze(1) - torch.arange(n).unsqueeze(0)\n        pos = torch.arange(-n + 1, n, device = device).float().unsqueeze(-1)\n        for layer in self.mlp:\n            pos = layer(pos)\n        bias = pos[indices]\n        bias = rearrange(bias, 'i j h -> h i j')\n        return bias\n\n# self.pe = DynamicPositionBias(embed_dim // 4, n_heads, 3)\n# ...\n# logits = F.scaled_dot_product_attention(\n#     q, k, v,\n#     attn_mask=F.pad(self.pe(sl, x.device), (0, 1)),\n#     is_causal=False,\n#     dropout_p=0.0\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.255107Z","iopub.execute_input":"2025-06-10T15:00:35.255355Z","iopub.status.idle":"2025-06-10T15:00:35.291118Z","shell.execute_reply.started":"2025-06-10T15:00:35.255326Z","shell.execute_reply":"2025-06-10T15:00:35.290541Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class EncoderDecoderwAlibi(nn.Module):\n    def __init__(self, hidden_dim, ffn_dim, in_vocab_size, out_vocab_size, seq_len, num_layers):\n        super().__init__()\n        self.src_embedding = nn.Embedding(in_vocab_size, hidden_dim)\n        self.trg_embedding = nn.Embedding(out_vocab_size, hidden_dim)\n        # encoder_layer = nn.TransformerEncoderLayer(\n        #     d_model=hidden_dim,\n        #     nhead=8,\n        #     dim_feedforward=ffn_dim,\n        #     norm_first=True,\n        #     batch_first=True,\n        #     activation=relu2,\n        #     # activation=\"gelu\",\n        # )\n        # self.encoder = nn.TransformerEncoder(\n        #     encoder_layer,\n        #     num_layers=num_layers[0]\n        # )\n        # decoder_layer = nn.TransformerDecoderLayer(\n        #     d_model=hidden_dim,\n        #     nhead=8,\n        #     dim_feedforward=ffn_dim,\n        #     norm_first=True,\n        #     batch_first=True,\n        #     activation=relu2,\n        #     # activation=\"gelu\",\n        # )\n        # self.decoder = nn.TransformerDecoder(\n        #     decoder_layer,\n        #     num_layers=num_layers[1],\n        # )\n\n        self.encoder = nn.ModuleList([EncoderLayer(hidden_dim, 8, ffn_dim) for _ in range(num_layers[0])])\n        self.decoder = nn.ModuleList([DecoderLayer(hidden_dim, 8, ffn_dim) for _ in range(num_layers[1])])\n        \n        self.alibi_m = [1 / (2**i) for i in range(1, 9)]\n        x = torch.arange(seq_len)\n        y = torch.arange(seq_len).unsqueeze(-1)\n        self.alibi_val = x - y\n        self.alibi_val = self.alibi_val.to(device).unsqueeze(0)\n        self.alibi_val.requires_grad = False\n        \n        self.pe = DynamicPositionBias(hidden_dim // 4, 8, 3)\n        # self.pe.require_grad = False\n        \n        self.causal_mask = torch.ones(1, seq_len, seq_len, requires_grad=False, device=device) * (float('-inf'))\n        self.causal_mask = torch.triu(self.causal_mask, diagonal=1)\n        \n        self.output = nn.Linear(hidden_dim, out_vocab_size)\n\n    def forward(self, src, trg):\n        batch_size = src.shape[0]\n        x = self.src_embedding(src)\n        # x = F.rms_norm(x, (x.size(-1),))\n\n        # MASK COMPUTATION\n        # alibi_mask = (self.alibi_val * self.alibi_m[0]).expand(batch_size, -1, -1)\n        # for i in range(1, 8):\n        #     alibi_mask = torch.cat([alibi_mask, (self.alibi_val * self.alibi_m[i]).expand(batch_size, -1, -1)])\n        # alibi_mask = torch.tril(alibi_mask)\n        alibi_mask = self.pe(x.shape[1], device=device)\n        alibi_mask = alibi_mask.repeat(batch_size, 1, 1)\n        # END MASK COMPUTATION\n\n        # x = self.encoder(x, mask=alibi_mask)\n        for layer in self.encoder:\n            x = layer(x, mask=alibi_mask)\n        # x = F.rms_norm(x, (x.size(-1),))\n\n        # DECODER MASK\n        mask = self.causal_mask.expand(batch_size * 8, -1, -1)\n        mask = mask + alibi_mask\n        # END OF DECODER MASK\n        \n        trg = self.trg_embedding(trg)\n        trg_len = trg.shape[1]\n        mask = mask[:, :trg_len, :trg_len]\n        # x = self.decoder(\n        #     tgt=trg, \n        #     memory=x,\n        #     tgt_mask=mask\n        # )\n        # return self.output(x)\n        for layer in self.decoder:\n            trg = layer(trg, x, mask)\n        return self.output(trg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.291917Z","iopub.execute_input":"2025-06-10T15:00:35.292129Z","iopub.status.idle":"2025-06-10T15:00:35.301569Z","shell.execute_reply.started":"2025-06-10T15:00:35.292113Z","shell.execute_reply":"2025-06-10T15:00:35.300892Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"model = EncoderDecoderwAlibi(\n    hidden_dim=32,\n    ffn_dim=16,\n    in_vocab_size=2048, \n    out_vocab_size=2048, \n    seq_len=32, \n    num_layers=(1, 1),\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.302326Z","iopub.execute_input":"2025-06-10T15:00:35.302588Z","iopub.status.idle":"2025-06-10T15:00:35.553145Z","shell.execute_reply.started":"2025-06-10T15:00:35.302571Z","shell.execute_reply":"2025-06-10T15:00:35.552588Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"%%time\nfor el, fr in train_loader:\n    print(el.shape)\n    print(fr.shape)\n    output = model(el.to(device), fr[:, :-1].to(device))\n    print(output.shape)\n    # print(output[0])\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:00:35.553914Z","iopub.execute_input":"2025-06-10T15:00:35.554111Z","iopub.status.idle":"2025-06-10T15:00:41.107884Z","shell.execute_reply.started":"2025-06-10T15:00:35.554096Z","shell.execute_reply":"2025-06-10T15:00:41.107189Z"}},"outputs":[{"name":"stdout","text":"torch.Size([512, 32])\ntorch.Size([512, 33])\ntorch.Size([512, 32, 2048])\nCPU times: user 3.64 s, sys: 880 ms, total: 4.52 s\nWall time: 5.55 s\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Training and Testing Loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train():\n    model.train()\n    total_loss = 0\n    cnt = 0\n    prev_avg = 999999.9\n    for batch in (pbar := tqdm(train_loader)):\n        el, fr = batch\n        el, fr = el.to(device), fr.to(device)\n        fr_in = fr[:, :-1]\n        fr_actual = fr[:, 1:]\n        pred = model(el, fr_in)\n        loss = criteria(pred.reshape(-1, 2048), fr_actual.reshape(-1))\n        total_loss += loss.item()\n        cnt += 1\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        pbar.set_description(f\"Average Loss: {total_loss / cnt :6f}\")\n        if cnt % 20 == 0:\n            # if total_loss / cnt < prev_avg:\n            #     prev_avg = total_loss / cnt\n            # else:\n            #     if total_loss / cnt > prev_avg * 1.02:\n            #         scheduler.step()\n            #         print(\"Stepping Scheduler\")\n            cnt = 0\n            total_loss = 0\n    # if scheduler is not None:\n    #     scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:10:25.668274Z","iopub.execute_input":"2025-06-10T15:10:25.668873Z","iopub.status.idle":"2025-06-10T15:10:25.674281Z","shell.execute_reply.started":"2025-06-10T15:10:25.668844Z","shell.execute_reply":"2025-06-10T15:10:25.673556Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"def test():\n    model.eval()\n    total_loss = 0\n    cnt = 0\n    num_correct = 0\n    total_cnt = 0\n    with torch.no_grad():\n        for batch in (pbar := tqdm(test_loader)):\n            el, fr = batch\n            el, fr = el.to(device), fr.to(device)\n            fr_in = fr[:, :-1]\n            fr_actual = fr[:, 1:]\n            pred = model(el, fr_in)\n            loss = criteria(pred.reshape(-1, 2048), fr_actual.reshape(-1))\n            total_loss += loss.item()\n            cnt += 1\n            # Obtain the token prediction\n            pred_argmax = pred.argmax(-1)\n            pred_argmax = pred_argmax.reshape(-1)\n            fr_actual = fr_actual.reshape(-1)\n            # Mask out pad token to not count towards final accuracy\n            not_pad_mask = (fr_actual != 0)\n            pred_argmax = pred_argmax[not_pad_mask]\n            fr_actual = fr_actual[not_pad_mask]\n            # Compute accuracy\n            num_correct += torch.sum(fr_actual == pred_argmax).cpu().numpy()\n            total_cnt += fr_actual.shape[0]\n            pbar.set_description(f\"Testing Loss: {total_loss / cnt :6f} | Testing Accuracy: {num_correct / total_cnt : 6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:10:25.802834Z","iopub.execute_input":"2025-06-10T15:10:25.803122Z","iopub.status.idle":"2025-06-10T15:10:25.809770Z","shell.execute_reply.started":"2025-06-10T15:10:25.803101Z","shell.execute_reply":"2025-06-10T15:10:25.809134Z"}},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":"# Model Definition and Training","metadata":{}},{"cell_type":"code","source":"class MultipleOptimizer(object):\n    def __init__(self, *op):\n        self.optimizers = op\n\n    def zero_grad(self):\n        for op in self.optimizers:\n            op.zero_grad()\n\n    def step(self):\n        for op in self.optimizers:\n            op.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:14:22.496929Z","iopub.execute_input":"2025-06-10T15:14:22.497233Z","iopub.status.idle":"2025-06-10T15:14:22.501982Z","shell.execute_reply.started":"2025-06-10T15:14:22.497213Z","shell.execute_reply":"2025-06-10T15:14:22.501211Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"class MultipleScheduler(object):\n    def __init__(self, *op):\n        self.schedulers = op\n\n    def step(self):\n        for op in self.schedulers:\n            op.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:14:22.684618Z","iopub.execute_input":"2025-06-10T15:14:22.685227Z","iopub.status.idle":"2025-06-10T15:14:22.689259Z","shell.execute_reply.started":"2025-06-10T15:14:22.685203Z","shell.execute_reply":"2025-06-10T15:14:22.688510Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"model = EncoderDecoderwAlibi(\n    hidden_dim=128,\n    ffn_dim=280,\n    in_vocab_size=2048, \n    out_vocab_size=2048,\n    seq_len=32, \n    num_layers=(3, 5),\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:39.197814Z","iopub.execute_input":"2025-06-10T15:23:39.198447Z","iopub.status.idle":"2025-06-10T15:23:39.234474Z","shell.execute_reply.started":"2025-06-10T15:23:39.198422Z","shell.execute_reply":"2025-06-10T15:23:39.233904Z"}},"outputs":[],"execution_count":151},{"cell_type":"code","source":"sum(p.numel() for p in model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:39.355474Z","iopub.execute_input":"2025-06-10T15:23:39.355980Z","iopub.status.idle":"2025-06-10T15:23:39.361951Z","shell.execute_reply.started":"2025-06-10T15:23:39.355950Z","shell.execute_reply":"2025-06-10T15:23:39.361350Z"}},"outputs":[{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"2228957"},"metadata":{}}],"execution_count":152},{"cell_type":"code","source":"from torch.optim import *\nfrom torch.optim.lr_scheduler import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:40.392387Z","iopub.execute_input":"2025-06-10T15:23:40.392980Z","iopub.status.idle":"2025-06-10T15:23:40.396609Z","shell.execute_reply.started":"2025-06-10T15:23:40.392945Z","shell.execute_reply":"2025-06-10T15:23:40.395899Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"from muon import SingleDeviceMuon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:40.594514Z","iopub.execute_input":"2025-06-10T15:23:40.595077Z","iopub.status.idle":"2025-06-10T15:23:40.598419Z","shell.execute_reply.started":"2025-06-10T15:23:40.595056Z","shell.execute_reply":"2025-06-10T15:23:40.597668Z"}},"outputs":[],"execution_count":154},{"cell_type":"code","source":"hidden_weights = [p for p in model.parameters() if p.ndim >= 2]\nhidden_gains_biases = [p for p in model.parameters() if p.ndim < 2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:40.709118Z","iopub.execute_input":"2025-06-10T15:23:40.709803Z","iopub.status.idle":"2025-06-10T15:23:40.714075Z","shell.execute_reply.started":"2025-06-10T15:23:40.709779Z","shell.execute_reply":"2025-06-10T15:23:40.713459Z"}},"outputs":[],"execution_count":155},{"cell_type":"code","source":"param_groups = [\n    dict(params=hidden_weights, use_muon=True,\n         lr=1.5e-2, weight_decay=0, momentum=0.95, nesterov=True),\n]\nmuon_optimizer = SingleDeviceMuon(param_groups)\nmuon_scheduler = ExponentialLR(muon_optimizer, gamma=0.6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:41.388335Z","iopub.execute_input":"2025-06-10T15:23:41.388587Z","iopub.status.idle":"2025-06-10T15:23:41.393052Z","shell.execute_reply.started":"2025-06-10T15:23:41.388571Z","shell.execute_reply":"2025-06-10T15:23:41.392415Z"}},"outputs":[],"execution_count":156},{"cell_type":"code","source":"param_groups = [\n    dict(params=hidden_gains_biases, lr=5e-3, weight_decay=0),\n]\nadamw_optimizer = AdamW(param_groups)\nadamw_scheduler = ExponentialLR(adamw_optimizer, gamma=0.8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:41.560289Z","iopub.execute_input":"2025-06-10T15:23:41.560771Z","iopub.status.idle":"2025-06-10T15:23:41.565430Z","shell.execute_reply.started":"2025-06-10T15:23:41.560751Z","shell.execute_reply":"2025-06-10T15:23:41.564579Z"}},"outputs":[],"execution_count":157},{"cell_type":"code","source":"criteria = nn.CrossEntropyLoss(ignore_index=0)\n# class_weights = torch.ones(2048).to(device)\n# class_weights[0] = 0.0\n# class_weights[fr_tokenizer.eos_token_id] = 100.0\n# criteria = nn.CrossEntropyLoss(weight = class_weights)\noptimizer = MultipleOptimizer(muon_optimizer, adamw_optimizer)\nscheduler = MultipleScheduler(muon_scheduler, adamw_scheduler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:41.735550Z","iopub.execute_input":"2025-06-10T15:23:41.736278Z","iopub.status.idle":"2025-06-10T15:23:41.741913Z","shell.execute_reply.started":"2025-06-10T15:23:41.736251Z","shell.execute_reply":"2025-06-10T15:23:41.741208Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"for _ in range(3):\n    train()\n    test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:23:41.912663Z","iopub.execute_input":"2025-06-10T15:23:41.913378Z","iopub.status.idle":"2025-06-10T15:25:54.262244Z","shell.execute_reply.started":"2025-06-10T15:23:41.913355Z","shell.execute_reply":"2025-06-10T15:25:54.261354Z"}},"outputs":[{"name":"stderr","text":"Average Loss: 1.401731: 100%|██████████| 277/277 [00:43<00:00,  6.34it/s]\nTesting Loss: 1.377325 | Testing Accuracy:  0.697414: 100%|██████████| 4/4 [00:00<00:00,  9.80it/s]\nAverage Loss: 1.077258: 100%|██████████| 277/277 [00:43<00:00,  6.34it/s]\nTesting Loss: 1.069161 | Testing Accuracy:  0.755701: 100%|██████████| 4/4 [00:00<00:00,  9.98it/s]\nAverage Loss: 0.930102: 100%|██████████| 277/277 [00:43<00:00,  6.33it/s]\nTesting Loss: 0.942886 | Testing Accuracy:  0.779280: 100%|██████████| 4/4 [00:00<00:00,  9.88it/s]\n","output_type":"stream"}],"execution_count":159},{"cell_type":"code","source":"model.encoder[0].alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:25:56.195143Z","iopub.execute_input":"2025-06-10T15:25:56.196093Z","iopub.status.idle":"2025-06-10T15:25:56.203129Z","shell.execute_reply.started":"2025-06-10T15:25:56.196062Z","shell.execute_reply":"2025-06-10T15:25:56.202391Z"}},"outputs":[{"execution_count":160,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([0.0914, 0.1404], device='cuda:0', requires_grad=True)"},"metadata":{}}],"execution_count":160},{"cell_type":"code","source":"model.encoder[1].alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:25:56.352944Z","iopub.execute_input":"2025-06-10T15:25:56.353469Z","iopub.status.idle":"2025-06-10T15:25:56.359730Z","shell.execute_reply.started":"2025-06-10T15:25:56.353446Z","shell.execute_reply":"2025-06-10T15:25:56.358917Z"}},"outputs":[{"execution_count":161,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([0.1138, 0.1034], device='cuda:0', requires_grad=True)"},"metadata":{}}],"execution_count":161},{"cell_type":"code","source":"model.encoder[2].alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:25:56.489669Z","iopub.execute_input":"2025-06-10T15:25:56.490380Z","iopub.status.idle":"2025-06-10T15:25:56.495889Z","shell.execute_reply.started":"2025-06-10T15:25:56.490355Z","shell.execute_reply":"2025-06-10T15:25:56.495226Z"}},"outputs":[{"execution_count":162,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([0.3295, 0.1077], device='cuda:0', requires_grad=True)"},"metadata":{}}],"execution_count":162},{"cell_type":"code","source":"model.decoder[0].alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:25:57.270214Z","iopub.execute_input":"2025-06-10T15:25:57.270970Z","iopub.status.idle":"2025-06-10T15:25:57.276879Z","shell.execute_reply.started":"2025-06-10T15:25:57.270944Z","shell.execute_reply":"2025-06-10T15:25:57.276235Z"}},"outputs":[{"execution_count":163,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([0.1384, 0.0530, 0.0574], device='cuda:0', requires_grad=True)"},"metadata":{}}],"execution_count":163},{"cell_type":"code","source":"model.decoder[1].alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:25:57.483410Z","iopub.execute_input":"2025-06-10T15:25:57.484288Z","iopub.status.idle":"2025-06-10T15:25:57.490093Z","shell.execute_reply.started":"2025-06-10T15:25:57.484262Z","shell.execute_reply":"2025-06-10T15:25:57.489361Z"}},"outputs":[{"execution_count":164,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([0.1571, 0.0633, 0.0631], device='cuda:0', requires_grad=True)"},"metadata":{}}],"execution_count":164},{"cell_type":"code","source":"model.decoder[2].alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:25:57.630712Z","iopub.execute_input":"2025-06-10T15:25:57.631202Z","iopub.status.idle":"2025-06-10T15:25:57.637527Z","shell.execute_reply.started":"2025-06-10T15:25:57.631180Z","shell.execute_reply":"2025-06-10T15:25:57.636851Z"}},"outputs":[{"execution_count":165,"output_type":"execute_result","data":{"text/plain":"Parameter containing:\ntensor([0.2079, 0.0954, 0.0671], device='cuda:0', requires_grad=True)"},"metadata":{}}],"execution_count":165},{"cell_type":"markdown","source":"# Sanity Check","metadata":{}},{"cell_type":"code","source":"def translate(input_str):\n    model.eval()\n    en_ids = el_tokenizer(input_str, return_tensors=\"pt\", max_length = 32, padding=\"max_length\", truncation=True)[\"input_ids\"]\n    en_ids = en_ids.to(device)\n    # print(en_ids)\n    fr_ids = [2] # The first token\n    with torch.no_grad():\n        while len(fr_ids) <= 32:\n            # print(torch.tensor(fr_ids).to(torch.long).to(device).unsqueeze(0))\n            pred = model(en_ids, torch.tensor(fr_ids).to(torch.long).to(device).unsqueeze(0))\n            pred = pred[0].argmax(-1)\n            fr_ids.append(int(pred[-1].cpu().numpy()))\n            # print(fr_ids)\n    return fr_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:25:59.188842Z","iopub.execute_input":"2025-06-10T15:25:59.189491Z","iopub.status.idle":"2025-06-10T15:25:59.194912Z","shell.execute_reply.started":"2025-06-10T15:25:59.189467Z","shell.execute_reply":"2025-06-10T15:25:59.194205Z"}},"outputs":[],"execution_count":166},{"cell_type":"code","source":"output = translate(\"I will not kill you and I hate you.\")\nlen(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:25:59.333931Z","iopub.execute_input":"2025-06-10T15:25:59.334567Z","iopub.status.idle":"2025-06-10T15:25:59.648836Z","shell.execute_reply.started":"2025-06-10T15:25:59.334543Z","shell.execute_reply":"2025-06-10T15:25:59.648029Z"}},"outputs":[{"execution_count":167,"output_type":"execute_result","data":{"text/plain":"33"},"metadata":{}}],"execution_count":167},{"cell_type":"code","source":"fr_tokenizer.decode(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:26:00.044320Z","iopub.execute_input":"2025-06-10T15:26:00.044606Z","iopub.status.idle":"2025-06-10T15:26:00.050018Z","shell.execute_reply.started":"2025-06-10T15:26:00.044584Z","shell.execute_reply":"2025-06-10T15:26:00.049358Z"}},"outputs":[{"execution_count":168,"output_type":"execute_result","data":{"text/plain":"\"[BOS] Je ne te tuerais pas et je ne te tuerais et je t ' hais et je t ' hais d ' une telle part\""},"metadata":{}}],"execution_count":168},{"cell_type":"markdown","source":"# Bleu Score","metadata":{}},{"cell_type":"code","source":"def translate_for_bleu(en_str, max_len=32):\n    model.eval()\n    en_ids = el_tokenizer(en_str, return_tensors=\"pt\", max_length = 32, padding=\"max_length\", truncation=True)[\"input_ids\"]\n    en_ids = en_ids.to(device)\n    # print(en_ids)\n    fr_ids = [2] # The first token\n    with torch.no_grad():\n        while len(fr_ids) <= max_len:\n            # print(torch.tensor(fr_ids).to(torch.long).to(device).unsqueeze(0))\n            pred = model(en_ids, torch.tensor(fr_ids).to(torch.long).to(device).unsqueeze(0))\n            pred = pred[0].argmax(-1)\n            fr_ids.append(int(pred[-1].cpu().numpy()))\n            # print(fr_ids)\n            if fr_ids[-1] == fr_tokenizer.eos_token_id:\n                break\n    return fr_ids[1:-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:26:00.376804Z","iopub.execute_input":"2025-06-10T15:26:00.377552Z","iopub.status.idle":"2025-06-10T15:26:00.382859Z","shell.execute_reply.started":"2025-06-10T15:26:00.377530Z","shell.execute_reply":"2025-06-10T15:26:00.382178Z"}},"outputs":[],"execution_count":169},{"cell_type":"code","source":"import sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:26:00.509565Z","iopub.execute_input":"2025-06-10T15:26:00.510384Z","iopub.status.idle":"2025-06-10T15:26:00.514041Z","shell.execute_reply.started":"2025-06-10T15:26:00.510360Z","shell.execute_reply":"2025-06-10T15:26:00.513201Z"}},"outputs":[],"execution_count":170},{"cell_type":"code","source":"hypotheses = []\nreferences = []\nmodel.eval()\nbleu_loader = DataLoader(\n    train_ds,\n    batch_size=1024,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=4,\n)\nwith torch.no_grad():\n    batch = next(iter(bleu_loader))\n    src_batch, fr_batch = batch\n    fr_in_batch, fr_t_batch = fr_batch[:, :-1], fr_batch[:, 1:]\n    for ref_ids in fr_t_batch:\n        ref_str = fr_tokenizer.decode(ref_ids,\n                                      skip_special_tokens=True,\n                                      clean_up_tokenization_spaces=True)\n        references.append(ref_str)\n    \n    for src_ids in tqdm(src_batch):\n        src_str = el_tokenizer.decode(src_ids,\n                                      skip_special_tokens=True,\n                                      clean_up_tokenization_spaces=True)\n        hyp_str = translate_for_bleu(src_str, len(src_ids))\n        pred_string = fr_tokenizer.decode(hyp_str)\n        hypotheses.append(pred_string)\n\nmodel.train()\nbleu = sacrebleu.corpus_bleu(hypotheses, [references])\n\nprint(f\"Corpus BLEU = {bleu.score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:26:00.651757Z","iopub.execute_input":"2025-06-10T15:26:00.652325Z","execution_failed":"2025-06-10T15:26:41.337Z"}},"outputs":[{"name":"stderr","text":" 13%|█▎        | 133/1024 [00:40<04:28,  3.32it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"bleu","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-10T15:26:41.338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}