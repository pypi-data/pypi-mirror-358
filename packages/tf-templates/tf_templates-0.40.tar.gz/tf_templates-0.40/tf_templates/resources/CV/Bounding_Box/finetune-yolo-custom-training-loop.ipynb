{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23902,"sourceType":"datasetVersion","datasetId":18276}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:55:44.619949Z","iopub.execute_input":"2025-06-26T14:55:44.620657Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.159-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.159-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport os\n\nfrom torchvision.io import read_image\nfrom torchvision.ops.boxes import masks_to_boxes\nfrom torchvision import tv_tensors\nfrom torchvision.transforms import v2\nfrom torchvision.transforms.v2 import functional as vF\nfrom torchvision.ops import box_convert\nfrom einops import rearrange\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n# train_dataset = load_dataset(import os\nimport torch\nimport torchvision.transforms.v2\nfrom torch.utils.data.dataset import Dataset\nimport xml.etree.ElementTree as ET\nfrom torchvision import tv_tensors\nfrom torchvision.io import read_image\n\n\ndef load_images_and_anns(im_sets, label2idx, ann_fname, split):\n    r\"\"\"\n    Method to get the xml files and for each file\n    get all the objects and their ground truth detection\n    information for the dataset\n    :param im_sets: Sets of images to consider\n    :param label2idx: Class Name to index mapping for dataset\n    :param ann_fname: txt file containing image names{trainval.txt/test.txt}\n    :param split: train/test\n    :return:\n    \"\"\"\n    im_infos = []\n    for im_set in im_sets:\n        im_names = []\n        # Fetch all image names in txt file for this imageset\n        for line in open(os.path.join(\n                im_set, 'ImageSets', 'Main', '{}.txt'.format(ann_fname))):\n            im_names.append(line.strip())\n\n        # Set annotation and image path\n        ann_dir = os.path.join(im_set, 'Annotations')\n        im_dir = os.path.join(im_set, 'JPEGImages')\n\n        for im_name in im_names:\n            ann_file = os.path.join(ann_dir, '{}.xml'.format(im_name))\n            im_info = {}\n            ann_info = ET.parse(ann_file)\n            root = ann_info.getroot()\n            size = root.find('size')\n            width = int(size.find('width').text)\n            height = int(size.find('height').text)\n            im_info['img_id'] = os.path.basename(ann_file).split('.xml')[0]\n            im_info['filename'] = os.path.join(\n                im_dir, '{}.jpg'.format(im_info['img_id'])\n            )\n            im_info['width'] = width\n            im_info['height'] = height\n            detections = []\n            for obj in ann_info.findall('object'):\n                det = {}\n                label = label2idx[obj.find('name').text]\n                difficult = int(obj.find('difficult').text)\n                bbox_info = obj.find('bndbox')\n                bbox = [\n                    int(bbox_info.find('xmin').text) - 1,\n                    int(bbox_info.find('ymin').text) - 1,\n                    int(bbox_info.find('xmax').text) - 1,\n                    int(bbox_info.find('ymax').text) - 1\n                ]\n                det['label'] = label\n                det['bbox'] = bbox\n                det['difficult'] = difficult\n                detections.append(det)\n            im_info['detections'] = detections\n            # Because we are using 25 as num_queries,\n            # so we ignore all images in VOC with greater\n            # than 25 target objects.\n            # This is okay, since this just means we are\n            # ignoring a small number of images(15 to be precise)\n            if len(detections) <= 25:\n                im_infos.append(im_info)\n    print('Total {} images found'.format(len(im_infos)))\n    return im_infos\n\n\nclass VOCDataset(Dataset):\n    def __init__(self, split, im_sets, im_size=640):\n        self.split = split\n\n        # Imagesets for this dataset instance (VOC2007/VOC2007+VOC2012/VOC2007-test)\n        self.im_sets = im_sets\n        self.fname = 'trainval' if self.split == 'train' else 'test'\n        self.im_size = im_size\n        self.im_mean = [123.0, 117.0, 104.0]\n        self.imagenet_mean = [0.485, 0.456, 0.406]\n        self.imagenet_std = [0.229, 0.224, 0.225]\n\n        # Train and test transformations\n        self.transforms = {\n            'train': torchvision.transforms.v2.Compose([\n                torchvision.transforms.v2.RandomHorizontalFlip(p=0.5),\n                torchvision.transforms.v2.RandomZoomOut(fill=self.im_mean),\n                torchvision.transforms.v2.RandomIoUCrop(),\n                torchvision.transforms.v2.RandomPhotometricDistort(),\n                torchvision.transforms.v2.Resize(size=(self.im_size, self.im_size)),\n                torchvision.transforms.v2.SanitizeBoundingBoxes(\n                    labels_getter=lambda transform_input:\n                    (transform_input[1][\"labels\"], transform_input[1][\"difficult\"])),\n                torchvision.transforms.v2.ToPureTensor(),\n                torchvision.transforms.v2.ToDtype(torch.float32, scale=True),\n                # torchvision.transforms.v2.Normalize(mean=self.imagenet_mean, std=self.imagenet_std)\n\n            ]),\n            'test': torchvision.transforms.v2.Compose([\n                torchvision.transforms.v2.Resize(size=(self.im_size, self.im_size)),\n                torchvision.transforms.v2.ToPureTensor(),\n                torchvision.transforms.v2.ToDtype(torch.float32, scale=True),\n                # torchvision.transforms.v2.Normalize(mean=self.imagenet_mean, std=self.imagenet_std)\n            ]),\n        }\n\n        classes = [\n            'person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep',\n            'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train',\n            'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor'\n        ]\n        classes = sorted(classes)\n        # We need to add background class as well with 0 index\n        classes = ['background'] + classes\n\n        self.label2idx = {classes[idx]: idx for idx in range(len(classes))}\n        self.idx2label = {idx: classes[idx] for idx in range(len(classes))}\n        print(self.idx2label)\n        self.images_info = load_images_and_anns(self.im_sets,\n                                                self.label2idx,\n                                                self.fname,\n                                                self.split)\n\n    def __len__(self):\n        return len(self.images_info)\n\n    def __getitem__(self, index):\n        im_info = self.images_info[index]\n        im = read_image(im_info['filename'])\n        # print(im)\n\n        # Get annotations for this image\n        targets = {}\n        targets['boxes'] = tv_tensors.BoundingBoxes(\n            [detection['bbox'] for detection in im_info['detections']],\n            format='XYXY', canvas_size=im.shape[-2:])\n        targets['labels'] = torch.as_tensor(\n            [detection['label'] for detection in im_info['detections']])\n        targets['difficult'] = torch.as_tensor(\n            [detection['difficult']for detection in im_info['detections']])\n\n        # Transform the image and targets\n        transformed_info = self.transforms[\"test\"](im, targets)\n        im_tensor, targets = transformed_info\n        # print(im_tensor)\n\n        h, w = im_tensor.shape[-2:]\n\n        # Boxes returned are in x1y1x2y2 format normalized from 0-1\n        wh_tensor = torch.as_tensor([[w, h, w, h]]).expand_as(targets['boxes'])\n        # return im_tensor, targets, im_info['filename']\n        return_dict = {\n            'img': im_tensor,\n            'bboxes': targets['boxes'] / 320,\n            'cls': targets['labels'],\n            # 'segments':,\n            # 'keypoints':,\n            # 'mask':,\n            'batch_idx': torch.zeros(targets['labels'].shape[0], dtype=torch.int64),\n            'bbox_format': 'xyxy',\n            'normalized': True,\n        }\n        return return_dict\n        # {\n        #   \"img\":        Tensor[C,H,W],           # the image tensor\n        #   \"bboxes\":     Tensor[N,4],             # N boxes as float32, in whatever format (XYXY or XYWH)\n        #   \"cls\":        Tensor[N,1] or Tensor[N],# N class indices\n        #   \"segments\":   List[np.ndarray] or [],  # empty list if no segments\n        #   \"keypoints\":  List[...] or None,       # None if no keypoints\n        #   \"mask\":       Tensor[...] (optional),  # if doing segmentation masks\n        #   \"batch_idx\":  Tensor[N],               # zeros (or the image’s batch index) per box\n        #   \"bbox_format\": str,                    # e.g. \"xywh\" or \"xyxy\"\n        #   \"normalized\": bool                     # True if box coords are in [0,1]\n        # }\n\n# test_dataset = load_dataset(\"detection-datasets/coco\", split='val', streaming=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transforms = v2.Compose([\n    # v2.RGB(),\n    # v2.ToImage(),  # Convert to tensor, only needed if you had a PIL image\n    # v2.PILToTensor(),\n    # v2.ToDtype(torch.uint8, scale=True),  # optional, most input are already uint8 at this point\n    # ...\n    # v2.Resize(size=(320, 320), antialias=True),  # Or Resize(antialias=True)\n    # ...\n    v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n    # v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import tv_tensors\ndef preprocess_ds(batch):\n    img = batch[0]\n    w = img.shape[1]\n    h = img.shape[2]\n    obj = batch[1]\n    boxes_xyxy = obj['boxes'].to(torch.float32)\n    output = {}\n    output['new_image'] = img\n    output['boxes_xyxy'] = tv_tensors.BoundingBoxes(\n        boxes_xyxy,\n        format=\"XYXY\", canvas_size=(1, 1)\n    )\n    output['labels'] = obj[\"labels\"]\n    obj_mask = torch.zeros(320, 320)\n    for obx in boxes_xyxy:\n        x1, y1, x2, y2 = tuple(torch.round(obx * 320).to(torch.int32))\n        # print(x1, y1, x2, y2)\n        obj_mask[x1:x2, y1:y2] = 1\n    output['mask'] = obj_mask\n    return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ndef collate_function(data):\n    return tuple(zip(*data))\n\n\nvoc = VOCDataset('train',\n                 im_sets=[\"/kaggle/input/pascal-voc-2007/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007\"],\n                 im_size=320)\n# train_ds = []\n# for i in tqdm(range(len(voc))):\n#     train_ds.append(preprocess_ds(voc[i]))\nvoc2 = VOCDataset('test',\n                 im_sets=[\"/kaggle/input/pascal-voc-2007/VOCtest_06-Nov-2007/VOCdevkit/VOC2007\"],\n                 im_size=320)\n# test_ds = []\n# for i in tqdm(range(len(voc2))):\n#     test_ds.append(preprocess_ds(voc2[i]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# voc[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_augs = v2.Compose([\n#     v2.RandomRotation(30),\n#     v2.RandomHorizontalFlip(p=0.5),\n#     v2.ClampBoundingBoxes(),\n# ])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import random\n\n# def collate_fn(in_batch):\n#     images = []\n#     targets = []\n#     for batch in in_batch:\n#         img = batch['new_image']\n#         lbl = batch['labels']\n#         bbox = batch['boxes_xyxy']\n#         mask = batch['mask']\n#         img, bbox = train_augs(img, bbox)\n#         # images.append(img)\n#         temp = {\n#             \"img\": img.unsqueeze(0), # 4D Batched input\n#             \"labels\": lbl,\n#             \"boxes\": bbox,\n#             # \"mask\": mask\n#         }\n#         targets.append(temp)\n#     return targets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics.data.dataset import YOLOConcatDataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(\n    voc,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=YOLOConcatDataset.collate_fn,\n)\ntest_loader = DataLoader(\n    voc2,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=YOLOConcatDataset.collate_fn,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nfor batch in train_loader:\n    # print(batch)\n    new_batch = {}\n    new_batch['batch_idx'] = batch['batch_idx'].to(device)\n    new_batch['bboxes'] = batch['bboxes'].to(device)\n    new_batch['bbox_format'] = batch['bbox_format']\n    new_batch['cls'] = batch['cls'].to(device)\n    new_batch['img'] = batch['img'].to(device)\n    new_batch['normalized'] = batch['normalized']\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pretrained YOLO Model","metadata":{}},{"cell_type":"code","source":"from ultralytics import YOLO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YOLO('yolo11m.pt').to(device)\nmodel = model.model # VERY IMPORTANT, AS THE OUTER MODEL IS WRAPPED IN YOLO CLASS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from types import SimpleNamespace\nmodel.args = SimpleNamespace(box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum(p.numel() for p in model.parameters())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"help(model.forward)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nmodel.train()\nfor batch in train_loader:\n    new_batch = {}\n    new_batch['batch_idx'] = batch['batch_idx'].to(device)\n    new_batch['bboxes'] = batch['bboxes'].to(device)\n    new_batch['bbox_format'] = batch['bbox_format']\n    new_batch['cls'] = batch['cls'].to(device)\n    new_batch['img'] = batch['img'].to(device)\n    new_batch['normalized'] = batch['normalized']\n    loss_components, others = model(new_batch)\n    print(loss_components)\n    print(others)\n    loss = loss_components.mean()\n    loss.backward()\n    break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train():\n    model.train()\n    total_loss = 0\n    cnt = 0\n    for batch in (pbar := tqdm(train_loader)):\n        new_batch = {}\n        new_batch['batch_idx'] = batch['batch_idx'].to(device)\n        new_batch['bboxes'] = batch['bboxes'].to(device)\n        new_batch['bbox_format'] = batch['bbox_format']\n        new_batch['cls'] = batch['cls'].to(device)\n        new_batch['img'] = batch['img'].to(device)\n        new_batch['normalized'] = batch['normalized']\n        loss_components, _ = model(new_batch)\n        loss = loss_components.mean()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        cnt += 1\n        pbar.set_description(f\"Average Loss: {total_loss / cnt :6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test():\n    model.eval()\n    total_loss = 0\n    cnt = 0\n    with torch.no_grad():\n        for batch in (pbar := tqdm(test_loader)):\n            new_batch = {}\n            new_batch['batch_idx'] = batch['batch_idx'].to(device)\n            new_batch['bboxes'] = batch['bboxes'].to(device)\n            new_batch['bbox_format'] = batch['bbox_format']\n            new_batch['cls'] = batch['cls'].to(device)\n            new_batch['img'] = batch['img'].to(device)\n            new_batch['normalized'] = batch['normalized']\n            loss_components, _ = model(new_batch)\n            loss = loss_components.mean()\n            total_loss += loss.item()\n            cnt += 1\n            pbar.set_description(f\"Testing: Average Loss: {total_loss / cnt :6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Actual Training","metadata":{}},{"cell_type":"code","source":"model = YOLO('yolo11m.pt').to(device)\nmodel = model.model # VERY IMPORTANT, AS THE OUTER MODEL IS WRAPPED IN YOLO CLASS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from types import SimpleNamespace\nmodel.args = SimpleNamespace(box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sum(p.numel() for p in model.parameters())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import *","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualisation","metadata":{}},{"cell_type":"code","source":"inference_model = YOLO('yolo11m.pt').to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_model.model.load_state_dict(model.state_dict())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch['img'].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_model.eval()\nwith torch.no_grad():\n    for batch in (pbar := tqdm(test_loader)):\n        results = inference_model.predict(              # run batched inference + NMS\n            batch['img'],                             # list/array/tensor of images\n            conf=0.75,                        # confidence threshold\n            iou=0.45,                         # NMS IoU threshold\n            device=device,                   # 'cpu' or 'cuda'\n            stream=False                      # returns list of Results\n        )\n        \n        # unpack the first image’s detections:\n        res     = results[0]\n        boxes   = res.boxes.xyxy.cpu().numpy()   # (N,4) array of x1,y1,x2,y2\n        scores  = res.boxes.conf.cpu().numpy()   # (N,) objectness scores\n        classes = res.boxes.cls.cpu().numpy()    # (N,) zero-based class IDs        \n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(batch['img'][1].permute(1, 2, 0).cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}