#!/usr/bin/env python3

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from typing import List
import tempfile
from datetime import datetime

# refinire-rag imports
from refinire_rag import Document, TFIDFEmbedder, TFIDFEmbeddingConfig

# Our ChromaDB vector store implementation
from src.chroma_vector_store import ChromaVectorStore


def create_sample_documents() -> List[Document]:
    """Create sample documents using refinire-rag Document class"""
    
    documents = [
        Document(
            id="doc_001",
            content="æ©Ÿæ¢°å­¦ç¿’ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç ”ç©¶ã—ã¾ã™ã€‚æ•™å¸«ã‚ã‚Šå­¦ç¿’ã€æ•™å¸«ãªã—å­¦ç¿’ã€å¼·åŒ–å­¦ç¿’ãªã©ã®æ‰‹æ³•ãŒã‚ã‚Šã¾ã™ã€‚",
            metadata={
                "path": "/docs/ml_intro.txt",
                "created_at": datetime.now().isoformat(),
                "file_type": "txt",
                "size_bytes": 150,
                "category": "machine_learning",
                "language": "japanese"
            }
        ),
        Document(
            id="doc_002",
            content="æ·±å±¤å­¦ç¿’ï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¤šå±¤åŒ–ã—ãŸæ‰‹æ³•ã§ã™ã€‚ç”»åƒèªè­˜ã€è‡ªç„¶è¨€èªå‡¦ç†ã€éŸ³å£°èªè­˜ãªã©ã§é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚",
            metadata={
                "path": "/docs/deep_learning.txt",
                "created_at": datetime.now().isoformat(),
                "file_type": "txt",
                "size_bytes": 120,
                "category": "deep_learning",
                "language": "japanese"
            }
        ),
        Document(
            id="doc_003",
            content="ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼æ€§æ¤œç´¢ã‚’åŠ¹ç‡çš„ã«è¡Œã†ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚embeddingæŠ€è¡“ã¨çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚",
            metadata={
                "path": "/docs/vector_db.txt",
                "created_at": datetime.now().isoformat(),
                "file_type": "txt",
                "size_bytes": 100,
                "category": "database",
                "language": "japanese"
            }
        ),
        Document(
            id="doc_004",
            content="ChromaDBã¯è»½é‡ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚Pythonã§æ›¸ã‹ã‚Œã¦ãŠã‚Šã€embeddingã®ä¿å­˜ã¨æ¤œç´¢ã«ç‰¹åŒ–ã—ã¦ã„ã¾ã™ã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            metadata={
                "path": "/docs/chromadb_overview.txt",
                "created_at": datetime.now().isoformat(),
                "file_type": "txt",
                "size_bytes": 130,
                "category": "database",
                "language": "japanese"
            }
        ),
        Document(
            id="doc_005",
            content="RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯æ¤œç´¢ã«ã‚ˆã‚Šå–å¾—ã—ãŸé–¢é€£æƒ…å ±ã‚’ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‹æ‰‹æ³•ã§ã™ã€‚çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ´»ç”¨ã—ã¦ç²¾åº¦ã®é«˜ã„å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚",
            metadata={
                "path": "/docs/rag_explanation.txt",
                "created_at": datetime.now().isoformat(),
                "file_type": "txt",
                "size_bytes": 140,
                "category": "nlp",
                "language": "japanese"
            }
        )
    ]
    
    return documents


def main():
    print("ğŸš€ refinire-rag + ChromaDB Vector Store å®Ÿçµ±åˆä¾‹")
    print("=" * 70)
    
    # 1. åˆæœŸåŒ–
    print("\nğŸ“‹ 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
    
    # TF-IDF Embedder ã‚’åˆæœŸåŒ–ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
    embedding_config = TFIDFEmbeddingConfig(
        max_features=1000,
        ngram_range=(1, 2),
        min_df=1,
        max_df=0.8
    )
    embedder = TFIDFEmbedder(config=embedding_config)
    
    # ChromaDB Vector Store ã‚’åˆæœŸåŒ–
    with tempfile.TemporaryDirectory() as temp_dir:
        vector_store = ChromaVectorStore(
            collection_name="refinire_rag_real_demo",
            persist_directory=temp_dir,
            distance_metric="cosine"
        )
        
        print(f"âœ… Embedder: TFIDFEmbedder (max_features={embedding_config.max_features})")
        print(f"âœ… Vector Store: ChromaDB (collection: {vector_store.collection_name})")
        print(f"âœ… Storage: {temp_dir}")
        
        # 2. ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã‚’ä½œæˆ
        print("\nğŸ“š 2. ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸ã®æº–å‚™")
        documents = create_sample_documents()
        print(f"ğŸ“„ {len(documents)}ä»¶ã®æ–‡æ›¸ã‚’æº–å‚™ã—ã¾ã—ãŸ")
        
        for i, doc in enumerate(documents, 1):
            print(f"   {i}. {doc.id}: {doc.content[:50]}...")
        
        # 3. æ–‡æ›¸ã‚’embeddingåŒ–
        print("\nğŸ§® 3. Embeddingç”Ÿæˆ")
        
        # TF-IDFãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ–‡æ›¸ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã§fitã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
        all_texts = [doc.content for doc in documents]
        embedder.fit(all_texts)
        print(f"âœ… TF-IDFãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸï¼ˆèªå½™æ•°: {len(embedder.get_vocabulary())}ï¼‰")
        
        # å„æ–‡æ›¸ã®embeddingã‚’ç”Ÿæˆ
        embedding_results = embedder.embed_documents(documents)
        embeddings = [result.vector.tolist() for result in embedding_results]
        print(f"âœ… {len(embeddings)}ä»¶ã®embeddingã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼ˆæ¬¡å…ƒ: {embedder.get_embedding_dimension()}ï¼‰")
        
        # 4. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ä¿å­˜
        print("\nğŸ’¾ 4. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¸ã®ä¿å­˜")
        vector_store.add_documents_with_embeddings(documents, embeddings)
        
        stats = vector_store.get_stats()
        print(f"âœ… ä¿å­˜å®Œäº†:")
        print(f"   - ç·ãƒ™ã‚¯ãƒˆãƒ«æ•°: {stats.total_vectors}")
        print(f"   - ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {stats.vector_dimension}")
        print(f"   - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¿ã‚¤ãƒ—: {stats.index_type}")
        print(f"   - ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: {vector_store.collection_name}")
        
        # 5. æ¤œç´¢ãƒ†ã‚¹ãƒˆ1: åŸºæœ¬æ¤œç´¢
        print("\nğŸ” 5. é¡ä¼¼æ¤œç´¢ãƒ†ã‚¹ãƒˆ")
        
        query_text = "æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¤ã„ã¦"
        print(f"ã‚¯ã‚¨ãƒª: '{query_text}'")
        
        # ã‚¯ã‚¨ãƒªã‚’embeddingåŒ–
        query_result = embedder.embed_text(query_text)
        query_embedding = query_result.vector.tolist()
        
        # é¡ä¼¼æ¤œç´¢å®Ÿè¡Œ
        search_results = vector_store.search_similar(
            query_embedding=query_embedding,
            top_k=3
        )
        
        print(f"\nğŸ“‹ æ¤œç´¢çµæœï¼ˆä¸Šä½{len(search_results)}ä»¶ï¼‰:")
        for i, result in enumerate(search_results, 1):
            category = result.metadata.get('category', 'unknown')
            
            print(f"  {i}. ID: {result.document_id}")
            print(f"     ã‚¹ã‚³ã‚¢: {result.score:.4f}")
            print(f"     ã‚«ãƒ†ã‚´ãƒª: {category}")
            print(f"     å†…å®¹: {result.content[:80]}...")
            print()
        
        # 6. æ¤œç´¢ãƒ†ã‚¹ãƒˆ2: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿
        print("\nğŸ” 6. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿æ¤œç´¢")
        
        filter_query = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ "
        metadata_filter = {"category": "database"}
        
        print(f"ã‚¯ã‚¨ãƒª: '{filter_query}'")
        print(f"ãƒ•ã‚£ãƒ«ã‚¿: {metadata_filter}")
        
        filter_query_result = embedder.embed_text(filter_query)
        filter_query_embedding = filter_query_result.vector.tolist()
        filtered_results = vector_store.search_similar(
            query_embedding=filter_query_embedding,
            top_k=5,
            metadata_filter=metadata_filter
        )
        
        print(f"\nğŸ“‹ ãƒ•ã‚£ãƒ«ã‚¿æ¤œç´¢çµæœï¼ˆ{len(filtered_results)}ä»¶ï¼‰:")
        for i, result in enumerate(filtered_results, 1):
            category = result.metadata.get('category', 'unknown')
            
            print(f"  {i}. ID: {result.document_id}")
            print(f"     ã‚¹ã‚³ã‚¢: {result.score:.4f}")
            print(f"     ã‚«ãƒ†ã‚´ãƒª: {category}")
            print(f"     å†…å®¹: {result.content[:80]}...")
            print()
        
        # 7. é¡ä¼¼æ–‡æ›¸æ¤œç´¢
        print("\nğŸ” 7. æ–‡æ›¸é–“é¡ä¼¼æ€§æ¤œç´¢")
        
        reference_doc_id = "doc_001"  # æ©Ÿæ¢°å­¦ç¿’ã®æ–‡æ›¸
        print(f"åŸºæº–æ–‡æ›¸: {reference_doc_id}")
        
        similar_docs = vector_store.search_similar_to_document(
            document_id=reference_doc_id,
            top_k=3
        )
        
        print(f"\nğŸ“‹ {reference_doc_id}ã«é¡ä¼¼ã™ã‚‹æ–‡æ›¸ï¼ˆ{len(similar_docs)}ä»¶ï¼‰:")
        for i, result in enumerate(similar_docs, 1):
            category = result.metadata.get('category', 'unknown')
            
            print(f"  {i}. ID: {result.document_id}")
            print(f"     ã‚¹ã‚³ã‚¢: {result.score:.4f}")
            print(f"     ã‚«ãƒ†ã‚´ãƒª: {category}")
            print(f"     å†…å®¹: {result.content[:80]}...")
            print()
        
        # 8. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢
        print("\nğŸ” 8. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢")
        
        metadata_search_filter = {"language": "japanese", "category": "machine_learning"}
        print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢æ¡ä»¶: {metadata_search_filter}")
        
        metadata_results = vector_store.search_by_metadata(metadata_search_filter)
        
        print(f"\nğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢çµæœï¼ˆ{len(metadata_results)}ä»¶ï¼‰:")
        for i, vector in enumerate(metadata_results, 1):
            category = vector.metadata.get('category', 'unknown')
            
            print(f"  {i}. ID: {vector.document_id}")
            print(f"     ã‚«ãƒ†ã‚´ãƒª: {category}")
            print(f"     å†…å®¹: {vector.content[:80]}...")
            print()
        
        # 9. Embedderæƒ…å ±è¡¨ç¤º
        print("\nğŸ“Š 9. ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        
        try:
            embedder_info = embedder.get_embedder_info()
            print(f"Embedderæƒ…å ±:")
            print(f"  - ã‚¿ã‚¤ãƒ—: {embedder_info.get('type', 'TFIDFEmbedder')}")
            print(f"  - æ¬¡å…ƒæ•°: {embedder.get_embedding_dimension()}")
            print(f"  - èªå½™ã‚µã‚¤ã‚º: {len(embedder.get_vocabulary()) if hasattr(embedder, 'get_vocabulary') else 'N/A'}")
        except Exception as e:
            print(f"Embedderæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        try:
            embedding_stats = embedder.get_embedding_stats()
            print(f"\nEmbeddingçµ±è¨ˆ:")
            print(f"  - å‡¦ç†æ¸ˆã¿æ–‡æ›¸æ•°: {embedding_stats.get('documents_processed', 0)}")
            print(f"  - å‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆæ•°: {embedding_stats.get('texts_processed', 0)}")
        except Exception as e:
            print(f"Embeddingçµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        
        final_stats = vector_store.get_stats()
        print(f"\nVector Storeçµ±è¨ˆ:")
        print(f"  - ç·ãƒ™ã‚¯ãƒˆãƒ«æ•°: {final_stats.total_vectors}")
        print(f"  - ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {final_stats.vector_dimension}")
        print(f"  - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¿ã‚¤ãƒ—: {final_stats.index_type}")
        print(f"  - è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯: {vector_store.distance_metric}")
        
    print("\nğŸ‰ å®Ÿçµ±åˆä¾‹ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("\nğŸ’¡ ã“ã®ä¾‹ã§ã¯ä»¥ä¸‹ã‚’å®Ÿæ¼”ã—ã¾ã—ãŸ:")
    print("   â€¢ å®Ÿéš›ã®refinire-rag Document/Embedderã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨")
    print("   â€¢ ChromaDB Vector Storeã®ç¶™æ‰¿å®Ÿè£…")
    print("   â€¢ TF-IDFåŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨ä¿å­˜")
    print("   â€¢ å„ç¨®æ¤œç´¢æ©Ÿèƒ½ï¼ˆé¡ä¼¼æ€§ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼‰")
    print("   â€¢ refinire-ragã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å®Œå…¨çµ±åˆ")


if __name__ == "__main__":
    main()