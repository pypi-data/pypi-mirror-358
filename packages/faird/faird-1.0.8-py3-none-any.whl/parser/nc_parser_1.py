import os
import netCDF4 as nc
import numpy as np
import pyarrow as pa
import pyarrow.ipc as ipc
from utils.logger_utils import get_logger
logger = get_logger(__name__)

from parser.abstract_parser import BaseParser


class NCParser(BaseParser):
    """
    NetCDF file parser implementing the BaseParser interface.
    æ”¯æŒå¤šç»´æ•°ç»„ï¼ˆ1D, 2D, 3Dï¼‰ï¼Œæ”¯æŒ object ç±»å‹å­—æ®µï¼Œå¹¶å¯æ­£ç¡®è¯»å†™ã€‚
    """

    def parse(self, file_path: str) -> pa.Table:
        """
        Parse a NetCDF (.nc) file into a pyarrow Table.

        Args:
            file_path (str): Path to the input NetCDF file.
        Returns:
            pa.Table: A pyarrow Table object representing the NetCDF data.
        """

        # è®¾ç½®ç¼“å­˜è·¯å¾„
        DEFAULT_ARROW_CACHE_PATH = os.path.expanduser("~/.cache/faird/dataframe/nc/")
        os.makedirs(DEFAULT_ARROW_CACHE_PATH, exist_ok=True)

        # æ„é€ ç¼“å­˜æ–‡ä»¶è·¯å¾„
        arrow_file_name = os.path.basename(file_path).rsplit(".", 1)[0] + ".arrow"
        arrow_file_path = os.path.join(DEFAULT_ARROW_CACHE_PATH, arrow_file_name)

        # å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥ä»ç¼“å­˜åŠ è½½
        if os.path.exists(arrow_file_path):
            logger.info(f"ğŸ” ä»ç¼“å­˜åŠ è½½ {arrow_file_path}")
            try:
                with pa.memory_map(arrow_file_path, "r") as source:
                    result = ipc.open_file(source).read_all()
                logger.info("âœ… ç¼“å­˜åŠ è½½æˆåŠŸï¼Œè¿”å› Arrow Table")
                logger.info("   Schema:", result.schema)
                logger.info("   Type of result:", type(result))
                return result
            except Exception as e:
                logger.info(f"ğŸš¨ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°è§£æ: {e}")

        # æ‰“å¼€ NetCDF æ–‡ä»¶
        logger.info(f"ğŸ“‚ æ­£åœ¨è§£æ NetCDF æ–‡ä»¶: {file_path}")
        dataset = nc.Dataset(file_path)

        arrays = []
        names = []

        # å­˜å‚¨ä¸€ç»´ç»´åº¦æ•°æ®ï¼ˆå¦‚ time, lat, lonï¼‰
        dim_vars = {}

        # ç¬¬ä¸€æ¬¡éå†ï¼šæ”¶é›†æ‰€æœ‰ä¸€ç»´å˜é‡ä½œä¸ºç»´åº¦
        for var_name in dataset.variables:
            variable = dataset.variables[var_name]
            if len(variable.shape) == 1:
                dim_vars[var_name] = variable[:]
                logger.info(f"   ğŸ“ ç»´åº¦å˜é‡ '{var_name}' åŠ è½½å®Œæˆï¼Œé•¿åº¦: {len(dim_vars[var_name])}")

        # ç¬¬äºŒæ¬¡éå†ï¼šå¤„ç†å¤šç»´å˜é‡å¹¶æ„å»ºæ•°ç»„
        for var_name in dataset.variables:
            variable = dataset.variables[var_name]
            data = variable[:]
            dtype = data.dtype.name

            # è·³è¿‡å·²å¤„ç†çš„ä¸€ç»´ç»´åº¦å˜é‡
            if len(data.shape) == 1 and var_name in dim_vars:
                continue

            # è·å–å˜é‡çš„ç»´åº¦å
            dim_names = variable.dimensions
            if not dim_names:
                logger.info(f"âš ï¸ å˜é‡ '{var_name}' æ²¡æœ‰ç»´åº¦ä¿¡æ¯ï¼Œè·³è¿‡")
                continue

            # å¦‚æœæ˜¯æ ‡é‡æˆ–ç©ºå˜é‡ï¼Œè·³è¿‡
            if len(data.shape) == 0:
                logger.info(f"âš ï¸ å˜é‡ '{var_name}' æ˜¯æ ‡é‡ï¼Œè·³è¿‡")
                continue

            # ä¸»ç»´åº¦ï¼ˆé€šå¸¸ç¬¬ä¸€ä¸ªç»´åº¦ä¸ºä¸»æ—¶é—´è½´ç­‰ï¼‰
            main_dim_name = dim_names[0]
            main_dim_size = data.shape[0]

            # æ„å»ºå¹¿æ’­åçš„åæ ‡ç½‘æ ¼
            coord_grids = []

            for i, dim_name in enumerate(dim_names):
                if dim_name not in dim_vars:
                    raise ValueError(f"Dimension '{dim_name}' not found in variables")

                dim_data = dim_vars[dim_name]
                expand_shape = [1] * len(dim_names)
                expand_shape[i] = -1
                expanded = np.broadcast_to(dim_data.reshape(expand_shape), data.shape)
                coord_grids.append(expanded.flatten())

            # æ·»åŠ æœªæ·»åŠ è¿‡çš„ç»´åº¦åˆ—
            for i, dim_name in enumerate(dim_names):
                if dim_name not in names:
                    arrays.append(pa.array(coord_grids[i], type=pa.from_numpy_dtype(coord_grids[i].dtype)))
                    names.append(dim_name)
                    logger.info(f"   â• æ·»åŠ ç»´åº¦åˆ— '{dim_name}', é•¿åº¦: {coord_grids[i].shape[0]}")

            # å±•å¹³æ•°æ®å¹¶æ·»åŠ åˆ°æ•°ç»„ä¸­
            flat_data = data.flatten()
            if dtype == 'object':
                if isinstance(flat_data[0], str):
                    pa_type = pa.string()
                else:
                    pa_type = pa.binary()
                array = pa.array(flat_data, type=pa_type)
            else:
                pa_type_class = getattr(pa, dtype, None)
                if pa_type_class is None:
                    raise ValueError(f"Unsupported data type: {dtype}")
                array = pa.array(flat_data, type=pa_type_class())

            arrays.append(array)
            names.append(var_name)
            logger.info(f"   â• æ·»åŠ å˜é‡ '{var_name}', æ•°æ®é•¿åº¦: {flat_data.shape[0]}")

        # åˆ›å»º Arrow è¡¨æ ¼
        logger.info("ğŸ“Š å¼€å§‹æ„å»º Arrow Table...")
        try:
            table = pa.Table.from_arrays(arrays, names=names)
            logger.info("âœ… Arrow Table æ„å»ºæˆåŠŸ")
            logger.info("   Schema:", table.schema)
            logger.info("   Number of rows:", table.num_rows)
        except pa.lib.ArrowInvalid as e:
            logger.info("âŒ æ„å»º Arrow Table å¤±è´¥: åˆ—é•¿åº¦ä¸ä¸€è‡´")
            for i, arr in enumerate(arrays):
                logger.info(f"   Column '{names[i]}': length={len(arr)}")
            raise

        # å†™å…¥ç¼“å­˜
        logger.info(f"ğŸ’¾ å†™å…¥ç¼“å­˜æ–‡ä»¶: {arrow_file_path}")
        try:
            with ipc.new_file(arrow_file_path, table.schema) as writer:
                writer.write_table(table)
            logger.info("   âœ… ç¼“å­˜å†™å…¥æˆåŠŸ")
        except Exception as e:
            logger.info(f"   âŒ ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
            raise

        # é›¶æ‹·è´è¯»å–è¿”å›
        logger.info("ğŸ” ä»ç¼“å­˜ä¸­åŠ è½½ Arrow Table è¿”å›")
        try:
            with pa.memory_map(arrow_file_path, "r") as source:
                result = ipc.open_file(source).read_all()
            logger.info("âœ… æˆåŠŸåŠ è½½ç¼“å­˜ä¸­çš„ Arrow Table")
            logger.info("   Schema:", result.schema)
            logger.info("   Type of result:", type(result))
            return result
        except Exception as e:
            logger.info(f"ğŸš¨ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            raise

    def write(self, table: pa.Table, output_path: str):
        """
        å°† pyarrow.Table å†™å› NetCDF æ–‡ä»¶ï¼Œæ”¯æŒå¤šç»´æ•°ç»„ã€‚

        Args:
            table (pa.Table): è¦å†™å…¥çš„æ•°æ®ã€‚
            output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
        """
        with nc.Dataset(output_path, 'w', format='NETCDF4') as dataset:
            # åˆ›å»ºä¸»ç»´åº¦ï¼ˆè¡Œæ•°ï¼‰
            row_dim = dataset.createDimension('row', len(table))

            # éå†æ¯ä¸€åˆ—
            for col_name in table.column_names:
                col_array = table[col_name]

                # åˆ¤æ–­æ˜¯å¦ä¸º FixedSizeListArrayï¼ˆå³äºŒç»´æˆ–ä¸‰ç»´ç»“æ„ï¼‰
                if pa.types.is_fixed_size_list(col_array.type):
                    inner_size = col_array.type.list_size
                    value_type = col_array.type.value_type.to_pandas_dtype()

                    # å¤„ç†äºŒç»´ç»“æ„
                    if pa.types.is_fixed_size_list(col_array.values.type):
                        outer_size = col_array.values.type.list_size
                        total_dim = dataset.createDimension(f"{col_name}_dim", inner_size * outer_size)
                        var = dataset.createVariable(col_name, value_type, ('row', total_dim))
                        values = col_array.flatten().to_numpy().reshape(-1, outer_size * inner_size)
                        var[:, :] = values

                    # å¤„ç†ä¸€ç»´åµŒå¥—ç»“æ„
                    else:
                        inner_dim_name = f"{col_name}_dim"
                        inner_dim = dataset.createDimension(inner_dim_name, inner_size)
                        var = dataset.createVariable(col_name, value_type, ('row', inner_dim_name))
                        values = col_array.flatten().to_numpy().reshape(-1, inner_size)
                        var[:, :] = values

                elif pa.types.is_string(col_array.type) or pa.types.is_binary(col_array.type):
                    # å­—ç¬¦ä¸²æˆ–äºŒè¿›åˆ¶ç±»å‹ï¼ˆæ¥è‡ªåŸå§‹ object ç±»å‹ï¼‰
                    var = dataset.createVariable(col_name, str, ('row',))
                    var[:] = col_array.to_numpy().astype(str)

                elif pa.types.is_primitive(col_array.type):
                    # ä¸€ç»´åŸºæœ¬ç±»å‹æ•°ç»„
                    var_type = col_array.type.to_pandas_dtype()
                    var = dataset.createVariable(col_name, var_type, ('row',))
                    var[:] = col_array.to_numpy()

                else:
                    raise ValueError(f"Unsupported array type: {col_array.type}")
