# Tamar Model Client

**Tamar Model Client** æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½çš„ Python SDKï¼Œç”¨äºè¿æ¥ Model Manager gRPC æœåŠ¡ï¼Œç»Ÿä¸€è°ƒç”¨å¤šå®¶ç¬¬ä¸‰æ–¹ AI
æ¨¡å‹æœåŠ¡å•†ï¼ˆå¦‚OpenAIã€Googleã€Azure OpenAIï¼‰ã€‚

## âœ¨ ç‰¹æ€§äº®ç‚¹

- ğŸ§© æ”¯æŒ **åŒæ­¥** / **å¼‚æ­¥**è°ƒç”¨ï¼Œ**æµå¼** / **éæµå¼** å“åº”
- âš¡ ç»Ÿä¸€å°è£… **OpenAI** / **Google** / **Azure OpenAI**ï¼Œå¹¶å…¼å®¹ **å®˜æ–¹SDK** è°ƒç”¨æ ‡å‡†
- ğŸ”— **gRPC** é«˜æ•ˆé€šä¿¡ï¼Œå†…ç½® **JWT** è®¤è¯ã€é‡è¯•æœºåˆ¶
- ğŸ›¡ï¸ **ç±»å‹å®‰å…¨æ ¡éªŒ**ï¼ˆåŸºäº Pydantic v2ï¼‰
- ğŸ“š **å®Œæ•´å¼‚å¸¸å¤„ç†**ï¼ŒAPI ç®€å•ç›´è§‚ï¼Œæ”¯æŒæ‰¹é‡è°ƒç”¨

## ğŸ“‹ å®‰è£…

```bash
pip install tamar-model-client
```

æ”¯æŒç¯å¢ƒï¼š

- Python â‰¥ 3.8
- Windows / Linux / macOS

## ğŸ—ï¸ é¡¹ç›®ç»“æ„æ¦‚è§ˆ

```
tamar_model_client/
â”œâ”€â”€ generated/                      # gRPC ç”Ÿæˆçš„ä»£ç 
â”‚   â”œâ”€â”€ model_service.proto         # åè®®å®šä¹‰æ–‡ä»¶
â”‚   â”œâ”€â”€ model_service_pb2.py        # ç”Ÿæˆçš„ protobuf ä»£ç 
â”‚   â””â”€â”€ model_service_pb2_grpc.py   # ç”Ÿæˆçš„ gRPC ä»£ç 
â”œâ”€â”€ schemas/                  # æ•°æ®æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ inputs.py             # è¾“å…¥æ¨¡å‹å®šä¹‰
â”‚   â””â”€â”€ outputs.py            # è¾“å‡ºæ¨¡å‹å®šä¹‰
â”œâ”€â”€ enums/                    # æšä¸¾ç±»å‹å®šä¹‰
â”‚   â”œâ”€â”€ providers.py          # æ¨¡å‹æä¾›å•†æšä¸¾
â”‚   â”œâ”€â”€ invoke.py             # è°ƒç”¨ç±»å‹æšä¸¾
â”‚   â””â”€â”€ channel.py            # æ¸ é“ç±»å‹æšä¸¾
â”œâ”€â”€ async_client.py           # å¼‚æ­¥å®¢æˆ·ç«¯å®ç°
â”œâ”€â”€ sync_client.py            # åŒæ­¥å®¢æˆ·ç«¯å®ç°
â”œâ”€â”€ exceptions.py             # è‡ªå®šä¹‰å¼‚å¸¸
â”œâ”€â”€ auth.py                   # JWTè®¤è¯å¤„ç†å™¨
â””â”€â”€ __init__.py               # åŒ…åˆå§‹åŒ–
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®¢æˆ·ç«¯åˆå§‹åŒ–

```python
from tamar_model_client import TamarModelClient, AsyncTamarModelClient

# åŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient(
    server_address="localhost:50051",
    jwt_token="your-jwt-token"
)

# å¼‚æ­¥å®¢æˆ·ç«¯
async_client = AsyncTamarModelClient(
    server_address="localhost:50051",
    jwt_secret_key="your-jwt-secret-key"  # ä½¿ç”¨å›ºå®šå¯†é’¥è‡ªåŠ¨ç”Ÿæˆ JWT
)
```

> ğŸ’¡ å»ºè®®é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®è¿æ¥ä¿¡æ¯ï¼Œå‡å°‘ç¡¬ç¼–ç é£é™©ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚


## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

#### OpenAI è°ƒç”¨ç¤ºä¾‹

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# OpenAI è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.OPENAI,  # é€‰æ‹© OpenAI ä½œä¸ºæä¾›å•†
    channel=Channel.OPENAI,  # ä½¿ç”¨ OpenAI æ¸ é“
    invoke_type=InvokeType.CHAT_COMPLETIONS,  # ä½¿ç”¨ chat completions è°ƒç”¨ç±»å‹
    model="gpt-4",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    stream=False,  # éæµå¼è°ƒç”¨
    temperature=0.7,  # å¯é€‰å‚æ•°
    max_tokens=1000,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")
```

#### Google è°ƒç”¨ç¤ºä¾‹ ï¼ˆAI Studio / Vertex AIï¼‰

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# Google AI Studio è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Google ä½œä¸ºæä¾›å•†
    channel=Channel.AI_STUDIO,  # ä½¿ç”¨ AI Studio æ¸ é“
    invoke_type=InvokeType.GENERATION,  # ä½¿ç”¨ç”Ÿæˆè°ƒç”¨ç±»å‹
    model="gemini-pro",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    contents=[
        {"role": "user", "parts": [{"text": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    temperature=0.7,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")

# Google Vertex AI è°ƒç”¨ç¤ºä¾‹
vertex_request = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Google ä½œä¸ºæä¾›å•†
    channel=Channel.VERTEXAI,  # ä½¿ç”¨ Vertex AI æ¸ é“
    invoke_type=InvokeType.GENERATION,  # ä½¿ç”¨ç”Ÿæˆè°ƒç”¨ç±»å‹
    model="gemini-pro",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    contents=[
        {"role": "user", "parts": [{"text": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    temperature=0.7,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
vertex_response = client.invoke(vertex_request)
if vertex_response.error:
    print(f"é”™è¯¯: {vertex_response.error}")
else:
    print(f"å“åº”: {vertex_response.content}")
    if vertex_response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {vertex_response.usage}")
```

#### Azure OpenAI è°ƒç”¨ç¤ºä¾‹

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# Azure OpenAI è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.AZURE,  # é€‰æ‹© Azure ä½œä¸ºæä¾›å•†
    channel=Channel.OPENAI,  # ä½¿ç”¨ OpenAI æ¸ é“
    invoke_type=InvokeType.CHAT_COMPLETIONS,  # ä½¿ç”¨ chat completions è°ƒç”¨ç±»å‹
    model="gpt-4o-mini",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    stream=False,  # éæµå¼è°ƒç”¨
    temperature=0.7,  # å¯é€‰å‚æ•°
    max_tokens=1000,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")
```

### å¼‚æ­¥è°ƒç”¨ç¤ºä¾‹

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def main():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…è¯·æ±‚å‚æ•°
    request_data = ModelRequest(
        provider=ProviderType.OPENAI,
        channel=Channel.OPENAI,
        invoke_type=InvokeType.CHAT_COMPLETIONS,
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ],
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        stream=False,
        temperature=0.7,
        max_tokens=1000,
    )

    # å‘é€è¯·æ±‚å¹¶è·å–å“åº”
    async for r in await client.invoke(model_request):
        if r.error:
            print(f"é”™è¯¯: {r.error}")
        else:
            print(f"å“åº”: {r.content}")
            if r.usage:
                print(f"Token ä½¿ç”¨æƒ…å†µ: {r.usage}")


# è¿è¡Œå¼‚æ­¥ç¤ºä¾‹
asyncio.run(main())
```

### æµå¼è°ƒç”¨ç¤ºä¾‹

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def stream_example():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…è¯·æ±‚å‚æ•°
    request_data = ModelRequest(
        provider=ProviderType.OPENAI,
        channel=Channel.OPENAI,
        invoke_type=InvokeType.CHAT_COMPLETIONS,
        model="gpt-4",
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ],
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        stream=True,  # å¯ç”¨æµå¼è¾“å‡º
        temperature=0.7,
    )

    # å‘é€è¯·æ±‚å¹¶è·å–æµå¼å“åº”
    async for response in client.invoke(request_data):
        if response.error:
            print(f"é”™è¯¯: {response.error}")
        else:
            print(f"å“åº”ç‰‡æ®µ: {response.content}", end="", flush=True)
            if response.usage:
                print(f"\nToken ä½¿ç”¨æƒ…å†µ: {response.usage}")


# è¿è¡Œæµå¼ç¤ºä¾‹
asyncio.run(stream_example())
```

### æ‰¹é‡è°ƒç”¨ç¤ºä¾‹

æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªæ¨¡å‹è¯·æ±‚ï¼š

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import (
    BatchModelRequest, BatchModelRequestItem,
    UserContext
)
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def batch_example():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…æ‰¹é‡è¯·æ±‚å‚æ•°
    batch_request = BatchModelRequest(
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        items=[
            BatchModelRequestItem(
                provider=ProviderType.OPENAI,
                channel=Channel.OPENAI,
                invoke_type=InvokeType.CHAT_COMPLETIONS,
                model="gpt-4",
                messages=[
                    {"role": "user", "content": "ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}
                ],
                priority=1,
                custom_id="q1"
            ),
            BatchModelRequestItem(
                provider=ProviderType.GOOGLE,
                channel=Channel.AI_STUDIO,
                invoke_type=InvokeType.GENERATION,
                model="gemini-pro",
                contents=[
                    {"role": "user", "parts": [{"text": "ç¬¬äºŒä¸ªé—®é¢˜ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}]}
                ],
                priority=2,
                custom_id="q2"
            )
        ]
    )

    # å‘é€æ‰¹é‡è¯·æ±‚å¹¶è·å–å“åº”
    response = await client.invoke_batch(batch_request)
    if response.responses:
        for resp in response.responses:
            print(f"\né—®é¢˜ {resp.custom_id} çš„å“åº”:")
            if resp.error:
                print(f"é”™è¯¯: {resp.error}")
            else:
                print(f"å†…å®¹: {resp.content}")
                if resp.usage:
                    print(f"Token ä½¿ç”¨æƒ…å†µ: {resp.usage}")


# è¿è¡Œæ‰¹é‡è°ƒç”¨ç¤ºä¾‹
asyncio.run(batch_example())
```

### æ–‡ä»¶è¾“å…¥ç¤ºä¾‹

æ”¯æŒå¤„ç†å›¾åƒç­‰æ–‡ä»¶è¾“å…¥ï¼ˆéœ€ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼Œå¦‚ gemini-2.0-flashï¼‰ï¼š

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType
from google.genai.types import Part
model_request = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Googleä½œä¸ºæä¾›å•†
    model="gemini-2.0-flash",
    contents=[
        "What is shown in this image?",
        Part.from_uri( # è¿™ä¸ªæ˜¯Googleé‚£è¾¹çš„å‚æ•°æ”¯æŒ
            file_uri="https://images.pexels.com/photos/248797/pexels-photo-248797.jpeg",
            mime_type="image/jpeg",
        ),
    ],
    user_context=UserContext(
        org_id="testllm",
        user_id="testllm",
        client_type="conversation-service"
    ),
)
client = TamarModelClient("localhost:50051")
response = client.invoke(
    model_request=model_request
)
```

### âš ï¸ æ³¨æ„äº‹é¡¹

ä»¥ä¸‹æ˜¯ä½¿ç”¨ Tamar Model Client æ—¶çš„é‡è¦æç¤ºï¼š

- **å‚æ•°å¤„ç†**
  - å…¬å…±å‚æ•°åŒ…æ‹¬ï¼š**æœåŠ¡å•† (provider)**ã€**æ¸ é“ (channel)** ã€ **è°ƒç”¨æ–¹æ³• (invoke_type)** ä»¥åŠ **ç”¨æˆ·ä¿¡æ¯ï¼ˆuser_contextï¼‰**
  - å…¶ä¸­ **channel** å’Œ **invoke_type** ä¸ºå¯é€‰å‚æ•°ï¼Œ**å»ºè®®é»˜è®¤ä½¿ç”¨ç³»ç»Ÿè‡ªåŠ¨æ¨æ–­**ï¼Œé™¤éæœ‰ç‰¹æ®Šéœ€æ±‚å†æ˜¾å¼æŒ‡å®š
  - æ˜¯å¦æµå¼è¾“å‡ºç”±å…¬å…±å‚æ•° **stream** æ§åˆ¶ï¼Œå…¶ä»–å‚æ•°éµå¾ªå¯¹åº”æœåŠ¡å•†å®˜æ–¹ SDK çš„æ ‡å‡†å®šä¹‰
- **å®¢æˆ·ç«¯è¿æ¥ç®¡ç†**
  - gRPC ä½¿ç”¨ HTTP/2 é•¿è¿æ¥ï¼Œ**å»ºè®®å°†å®¢æˆ·ç«¯å®ä¾‹ä½œä¸ºå•ä¾‹ä½¿ç”¨**
  - è‹¥éœ€åˆ›å»ºå¤šä¸ªå®ä¾‹ï¼Œ**è¯·åŠ¡å¿…è°ƒç”¨** `client.close()` **æ–¹æ³•æ‰‹åŠ¨å…³é—­è¿æ¥**ï¼Œä»¥é˜²æ­¢è¿æ¥å †ç§¯æˆ–èµ„æºæ³„éœ²
- **å¼‚å¸¸å¤„ç†**ï¼š
  - æ‰€æœ‰æ¥å£å‡æä¾›è¯¦ç»†çš„**é”™è¯¯ä¿¡æ¯** ä»¥åŠ **è¯·æ±‚IDï¼ˆrequest_idï¼‰**ï¼Œä¸šåŠ¡è°ƒç”¨æ—¶å»ºè®®çº³å…¥å¯¹åº”æ—¥å¿—ä¾¿äºåæœŸæ’é”™

## âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ¨èï¼‰

å¯ä»¥é€šè¿‡ .env æ–‡ä»¶æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼Œè‡ªåŠ¨é…ç½®è¿æ¥ä¿¡æ¯

```bash
export MODEL_MANAGER_SERVER_ADDRESS="localhost:50051"
export MODEL_MANAGER_SERVER_JWT_TOKEN="your-jwt-secret"
export MODEL_MANAGER_SERVER_GRPC_USE_TLS="false"
export MODEL_MANAGER_SERVER_GRPC_DEFAULT_AUTHORITY="localhost"
export MODEL_MANAGER_SERVER_GRPC_MAX_RETRIES="5"
export MODEL_MANAGER_SERVER_GRPC_RETRY_DELAY="1.5"
```

æˆ–è€…æœ¬åœ° `.env` æ–‡ä»¶

```
# ========================
# ğŸ”Œ gRPC é€šä¿¡é…ç½®
# ========================

# gRPC æœåŠ¡ç«¯åœ°å€ï¼ˆå¿…å¡«ï¼‰
MODEL_MANAGER_SERVER_ADDRESS=localhost:50051

# æ˜¯å¦å¯ç”¨ TLS åŠ å¯†é€šé“ï¼ˆtrue/falseï¼Œé»˜è®¤ trueï¼‰
MODEL_MANAGER_SERVER_GRPC_USE_TLS=true

# å½“ä½¿ç”¨ TLS æ—¶æŒ‡å®š authorityï¼ˆåŸŸåå¿…é¡»å’Œè¯ä¹¦åŒ¹é…æ‰éœ€è¦ï¼‰
MODEL_MANAGER_SERVER_GRPC_DEFAULT_AUTHORITY=localhost


# ========================
# ğŸ” é‰´æƒé…ç½®ï¼ˆJWTï¼‰
# ========================

# JWT ç­¾åå¯†é’¥ï¼ˆç”¨äºç”Ÿæˆ Tokenï¼‰
MODEL_MANAGER_SERVER_JWT_SECRET_KEY=your_jwt_secret_key


# ========================
# ğŸ” é‡è¯•é…ç½®ï¼ˆå¯é€‰ï¼‰
# ========================

# æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 3ï¼‰
MODEL_MANAGER_SERVER_GRPC_MAX_RETRIES=3

# åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼Œé»˜è®¤ 1.0ï¼‰ï¼ŒæŒ‡æ•°é€€é¿
MODEL_MANAGER_SERVER_GRPC_RETRY_DELAY=1.0
```

åŠ è½½åï¼Œåˆå§‹åŒ–æ—¶æ— éœ€ä¼ å‚ï¼š

```python
from tamar_model_client import TamarModelClient

client = TamarModelClient()  # å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®
```

## å¼€å‘

### ç¯å¢ƒè®¾ç½®

1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# æˆ–
.venv\Scripts\activate  # Windows
```

2. å®‰è£…å¼€å‘ä¾èµ–ï¼š

```bash
pip install -e .
```

### ç”Ÿæˆ gRPC ä»£ç 

è¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆ gRPC ç›¸å…³ä»£ç ï¼š

```bash
python make_grpc.py
```

### éƒ¨ç½²åˆ° pip
```bash
python setup.py sdist bdist_wheel
twine upload dist/*

```

## è®¸å¯è¯

MIT License

## ä½œè€…

- Oscar Ou (oscar.ou@tamaredge.ai)

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼ 