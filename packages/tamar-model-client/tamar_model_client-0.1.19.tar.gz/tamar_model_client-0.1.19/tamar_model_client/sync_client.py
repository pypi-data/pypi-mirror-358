import base64
import json
import logging
import os
import time
import uuid
import grpc
from typing import Optional, Union, Iterable, Iterator
from contextvars import ContextVar

from openai import NOT_GIVEN
from pydantic import BaseModel

from .auth import JWTAuthHandler
from .enums import ProviderType, InvokeType
from .exceptions import ConnectionError
from .generated import model_service_pb2, model_service_pb2_grpc
from .schemas import BatchModelResponse, ModelResponse
from .schemas.inputs import GoogleGenAiInput, GoogleVertexAIImagesInput, OpenAIResponsesInput, \
    OpenAIChatCompletionsInput, OpenAIImagesInput, OpenAIImagesEditInput, BatchModelRequest, ModelRequest
from .json_formatter import JSONFormatter

logger = logging.getLogger(__name__)

_request_id: ContextVar[str] = ContextVar('request_id', default='-')


class RequestIdFilter(logging.Filter):
    """è‡ªå®šä¹‰æ—¥å¿—è¿‡æ»¤å™¨ï¼Œå‘æ—¥å¿—ä¸­æ·»åŠ  request_id"""

    def filter(self, record):
        # ä» ContextVar ä¸­è·å–å½“å‰çš„ request_id
        record.request_id = _request_id.get()
        return True


if not logger.hasHandlers():
    # åˆ›å»ºæ—¥å¿—å¤„ç†å™¨ï¼Œè¾“å‡ºåˆ°æ§åˆ¶å°
    console_handler = logging.StreamHandler()

    # ä½¿ç”¨ JSON æ ¼å¼åŒ–å™¨
    formatter = JSONFormatter()
    console_handler.setFormatter(formatter)

    # ä¸ºå½“å‰è®°å½•å™¨æ·»åŠ å¤„ç†å™¨
    logger.addHandler(console_handler)

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.setLevel(logging.INFO)

    # å°†è‡ªå®šä¹‰çš„ RequestIdFilter æ·»åŠ åˆ° logger ä¸­
    logger.addFilter(RequestIdFilter())

MAX_MESSAGE_LENGTH = 2 ** 31 - 1  # å¯¹äº32ä½ç³»ç»Ÿ


def is_effective_value(value) -> bool:
    """
    é€’å½’åˆ¤æ–­valueæ˜¯å¦æ˜¯æœ‰æ„ä¹‰çš„æœ‰æ•ˆå€¼
    """
    if value is None or value is NOT_GIVEN:
        return False

    if isinstance(value, str):
        return value.strip() != ""

    if isinstance(value, bytes):
        return len(value) > 0

    if isinstance(value, dict):
        for v in value.values():
            if is_effective_value(v):
                return True
        return False

    if isinstance(value, list):
        for item in value:
            if is_effective_value(item):
                return True
        return False

    return True  # å…¶ä»–ç±»å‹ï¼ˆint/float/boolï¼‰åªè¦ä¸æ˜¯Noneå°±ç®—æœ‰æ•ˆ


def serialize_value(value):
    """é€’å½’å¤„ç†å•ä¸ªå€¼ï¼Œå¤„ç†BaseModel, dict, list, bytes"""
    if not is_effective_value(value):
        return None
    if isinstance(value, BaseModel):
        return serialize_value(value.model_dump())
    if hasattr(value, "dict") and callable(value.dict):
        return serialize_value(value.dict())
    if isinstance(value, dict):
        return {k: serialize_value(v) for k, v in value.items()}
    if isinstance(value, list) or (isinstance(value, Iterable) and not isinstance(value, (str, bytes))):
        return [serialize_value(v) for v in value]
    if isinstance(value, bytes):
        return f"bytes:{base64.b64encode(value).decode('utf-8')}"
    return value


from typing import Any


def remove_none_from_dict(data: Any) -> Any:
    """
    éå† dict/listï¼Œé€’å½’åˆ é™¤ value ä¸º None çš„å­—æ®µ
    """
    if isinstance(data, dict):
        new_dict = {}
        for key, value in data.items():
            if value is None:
                continue
            cleaned_value = remove_none_from_dict(value)
            new_dict[key] = cleaned_value
        return new_dict
    elif isinstance(data, list):
        return [remove_none_from_dict(item) for item in data]
    else:
        return data


def generate_request_id():
    """ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„request_id"""
    return str(uuid.uuid4())


def set_request_id(request_id: str):
    """è®¾ç½®å½“å‰è¯·æ±‚çš„ request_id"""
    _request_id.set(request_id)


class TamarModelClient:
    def __init__(
            self,
            server_address: Optional[str] = None,
            jwt_secret_key: Optional[str] = None,
            jwt_token: Optional[str] = None,
            default_payload: Optional[dict] = None,
            token_expires_in: int = 3600,
            max_retries: Optional[int] = None,  # æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: Optional[float] = None,  # åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    ):
        self.server_address = server_address or os.getenv("MODEL_MANAGER_SERVER_ADDRESS")
        if not self.server_address:
            raise ValueError("Server address must be provided via argument or environment variable.")
        self.default_invoke_timeout = float(os.getenv("MODEL_MANAGER_SERVER_INVOKE_TIMEOUT", 30.0))

        # JWT é…ç½®
        self.jwt_secret_key = jwt_secret_key or os.getenv("MODEL_MANAGER_SERVER_JWT_SECRET_KEY")
        self.jwt_handler = JWTAuthHandler(self.jwt_secret_key)
        self.jwt_token = jwt_token  # ç”¨æˆ·ä¼ å…¥çš„ Tokenï¼ˆå¯é€‰ï¼‰
        self.default_payload = default_payload
        self.token_expires_in = token_expires_in

        # === TLS/Authority é…ç½® ===
        self.use_tls = os.getenv("MODEL_MANAGER_SERVER_GRPC_USE_TLS", "true").lower() == "true"
        self.default_authority = os.getenv("MODEL_MANAGER_SERVER_GRPC_DEFAULT_AUTHORITY")

        # === é‡è¯•é…ç½® ===
        self.max_retries = max_retries if max_retries is not None else int(
            os.getenv("MODEL_MANAGER_SERVER_GRPC_MAX_RETRIES", 3))
        self.retry_delay = retry_delay if retry_delay is not None else float(
            os.getenv("MODEL_MANAGER_SERVER_GRPC_RETRY_DELAY", 1.0))

        # === gRPC é€šé“ç›¸å…³ ===
        self.channel: Optional[grpc.Channel] = None
        self.stub: Optional[model_service_pb2_grpc.ModelServiceStub] = None
        self._closed = False

    def _retry_request(self, func, *args, **kwargs):
        retry_count = 0
        while retry_count < self.max_retries:
            try:
                return func(*args, **kwargs)
            except (grpc.RpcError) as e:
                if e.code() in {grpc.StatusCode.UNAVAILABLE, grpc.StatusCode.DEADLINE_EXCEEDED}:
                    retry_count += 1
                    logger.warning(f"âš ï¸ gRPC error {e.code()}, retrying {retry_count}/{self.max_retries}...",
                                 extra={"log_type": "info", "data": {"retry_count": retry_count, "max_retries": self.max_retries, "error_code": str(e.code())}})
                    if retry_count < self.max_retries:
                        delay = self.retry_delay * (2 ** (retry_count - 1))
                        time.sleep(delay)
                    else:
                        logger.error(f"âŒ Max retry reached for {e.code()}",
                                   extra={"log_type": "info", "data": {"error_code": str(e.code()), "max_retries_reached": True}})
                        raise
                else:
                    logger.error(f"âŒ Non-retryable gRPC error: {e}", exc_info=True,
                               extra={"log_type": "info", "data": {"error_code": str(e.code()) if hasattr(e, 'code') else None, "retryable": False}})
                    raise

    def _build_auth_metadata(self, request_id: str) -> list:
        metadata = [("x-request-id", request_id)]  # å°† request_id æ·»åŠ åˆ° headers
        if self.jwt_handler:
            self.jwt_token = self.jwt_handler.encode_token(self.default_payload, expires_in=self.token_expires_in)
            metadata.append(("authorization", f"Bearer {self.jwt_token}"))
        return metadata

    def _ensure_initialized(self):
        """åˆå§‹åŒ– gRPC é€šé“ï¼Œæ”¯æŒ TLS ä¸é‡è¯•æœºåˆ¶"""
        if self.channel and self.stub:
            return

        retry_count = 0
        options = [
            ('grpc.max_send_message_length', MAX_MESSAGE_LENGTH),
            ('grpc.max_receive_message_length', MAX_MESSAGE_LENGTH),
            ('grpc.keepalive_permit_without_calls', True)  # å³ä½¿æ²¡æœ‰æ´»è·ƒè¯·æ±‚ä¹Ÿä¿æŒè¿æ¥
        ]
        if self.default_authority:
            options.append(("grpc.default_authority", self.default_authority))

        while retry_count <= self.max_retries:
            try:
                if self.use_tls:
                    credentials = grpc.ssl_channel_credentials()
                    self.channel = grpc.secure_channel(
                        self.server_address,
                        credentials,
                        options=options
                    )
                    logger.info("ğŸ” Using secure gRPC channel (TLS enabled)",
                              extra={"log_type": "info", "data": {"tls_enabled": True, "server_address": self.server_address}})
                else:
                    self.channel = grpc.insecure_channel(
                        self.server_address,
                        options=options
                    )
                    logger.info("ğŸ”“ Using insecure gRPC channel (TLS disabled)",
                              extra={"log_type": "info", "data": {"tls_enabled": False, "server_address": self.server_address}})

                # Wait for the channel to be ready (synchronously)
                grpc.channel_ready_future(self.channel).result()  # This is blocking in sync mode

                self.stub = model_service_pb2_grpc.ModelServiceStub(self.channel)
                logger.info(f"âœ… gRPC channel initialized to {self.server_address}",
                          extra={"log_type": "info", "data": {"status": "success", "server_address": self.server_address}})
                return
            except grpc.FutureTimeoutError as e:
                logger.error(f"âŒ gRPC channel initialization timed out: {str(e)}", exc_info=True,
                           extra={"log_type": "info", "data": {"error_type": "timeout", "server_address": self.server_address}})
            except grpc.RpcError as e:
                logger.error(f"âŒ gRPC channel initialization failed: {str(e)}", exc_info=True,
                           extra={"log_type": "info", "data": {"error_type": "rpc_error", "server_address": self.server_address}})
            except Exception as e:
                logger.error(f"âŒ Unexpected error during channel initialization: {str(e)}", exc_info=True,
                           extra={"log_type": "info", "data": {"error_type": "unexpected", "server_address": self.server_address}})

            retry_count += 1
            if retry_count > self.max_retries:
                logger.error(f"âŒ Failed to initialize gRPC channel after {self.max_retries} retries.", exc_info=True,
                           extra={"log_type": "info", "data": {"max_retries_reached": True, "server_address": self.server_address}})
                raise ConnectionError(f"âŒ Failed to initialize gRPC channel after {self.max_retries} retries.")

            # æŒ‡æ•°é€€é¿ï¼šå»¶è¿Ÿæ—¶é—´ = retry_delay * (2 ^ (retry_count - 1))
            delay = self.retry_delay * (2 ** (retry_count - 1))
            logger.warning(f"ğŸ”„ Retrying connection (attempt {retry_count}/{self.max_retries}) after {delay:.2f}s delay...",
                      extra={"log_type": "info", "data": {"retry_count": retry_count, "max_retries": self.max_retries, "delay": delay}})
            time.sleep(delay)  # Blocking sleep in sync version

    def _stream(self, request, metadata, invoke_timeout) -> Iterator[ModelResponse]:
        for response in self.stub.Invoke(request, metadata=metadata, timeout=invoke_timeout):
            yield ModelResponse(
                content=response.content,
                usage=json.loads(response.usage) if response.usage else None,
                error=response.error or None,
                raw_response=json.loads(response.raw_response) if response.raw_response else None,
                request_id=response.request_id if response.request_id else None,
            )
    
    def _stream_with_logging(self, request, metadata, invoke_timeout, start_time, model_request) -> Iterator[ModelResponse]:
        """æµå¼å“åº”çš„åŒ…è£…å™¨ï¼Œç”¨äºè®°å½•å®Œæ•´çš„å“åº”æ—¥å¿—"""
        total_content = ""
        final_usage = None
        error_occurred = None
        chunk_count = 0
        
        try:
            for response in self._stream(request, metadata, invoke_timeout):
                chunk_count += 1
                if response.content:
                    total_content += response.content
                if response.usage:
                    final_usage = response.usage
                if response.error:
                    error_occurred = response.error
                yield response
            
            # æµå¼å“åº”å®Œæˆï¼Œè®°å½•æˆåŠŸæ—¥å¿—
            duration = time.time() - start_time
            logger.info(
                f"âœ… Stream completed successfully | chunks: {chunk_count}",
                extra={
                    "log_type": "response",
                    "uri": f"/invoke/{model_request.provider.value}/{model_request.invoke_type.value}",
                    "duration": duration,
                    "data": {
                        "provider": model_request.provider.value,
                        "invoke_type": model_request.invoke_type.value,
                        "model": model_request.model,
                        "stream": True,
                        "chunks_count": chunk_count,
                        "total_length": len(total_content),
                        "usage": final_usage
                    }
                }
            )
        except Exception as e:
            # æµå¼å“åº”å‡ºé”™ï¼Œè®°å½•é”™è¯¯æ—¥å¿—
            duration = time.time() - start_time
            logger.error(
                f"âŒ Stream failed after {chunk_count} chunks: {str(e)}",
                exc_info=True,
                extra={
                    "log_type": "response",
                    "uri": f"/invoke/{model_request.provider.value}/{model_request.invoke_type.value}",
                    "duration": duration,
                    "data": {
                        "provider": model_request.provider.value,
                        "invoke_type": model_request.invoke_type.value,
                        "model": model_request.model,
                        "stream": True,
                        "chunks_count": chunk_count,
                        "error_type": type(e).__name__,
                        "partial_content_length": len(total_content)
                    }
                }
            )
            raise

    def _invoke_request(self, request, metadata, invoke_timeout):
        response = self.stub.Invoke(request, metadata=metadata, timeout=invoke_timeout)
        for response in response:
            return ModelResponse(
                content=response.content,
                usage=json.loads(response.usage) if response.usage else None,
                error=response.error or None,
                raw_response=json.loads(response.raw_response) if response.raw_response else None,
                request_id=response.request_id if response.request_id else None,
            )

    def invoke(self, model_request: ModelRequest, timeout: Optional[float] = None, request_id: Optional[str] = None) -> \
            Union[ModelResponse, Iterator[ModelResponse]]:
        """
       é€šç”¨è°ƒç”¨æ¨¡å‹æ–¹æ³•ã€‚

        Args:
            model_request: ModelRequest å¯¹è±¡ï¼ŒåŒ…å«è¯·æ±‚å‚æ•°ã€‚
            timeout: Optional[float]
            request_id: Optional[str]
        Yields:
            ModelResponse: æ”¯æŒæµå¼æˆ–éæµå¼çš„æ¨¡å‹å“åº”

        Raises:
            ValidationError: è¾“å…¥éªŒè¯å¤±è´¥ã€‚
            ConnectionError: è¿æ¥æœåŠ¡ç«¯å¤±è´¥ã€‚
        """
        self._ensure_initialized()

        if not self.default_payload:
            self.default_payload = {
                "org_id": model_request.user_context.org_id or "",
                "user_id": model_request.user_context.user_id or ""
            }

        if not request_id:
            request_id = generate_request_id()  # ç”Ÿæˆä¸€ä¸ªæ–°çš„ request_id
        set_request_id(request_id)  # è®¾ç½®å½“å‰è¯·æ±‚çš„ request_id
        metadata = self._build_auth_metadata(request_id)  # å°† request_id åŠ å…¥åˆ°è¯·æ±‚å¤´

        # è®°å½•å¼€å§‹æ—¥å¿—
        start_time = time.time()
        logger.info(
            f"ğŸ”µ Request Start | request_id: {request_id} | provider: {model_request.provider} | invoke_type: {model_request.invoke_type}",
            extra={
                "log_type": "request",
                "uri": f"/invoke/{model_request.provider.value}/{model_request.invoke_type.value}",
                "data": {
                    "provider": model_request.provider.value,
                    "invoke_type": model_request.invoke_type.value,
                    "model": model_request.model,
                    "stream": model_request.stream,
                    "org_id": model_request.user_context.org_id,
                    "user_id": model_request.user_context.user_id,
                    "client_type": model_request.user_context.client_type
                }
            })

        # åŠ¨æ€æ ¹æ® provider/invoke_type å†³å®šä½¿ç”¨å“ªä¸ª input å­—æ®µ
        try:
            # é€‰æ‹©éœ€è¦æ ¡éªŒçš„å­—æ®µé›†åˆ
            # åŠ¨æ€åˆ†æ”¯é€»è¾‘
            match (model_request.provider, model_request.invoke_type):
                case (ProviderType.GOOGLE, InvokeType.GENERATION):
                    allowed_fields = GoogleGenAiInput.model_fields.keys()
                case (ProviderType.GOOGLE, InvokeType.IMAGE_GENERATION):
                    allowed_fields = GoogleVertexAIImagesInput.model_fields.keys()
                case ((ProviderType.OPENAI | ProviderType.AZURE), InvokeType.RESPONSES | InvokeType.GENERATION):
                    allowed_fields = OpenAIResponsesInput.model_fields.keys()
                case ((ProviderType.OPENAI | ProviderType.AZURE), InvokeType.CHAT_COMPLETIONS):
                    allowed_fields = OpenAIChatCompletionsInput.model_fields.keys()
                case ((ProviderType.OPENAI | ProviderType.AZURE), InvokeType.IMAGE_GENERATION):
                    allowed_fields = OpenAIImagesInput.model_fields.keys()
                case ((ProviderType.OPENAI | ProviderType.AZURE), InvokeType.IMAGE_EDIT_GENERATION):
                    allowed_fields = OpenAIImagesEditInput.model_fields.keys()
                case _:
                    raise ValueError(
                        f"Unsupported provider/invoke_type combination: {model_request.provider} + {model_request.invoke_type}")

            # å°† ModelRequest è½¬ dictï¼Œè¿‡æ»¤åªä¿ç•™ base + allowed çš„å­—æ®µ
            model_request_dict = model_request.model_dump(exclude_unset=True)

            grpc_request_kwargs = {}
            for field in allowed_fields:
                if field in model_request_dict:
                    value = model_request_dict[field]

                    # è·³è¿‡æ— æ•ˆçš„å€¼
                    if not is_effective_value(value):
                        continue

                    # åºåˆ—åŒ–grpcä¸æ”¯æŒçš„ç±»å‹
                    grpc_request_kwargs[field] = serialize_value(value)

                    # æ¸…ç† serializeåçš„ grpc_request_kwargs
                    grpc_request_kwargs = remove_none_from_dict(grpc_request_kwargs)

            request = model_service_pb2.ModelRequestItem(
                provider=model_request.provider.value,
                channel=model_request.channel.value,
                invoke_type=model_request.invoke_type.value,
                stream=model_request.stream or False,
                org_id=model_request.user_context.org_id or "",
                user_id=model_request.user_context.user_id or "",
                client_type=model_request.user_context.client_type or "",
                extra=grpc_request_kwargs
            )

        except Exception as e:
            raise ValueError(f"æ„å»ºè¯·æ±‚å¤±è´¥: {str(e)}") from e

        try:
            invoke_timeout = timeout or self.default_invoke_timeout
            if model_request.stream:
                # å¯¹äºæµå¼å“åº”ï¼Œä½¿ç”¨å¸¦æ—¥å¿—è®°å½•çš„åŒ…è£…å™¨
                return self._stream_with_logging(request, metadata, invoke_timeout, start_time, model_request)
            else:
                result = self._retry_request(self._invoke_request, request, metadata, invoke_timeout)
                
                # è®°å½•éæµå¼å“åº”çš„æˆåŠŸæ—¥å¿—
                duration = time.time() - start_time
                logger.info(
                    f"âœ… Request completed successfully",
                    extra={
                        "log_type": "response",
                        "uri": f"/invoke/{model_request.provider.value}/{model_request.invoke_type.value}",
                        "duration": duration,
                        "data": {
                            "provider": model_request.provider.value,
                            "invoke_type": model_request.invoke_type.value,
                            "model": model_request.model,
                            "stream": False,
                            "content_length": len(result.content) if result.content else 0,
                            "usage": result.usage
                        }
                    }
                )
                return result
        except grpc.RpcError as e:
            duration = time.time() - start_time
            error_message = f"âŒ Invoke gRPC failed: {str(e)}"
            logger.error(error_message, exc_info=True,
                       extra={
                           "log_type": "response",
                           "uri": f"/invoke/{model_request.provider.value}/{model_request.invoke_type.value}",
                           "duration": duration,
                           "data": {
                               "error_type": "grpc_error",
                               "error_code": str(e.code()) if hasattr(e, 'code') else None,
                               "provider": model_request.provider.value,
                               "invoke_type": model_request.invoke_type.value,
                               "model": model_request.model
                           }
                       })
            raise e
        except Exception as e:
            duration = time.time() - start_time
            error_message = f"âŒ Invoke other error: {str(e)}"
            logger.error(error_message, exc_info=True,
                       extra={
                           "log_type": "response",
                           "uri": f"/invoke/{model_request.provider.value}/{model_request.invoke_type.value}",
                           "duration": duration,
                           "data": {
                               "error_type": "other_error",
                               "provider": model_request.provider.value,
                               "invoke_type": model_request.invoke_type.value,
                               "model": model_request.model
                           }
                       })
            raise e

    def invoke_batch(self, batch_request_model: BatchModelRequest, timeout: Optional[float] = None,
                     request_id: Optional[str] = None) -> BatchModelResponse:
        """
        æ‰¹é‡æ¨¡å‹è°ƒç”¨æ¥å£

        Args:
            batch_request_model: å¤šæ¡ BatchModelRequest è¾“å…¥
            timeout: è°ƒç”¨è¶…æ—¶ï¼Œå•ä½ç§’
            request_id: è¯·æ±‚id
        Returns:
            BatchModelResponse: æ‰¹é‡è¯·æ±‚çš„ç»“æœ
        """

        self._ensure_initialized()

        if not self.default_payload:
            self.default_payload = {
                "org_id": batch_request_model.user_context.org_id or "",
                "user_id": batch_request_model.user_context.user_id or ""
            }

        if not request_id:
            request_id = generate_request_id()  # ç”Ÿæˆä¸€ä¸ªæ–°çš„ request_id
        set_request_id(request_id)  # è®¾ç½®å½“å‰è¯·æ±‚çš„ request_id
        metadata = self._build_auth_metadata(request_id)  # å°† request_id åŠ å…¥åˆ°è¯·æ±‚å¤´

        # è®°å½•å¼€å§‹æ—¥å¿—
        start_time = time.time()
        logger.info(
            f"ğŸ”µ Batch Request Start | request_id: {request_id} | batch_size: {len(batch_request_model.items)}",
            extra={
                "log_type": "request",
                "uri": "/batch_invoke",
                "data": {
                    "batch_size": len(batch_request_model.items),
                    "org_id": batch_request_model.user_context.org_id,
                    "user_id": batch_request_model.user_context.user_id,
                    "client_type": batch_request_model.user_context.client_type
                }
            })

        # æ„é€ æ‰¹é‡è¯·æ±‚
        items = []
        for model_request_item in batch_request_model.items:
            # åŠ¨æ€æ ¹æ® provider/invoke_type å†³å®šä½¿ç”¨å“ªä¸ª input å­—æ®µ
            try:
                match (model_request_item.provider, model_request_item.invoke_type):
                    case (ProviderType.GOOGLE, InvokeType.GENERATION):
                        allowed_fields = GoogleGenAiInput.model_fields.keys()
                    case (ProviderType.GOOGLE, InvokeType.IMAGE_GENERATION):
                        allowed_fields = GoogleVertexAIImagesInput.model_fields.keys()
                    case ((ProviderType.OPENAI | ProviderType.AZURE), InvokeType.RESPONSES | InvokeType.GENERATION):
                        allowed_fields = OpenAIResponsesInput.model_fields.keys()
                    case ((ProviderType.OPENAI | ProviderType.AZURE), InvokeType.CHAT_COMPLETIONS):
                        allowed_fields = OpenAIChatCompletionsInput.model_fields.keys()
                    case ((ProviderType.OPENAI | ProviderType.AZURE), InvokeType.IMAGE_GENERATION):
                        allowed_fields = OpenAIImagesInput.model_fields.keys()
                    case ((ProviderType.OPENAI | ProviderType.AZURE), InvokeType.IMAGE_EDIT_GENERATION):
                        allowed_fields = OpenAIImagesEditInput.model_fields.keys()
                    case _:
                        raise ValueError(
                            f"Unsupported provider/invoke_type combination: {model_request_item.provider} + {model_request_item.invoke_type}")

                # å°† ModelRequest è½¬ dictï¼Œè¿‡æ»¤åªä¿ç•™ base + allowed çš„å­—æ®µ
                model_request_dict = model_request_item.model_dump(exclude_unset=True)

                grpc_request_kwargs = {}
                for field in allowed_fields:
                    if field in model_request_dict:
                        value = model_request_dict[field]

                        # è·³è¿‡æ— æ•ˆçš„å€¼
                        if not is_effective_value(value):
                            continue

                        # åºåˆ—åŒ–grpcä¸æ”¯æŒçš„ç±»å‹
                        grpc_request_kwargs[field] = serialize_value(value)

                        # æ¸…ç† serializeåçš„ grpc_request_kwargs
                        grpc_request_kwargs = remove_none_from_dict(grpc_request_kwargs)

                items.append(model_service_pb2.ModelRequestItem(
                    provider=model_request_item.provider.value,
                    channel=model_request_item.channel.value,
                    invoke_type=model_request_item.invoke_type.value,
                    stream=model_request_item.stream or False,
                    custom_id=model_request_item.custom_id or "",
                    priority=model_request_item.priority or 1,
                    org_id=batch_request_model.user_context.org_id or "",
                    user_id=batch_request_model.user_context.user_id or "",
                    client_type=batch_request_model.user_context.client_type or "",
                    extra=grpc_request_kwargs,
                ))

            except Exception as e:
                raise ValueError(f"æ„å»ºè¯·æ±‚å¤±è´¥: {str(e)}ï¼Œitem={model_request_item.custom_id}") from e

        try:
            # è¶…æ—¶å¤„ç†é€»è¾‘
            invoke_timeout = timeout or self.default_invoke_timeout

            # è°ƒç”¨ gRPC æ¥å£
            response = self._retry_request(self.stub.BatchInvoke, model_service_pb2.ModelRequest(items=items),
                                           timeout=invoke_timeout, metadata=metadata)

            result = []
            for res_item in response.items:
                result.append(ModelResponse(
                    content=res_item.content,
                    usage=json.loads(res_item.usage) if res_item.usage else None,
                    raw_response=json.loads(res_item.raw_response) if res_item.raw_response else None,
                    error=res_item.error or None,
                    custom_id=res_item.custom_id if res_item.custom_id else None
                ))
            batch_response = BatchModelResponse(
                request_id=response.request_id if response.request_id else None,
                responses=result
            )
            
            # è®°å½•æˆåŠŸæ—¥å¿—
            duration = time.time() - start_time
            logger.info(
                f"âœ… Batch request completed successfully",
                extra={
                    "log_type": "response",
                    "uri": "/batch_invoke",
                    "duration": duration,
                    "data": {
                        "batch_size": len(batch_request_model.items),
                        "responses_count": len(result)
                    }
                }
            )
            return batch_response
        except grpc.RpcError as e:
            duration = time.time() - start_time
            error_message = f"âŒ BatchInvoke gRPC failed: {str(e)}"
            logger.error(error_message, exc_info=True,
                       extra={
                           "log_type": "response",
                           "uri": "/batch_invoke",
                           "duration": duration,
                           "data": {
                               "error_type": "grpc_error",
                               "error_code": str(e.code()) if hasattr(e, 'code') else None,
                               "batch_size": len(batch_request_model.items)
                           }
                       })
            raise e
        except Exception as e:
            duration = time.time() - start_time
            error_message = f"âŒ BatchInvoke other error: {str(e)}"
            logger.error(error_message, exc_info=True,
                       extra={
                           "log_type": "response",
                           "uri": "/batch_invoke",
                           "duration": duration,
                           "data": {
                               "error_type": "other_error",
                               "batch_size": len(batch_request_model.items)
                           }
                       })
            raise e

    def close(self):
        """å…³é—­ gRPC é€šé“"""
        if self.channel and not self._closed:
            self.channel.close()
            self._closed = True
            logger.info("âœ… gRPC channel closed",
                      extra={"log_type": "info", "data": {"status": "success"}})

    def _safe_sync_close(self):
        """è¿›ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­ channelï¼ˆäº‹ä»¶å¾ªç¯å¤„ç†å…¼å®¹ï¼‰"""
        if self.channel and not self._closed:
            try:
                self.close()  # ç›´æ¥è°ƒç”¨å…³é—­æ–¹æ³•
            except Exception as e:
                logger.warning(f"âš ï¸ gRPC channel close failed at exit: {e}",
                           extra={"log_type": "info", "data": {"status": "failed", "error": str(e)}})

    def __enter__(self):
        """åŒæ­¥åˆå§‹åŒ–è¿æ¥"""
        self._ensure_initialized()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """åŒæ­¥å…³é—­è¿æ¥"""
        self.close()
