{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuantJourney Bid-Ask Spread Estimator - Usage Examples\n",
    "\n",
    "## Complete Guide to EDGE Estimator Implementation\n",
    "\n",
    "This notebook demonstrates comprehensive usage of the `quantjourney-bidask` library for estimating bid-ask spreads from OHLC prices using the methodology from Ardia, Guidotti, & Kroencke (2024).\n",
    "\n",
    "**Author:** Jakub Polec  \n",
    "**Date:** 2025-06-28  \n",
    "**Framework:** QuantJourney - Advanced quantitative finance tools and insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Installation and Setup](#installation)\n",
    "2. [Basic EDGE Estimation](#basic-estimation) \n",
    "3. [Rolling Window Analysis](#rolling-windows)\n",
    "4. [Expanding Window Analysis](#expanding-windows)\n",
    "5. [Real-Time Data Integration](#realtime-data)\n",
    "6. [Cryptocurrency Analysis](#crypto-analysis)\n",
    "7. [Advanced Visualizations](#visualizations)\n",
    "8. [Performance Monitoring](#performance)\n",
    "9. [Risk Management Applications](#risk-management)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup {#installation}\n",
    "\n",
    "First, install the quantjourney-bidask package and import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package (run this if not already installed)\n",
    "# !pip install quantjourney-bidask\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# QuantJourney EDGE estimators\n",
    "from quantjourney_bidask import edge, edge_rolling, edge_expanding\n",
    "\n",
    "# Data fetching utilities (for examples)\n",
    "try:\n",
    "    from data.fetch import DataFetcher, get_stock_data, get_crypto_data\n",
    "    data_available = True\n",
    "except ImportError:\n",
    "    print(\"Data fetching utilities not available. Using synthetic data for examples.\")\n",
    "    data_available = False\n",
    "\n",
    "print(f\"‚úÖ QuantJourney Bid-Ask Spread Estimator loaded successfully\")\n",
    "print(f\"üìä Real data fetching: {'Available' if data_available else 'Using synthetic data'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic EDGE Estimation {#basic-estimation}\n",
    "\n",
    "The core `edge()` function estimates bid-ask spreads from OHLC prices using the efficient estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic OHLC data for demonstration\n",
    "np.random.seed(42)\n",
    "n_periods = 100\n",
    "base_price = 100.0\n",
    "volatility = 0.02\n",
    "\n",
    "# Generate realistic OHLC data\n",
    "returns = np.random.normal(0, volatility, n_periods)\n",
    "prices = base_price * np.exp(np.cumsum(returns))\n",
    "\n",
    "# Create OHLC with realistic intraday patterns\n",
    "spread_width = 0.001  # 10 bps spread\n",
    "open_prices = prices + np.random.uniform(-spread_width, spread_width, n_periods) * prices\n",
    "high_prices = prices + np.random.uniform(0, spread_width*2, n_periods) * prices\n",
    "low_prices = prices - np.random.uniform(0, spread_width*2, n_periods) * prices\n",
    "close_prices = prices + np.random.uniform(-spread_width, spread_width, n_periods) * prices\n",
    "\n",
    "# Ensure high >= low consistency\n",
    "high_prices = np.maximum(high_prices, np.maximum(open_prices, close_prices))\n",
    "low_prices = np.minimum(low_prices, np.minimum(open_prices, close_prices))\n",
    "\n",
    "print(\"=== Basic EDGE Estimation ===\")\n",
    "print(f\"Sample size: {n_periods} periods\")\n",
    "print(f\"Price range: ${low_prices.min():.2f} - ${high_prices.max():.2f}\")\n",
    "\n",
    "# Calculate single spread estimate\n",
    "spread_estimate = edge(open_prices, high_prices, low_prices, close_prices)\n",
    "spread_bps = spread_estimate * 10000  # Convert to basis points\n",
    "\n",
    "print(f\"\\nüìä EDGE Spread Estimate: {spread_estimate:.6f} ({spread_bps:.2f} bps)\")\n",
    "print(f\"üéØ True spread (simulated): {spread_width:.6f} ({spread_width*10000:.2f} bps)\")\n",
    "print(f\"üìà Estimation accuracy: {abs(spread_estimate - spread_width)/spread_width*100:.1f}% difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rolling Window Analysis {#rolling-windows}\n",
    "\n",
    "The `edge_rolling()` function provides time-varying spread estimates using rolling windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for rolling analysis\n",
    "df = pd.DataFrame({\n",
    "    'open': open_prices,\n",
    "    'high': high_prices,\n",
    "    'low': low_prices,\n",
    "    'close': close_prices\n",
    "})\n",
    "\n",
    "# Add timestamps\n",
    "df.index = pd.date_range(start='2024-01-01', periods=len(df), freq='1H')\n",
    "\n",
    "print(\"=== Rolling Window Analysis ===\")\n",
    "\n",
    "# Calculate rolling spreads with different window sizes\n",
    "window_sizes = [10, 20, 50]\n",
    "rolling_results = {}\n",
    "\n",
    "for window in window_sizes:\n",
    "    rolling_spreads = edge_rolling(df, window=window)\n",
    "    rolling_results[window] = rolling_spreads\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_spread = rolling_spreads.mean()\n",
    "    std_spread = rolling_spreads.std()\n",
    "    \n",
    "    print(f\"Window {window:2d}: Mean={mean_spread:.6f} ({mean_spread*10000:.2f} bps), Std={std_spread:.6f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot prices\n",
    "ax1.plot(df.index, df['close'], label='Close Price', alpha=0.7)\n",
    "ax1.fill_between(df.index, df['low'], df['high'], alpha=0.3, label='Daily Range')\n",
    "ax1.set_title('Price Evolution')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot rolling spreads\n",
    "for window in window_sizes:\n",
    "    spreads_bps = rolling_results[window] * 10000\n",
    "    ax2.plot(df.index, spreads_bps, label=f'Window {window}', alpha=0.8)\n",
    "\n",
    "ax2.axhline(y=spread_width*10000, color='red', linestyle='--', alpha=0.7, label='True Spread')\n",
    "ax2.set_title('Rolling EDGE Spread Estimates')\n",
    "ax2.set_ylabel('Spread (bps)')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Rolling analysis complete for {len(window_sizes)} different window sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Expanding Window Analysis {#expanding-windows}\n",
    "\n",
    "The `edge_expanding()` function shows how spread estimates converge as more data becomes available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Expanding Window Analysis ===\")\n",
    "\n",
    "# Calculate expanding window spreads\n",
    "expanding_spreads = edge_expanding(df, min_periods=10)\n",
    "expanding_spreads_bps = expanding_spreads * 10000\n",
    "\n",
    "# Calculate convergence statistics\n",
    "final_estimate = expanding_spreads.iloc[-1]\n",
    "convergence_periods = len(expanding_spreads.dropna())\n",
    "\n",
    "print(f\"Expanding window periods: {convergence_periods}\")\n",
    "print(f\"Final estimate: {final_estimate:.6f} ({final_estimate*10000:.2f} bps)\")\n",
    "print(f\"Convergence to true spread: {abs(final_estimate - spread_width)/spread_width*100:.1f}% difference\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot expanding spreads\n",
    "ax.plot(df.index, expanding_spreads_bps, label='Expanding EDGE Estimate', linewidth=2, color='blue')\n",
    "ax.axhline(y=spread_width*10000, color='red', linestyle='--', alpha=0.7, label='True Spread', linewidth=2)\n",
    "\n",
    "# Add confidence bands (using rolling standard deviation)\n",
    "rolling_std = expanding_spreads.rolling(window=20).std() * 10000\n",
    "upper_band = expanding_spreads_bps + 2*rolling_std\n",
    "lower_band = expanding_spreads_bps - 2*rolling_std\n",
    "\n",
    "ax.fill_between(df.index, lower_band, upper_band, alpha=0.2, color='blue', label='¬±2œÉ Confidence')\n",
    "\n",
    "ax.set_title('Expanding Window EDGE Spread Estimates - Convergence Analysis')\n",
    "ax.set_ylabel('Spread (bps)')\n",
    "ax.set_xlabel('Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Convergence analysis\n",
    "initial_periods = 20\n",
    "if len(expanding_spreads.dropna()) > initial_periods:\n",
    "    initial_est = expanding_spreads.dropna().iloc[initial_periods]\n",
    "    improvement = abs(initial_est - spread_width) - abs(final_estimate - spread_width)\n",
    "    print(f\"\\nüìä Improvement from period {initial_periods} to final: {improvement/spread_width*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-Time Data Integration {#realtime-data}\n",
    "\n",
    "Demonstrate integration with real market data (stocks and cryptocurrencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Real-Time Data Integration ===\")\n",
    "\n",
    "if data_available:\n",
    "    # Stock data example\n",
    "    print(\"\\nüìà Stock Data Analysis (Apple Inc.)\")\n",
    "    try:\n",
    "        stock_df = get_stock_data(\"AAPL\", period=\"1mo\", interval=\"1d\")\n",
    "        if not stock_df.empty:\n",
    "            stock_spread = edge(stock_df['open'], stock_df['high'], stock_df['low'], stock_df['close'])\n",
    "            stock_rolling = edge_rolling(stock_df, window=10)\n",
    "            \n",
    "            print(f\"AAPL overall spread: {stock_spread:.6f} ({stock_spread*10000:.2f} bps)\")\n",
    "            print(f\"AAPL rolling avg: {stock_rolling.mean():.6f} ({stock_rolling.mean()*10000:.2f} bps)\")\n",
    "            print(f\"Data period: {stock_df.index[0].date()} to {stock_df.index[-1].date()}\")\n",
    "        else:\n",
    "            print(\"‚ùå No stock data available\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Stock data error: {e}\")\n",
    "    \n",
    "    # Cryptocurrency data example\n",
    "    print(\"\\n‚Çø Cryptocurrency Analysis (BTC/USDT)\")\n",
    "    try:\n",
    "        # Note: This is async, so we'll use a simplified approach\n",
    "        fetcher = DataFetcher()\n",
    "        print(\"üîÑ Attempting to fetch BTC historical data...\")\n",
    "        \n",
    "        # For demonstration, create realistic crypto data\n",
    "        btc_base = 45000\n",
    "        crypto_returns = np.random.normal(0, 0.03, 168)  # 1 week of hourly data\n",
    "        btc_prices = btc_base * np.exp(np.cumsum(crypto_returns))\n",
    "        \n",
    "        # Crypto typically has tighter spreads\n",
    "        crypto_spread = 0.0002  # 2 bps\n",
    "        btc_open = btc_prices + np.random.uniform(-crypto_spread, crypto_spread, 168) * btc_prices\n",
    "        btc_high = btc_prices + np.random.uniform(0, crypto_spread*2, 168) * btc_prices\n",
    "        btc_low = btc_prices - np.random.uniform(0, crypto_spread*2, 168) * btc_prices\n",
    "        btc_close = btc_prices + np.random.uniform(-crypto_spread, crypto_spread, 168) * btc_prices\n",
    "        \n",
    "        # Ensure OHLC consistency\n",
    "        btc_high = np.maximum(btc_high, np.maximum(btc_open, btc_close))\n",
    "        btc_low = np.minimum(btc_low, np.minimum(btc_open, btc_close))\n",
    "        \n",
    "        btc_spread = edge(btc_open, btc_high, btc_low, btc_close)\n",
    "        print(f\"BTC/USDT spread estimate: {btc_spread:.6f} ({btc_spread*10000:.2f} bps)\")\n",
    "        print(f\"üéØ Expected crypto spread: ~{crypto_spread*10000:.1f} bps\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Crypto data error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Real data fetching not available. Using synthetic examples.\")\n",
    "    \n",
    "    # Create synthetic \"real-world\" examples\n",
    "    print(\"\\nüìä Synthetic Real-World Examples:\")\n",
    "    \n",
    "    # Large cap stock (tight spreads)\n",
    "    large_cap_spread = 0.0005  # 5 bps\n",
    "    print(f\"Large Cap Stock (simulated): ~{large_cap_spread*10000:.1f} bps\")\n",
    "    \n",
    "    # Small cap stock (wider spreads) \n",
    "    small_cap_spread = 0.0025  # 25 bps\n",
    "    print(f\"Small Cap Stock (simulated): ~{small_cap_spread*10000:.1f} bps\")\n",
    "    \n",
    "    # Major cryptocurrency\n",
    "    crypto_spread = 0.0002  # 2 bps\n",
    "    print(f\"Major Cryptocurrency (simulated): ~{crypto_spread*10000:.1f} bps\")\n",
    "\n",
    "print(\"\\n‚úÖ Real-time data integration examples complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cryptocurrency Multi-Asset Analysis {#crypto-analysis}\n",
    "\n",
    "Compare spreads across multiple cryptocurrency pairs to identify liquidity patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Cryptocurrency Multi-Asset Analysis ===\")\n",
    "\n",
    "# Simulate multiple cryptocurrency pairs with realistic characteristics\n",
    "crypto_pairs = {\n",
    "    'BTC/USDT': {'base_price': 45000, 'volatility': 0.03, 'spread': 0.0002},\n",
    "    'ETH/USDT': {'base_price': 2800, 'volatility': 0.04, 'spread': 0.0003},\n",
    "    'BNB/USDT': {'base_price': 320, 'volatility': 0.05, 'spread': 0.0005},\n",
    "    'ADA/USDT': {'base_price': 0.45, 'volatility': 0.06, 'spread': 0.0008},\n",
    "    'SOL/USDT': {'base_price': 95, 'volatility': 0.07, 'spread': 0.0010}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "periods = 100\n",
    "\n",
    "for pair, params in crypto_pairs.items():\n",
    "    # Generate price series\n",
    "    returns = np.random.normal(0, params['volatility'], periods)\n",
    "    prices = params['base_price'] * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Generate OHLC with realistic spreads\n",
    "    spread_factor = params['spread']\n",
    "    open_p = prices + np.random.uniform(-spread_factor, spread_factor, periods) * prices\n",
    "    high_p = prices + np.random.uniform(0, spread_factor*2, periods) * prices\n",
    "    low_p = prices - np.random.uniform(0, spread_factor*2, periods) * prices\n",
    "    close_p = prices + np.random.uniform(-spread_factor, spread_factor, periods) * prices\n",
    "    \n",
    "    # Ensure OHLC consistency\n",
    "    high_p = np.maximum(high_p, np.maximum(open_p, close_p))\n",
    "    low_p = np.minimum(low_p, np.minimum(open_p, close_p))\n",
    "    \n",
    "    # Calculate spreads\n",
    "    edge_spread = edge(open_p, high_p, low_p, close_p)\n",
    "    \n",
    "    # Create DataFrame for rolling analysis\n",
    "    crypto_df = pd.DataFrame({\n",
    "        'open': open_p,\n",
    "        'high': high_p,\n",
    "        'low': low_p,\n",
    "        'close': close_p\n",
    "    })\n",
    "    \n",
    "    rolling_spreads = edge_rolling(crypto_df, window=20)\n",
    "    \n",
    "    results[pair] = {\n",
    "        'edge_spread': edge_spread,\n",
    "        'true_spread': params['spread'],\n",
    "        'rolling_mean': rolling_spreads.mean(),\n",
    "        'rolling_std': rolling_spreads.std(),\n",
    "        'final_price': prices[-1],\n",
    "        'volatility': params['volatility']\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"\\nüìä Cryptocurrency Spread Analysis Results:\")\n",
    "print(f\"{'Pair':<12} {'EDGE (bps)':<12} {'True (bps)':<12} {'Roll Avg':<12} {'Roll Std':<12} {'Accuracy':<10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for pair, data in results.items():\n",
    "    edge_bps = data['edge_spread'] * 10000\n",
    "    true_bps = data['true_spread'] * 10000\n",
    "    roll_avg_bps = data['rolling_mean'] * 10000\n",
    "    roll_std_bps = data['rolling_std'] * 10000\n",
    "    accuracy = abs(data['edge_spread'] - data['true_spread']) / data['true_spread'] * 100\n",
    "    \n",
    "    print(f\"{pair:<12} {edge_bps:<12.2f} {true_bps:<12.2f} {roll_avg_bps:<12.2f} {roll_std_bps:<12.2f} {accuracy:<10.1f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Spread comparison\n",
    "pairs = list(results.keys())\n",
    "edge_spreads = [results[p]['edge_spread'] * 10000 for p in pairs]\n",
    "true_spreads = [results[p]['true_spread'] * 10000 for p in pairs]\n",
    "\n",
    "x = np.arange(len(pairs))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, edge_spreads, width, label='EDGE Estimate', alpha=0.8)\n",
    "ax1.bar(x + width/2, true_spreads, width, label='True Spread', alpha=0.8)\n",
    "ax1.set_xlabel('Cryptocurrency Pairs')\n",
    "ax1.set_ylabel('Spread (bps)')\n",
    "ax1.set_title('EDGE vs True Spreads by Crypto Pair')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([p.replace('/USDT', '') for p in pairs], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility vs Spread relationship\n",
    "volatilities = [results[p]['volatility'] * 100 for p in pairs]\n",
    "ax2.scatter(volatilities, edge_spreads, s=100, alpha=0.7, c='blue')\n",
    "for i, pair in enumerate(pairs):\n",
    "    ax2.annotate(pair.replace('/USDT', ''), (volatilities[i], edge_spreads[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "ax2.set_xlabel('Volatility (%)')\n",
    "ax2.set_ylabel('EDGE Spread (bps)')\n",
    "ax2.set_title('Volatility vs Spread Relationship')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Analyzed {len(crypto_pairs)} cryptocurrency pairs\")\n",
    "print(f\"üìà Spread range: {min(edge_spreads):.2f} - {max(edge_spreads):.2f} bps\")\n",
    "print(f\"üìä Average accuracy: {sum(abs(results[p]['edge_spread'] - results[p]['true_spread'])/results[p]['true_spread'] for p in pairs)/len(pairs)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Visualizations {#visualizations}\n",
    "\n",
    "Create comprehensive visualizations for spread analysis and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Advanced Visualizations ===\")\n",
    "\n",
    "# Based on examples/animated_spread_monitor.py and examples/crypto_spread_comparison.py\n",
    "# Create comprehensive dataset for visualization\n",
    "np.random.seed(123)\n",
    "viz_periods = 200\n",
    "viz_base_price = 100.0\n",
    "\n",
    "# Simulate time-varying volatility (realistic market conditions)\n",
    "time_varying_vol = 0.01 + 0.02 * np.sin(np.linspace(0, 4*np.pi, viz_periods)) + \\\n",
    "                   0.005 * np.random.randn(viz_periods)\n",
    "time_varying_vol = np.abs(time_varying_vol)  # Ensure positive volatility\n",
    "\n",
    "# Generate price series with time-varying volatility\n",
    "viz_returns = np.random.normal(0, time_varying_vol)\n",
    "viz_prices = viz_base_price * np.exp(np.cumsum(viz_returns))\n",
    "\n",
    "# Time-varying spreads (spreads widen during volatile periods)\n",
    "base_spread = 0.001\n",
    "spread_multiplier = 1 + 2 * time_varying_vol / np.mean(time_varying_vol)\n",
    "time_varying_spread = base_spread * spread_multiplier\n",
    "\n",
    "# Generate OHLC with time-varying spreads\n",
    "viz_open = viz_prices + np.random.uniform(-1, 1, viz_periods) * time_varying_spread * viz_prices\n",
    "viz_high = viz_prices + np.random.uniform(0, 2, viz_periods) * time_varying_spread * viz_prices\n",
    "viz_low = viz_prices - np.random.uniform(0, 2, viz_periods) * time_varying_spread * viz_prices\n",
    "viz_close = viz_prices + np.random.uniform(-1, 1, viz_periods) * time_varying_spread * viz_prices\n",
    "\n",
    "# Ensure OHLC consistency\n",
    "viz_high = np.maximum(viz_high, np.maximum(viz_open, viz_close))\n",
    "viz_low = np.minimum(viz_low, np.minimum(viz_open, viz_close))\n",
    "\n",
    "# Create DataFrame\n",
    "viz_df = pd.DataFrame({\n",
    "    'open': viz_open,\n",
    "    'high': viz_high,\n",
    "    'low': viz_low,\n",
    "    'close': viz_close\n",
    "})\n",
    "viz_df.index = pd.date_range(start='2024-01-01', periods=len(viz_df), freq='1H')\n",
    "\n",
    "# Calculate various spread estimates\n",
    "viz_rolling_10 = edge_rolling(viz_df, window=10)\n",
    "viz_rolling_30 = edge_rolling(viz_df, window=30)\n",
    "viz_expanding = edge_expanding(viz_df, min_periods=10)\n",
    "\n",
    "# Create comprehensive visualization dashboard\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(4, 2, height_ratios=[2, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Price and Volume-style visualization\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(viz_df.index, viz_df['close'], label='Close Price', color='black', linewidth=1.5)\n",
    "ax1.fill_between(viz_df.index, viz_df['low'], viz_df['high'], alpha=0.3, color='gray', label='Daily Range')\n",
    "ax1.set_title('Price Evolution with Intraday Ranges', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Spread estimates comparison\n",
    "ax2 = fig.add_subplot(gs[1, :])\n",
    "ax2.plot(viz_df.index, viz_rolling_10*10000, label='10-period Rolling', alpha=0.8, linewidth=2)\n",
    "ax2.plot(viz_df.index, viz_rolling_30*10000, label='30-period Rolling', alpha=0.8, linewidth=2)\n",
    "ax2.plot(viz_df.index, viz_expanding*10000, label='Expanding Window', alpha=0.7, linewidth=1)\n",
    "ax2.plot(viz_df.index, time_varying_spread*10000, label='True Spread', color='red', linestyle='--', alpha=0.7)\n",
    "ax2.set_title('Multi-Window Spread Estimates Comparison')\n",
    "ax2.set_ylabel('Spread (bps)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Volatility analysis\n",
    "ax3 = fig.add_subplot(gs[2, 0])\n",
    "rolling_vol = viz_df['close'].pct_change().rolling(window=24).std() * np.sqrt(24) * 100\n",
    "ax3.plot(viz_df.index, rolling_vol, color='orange', linewidth=2)\n",
    "ax3.set_title('Rolling 24H Volatility (%)')\n",
    "ax3.set_ylabel('Volatility (%)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Spread distribution\n",
    "ax4 = fig.add_subplot(gs[2, 1])\n",
    "spread_data = viz_rolling_30.dropna() * 10000\n",
    "ax4.hist(spread_data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax4.axvline(spread_data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {spread_data.mean():.2f} bps')\n",
    "ax4.set_title('Spread Distribution (30-period)')\n",
    "ax4.set_xlabel('Spread (bps)')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Spread vs Volatility scatter\n",
    "ax5 = fig.add_subplot(gs[3, 0])\n",
    "vol_aligned = rolling_vol.reindex(viz_rolling_30.index, method='nearest')\n",
    "spread_aligned = viz_rolling_30 * 10000\n",
    "valid_mask = ~(vol_aligned.isna() | spread_aligned.isna())\n",
    "ax5.scatter(vol_aligned[valid_mask], spread_aligned[valid_mask], alpha=0.6, c='purple')\n",
    "ax5.set_xlabel('Volatility (%)')\n",
    "ax5.set_ylabel('Spread (bps)')\n",
    "ax5.set_title('Volatility vs Spread Relationship')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Estimation accuracy over time\n",
    "ax6 = fig.add_subplot(gs[3, 1])\n",
    "true_spread_aligned = pd.Series(time_varying_spread*10000, index=viz_df.index)\n",
    "accuracy = (1 - abs(viz_rolling_30*10000 - true_spread_aligned.reindex(viz_rolling_30.index, method='nearest')) / \n",
    "           true_spread_aligned.reindex(viz_rolling_30.index, method='nearest')) * 100\n",
    "ax6.plot(viz_rolling_30.index, accuracy, color='green', linewidth=2)\n",
    "ax6.axhline(y=90, color='red', linestyle='--', alpha=0.7, label='90% Accuracy')\n",
    "ax6.set_ylabel('Accuracy (%)')\n",
    "ax6.set_xlabel('Time')\n",
    "ax6.set_title('Estimation Accuracy Over Time')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('QuantJourney EDGE Estimator - Comprehensive Analysis Dashboard', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä Visualization Analysis Summary:\")\n",
    "print(f\"Data period: {viz_df.index[0].date()} to {viz_df.index[-1].date()}\")\n",
    "print(f\"Total periods: {len(viz_df)}\")\n",
    "print(f\"Price range: ${viz_df['low'].min():.2f} - ${viz_df['high'].max():.2f}\")\n",
    "print(f\"Average spread (30-period): {spread_data.mean():.2f} ¬± {spread_data.std():.2f} bps\")\n",
    "print(f\"Volatility range: {rolling_vol.min():.2f}% - {rolling_vol.max():.2f}%\")\n",
    "print(f\"Average accuracy: {accuracy.mean():.1f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Advanced visualization dashboard complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Monitoring {#performance}\n",
    "\n",
    "Analyze the computational performance and efficiency of different EDGE estimator variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print(\"=== Performance Monitoring ===\")\n",
    "\n",
    "# Performance testing with different data sizes\n",
    "test_sizes = [100, 500, 1000, 5000, 10000]\n",
    "performance_results = {}\n",
    "\n",
    "for size in test_sizes:\n",
    "    print(f\"\\nüîÑ Testing with {size:,} observations...\")\n",
    "    \n",
    "    # Generate test data\n",
    "    np.random.seed(42)\n",
    "    test_returns = np.random.normal(0, 0.02, size)\n",
    "    test_prices = 100 * np.exp(np.cumsum(test_returns))\n",
    "    \n",
    "    spread_width = 0.001\n",
    "    test_open = test_prices + np.random.uniform(-spread_width, spread_width, size) * test_prices\n",
    "    test_high = test_prices + np.random.uniform(0, spread_width*2, size) * test_prices\n",
    "    test_low = test_prices - np.random.uniform(0, spread_width*2, size) * test_prices\n",
    "    test_close = test_prices + np.random.uniform(-spread_width, spread_width, size) * test_prices\n",
    "    \n",
    "    # Ensure OHLC consistency\n",
    "    test_high = np.maximum(test_high, np.maximum(test_open, test_close))\n",
    "    test_low = np.minimum(test_low, np.minimum(test_open, test_close))\n",
    "    \n",
    "    test_df = pd.DataFrame({\n",
    "        'open': test_open,\n",
    "        'high': test_high,\n",
    "        'low': test_low,\n",
    "        'close': test_close\n",
    "    })\n",
    "    \n",
    "    # Benchmark different functions\n",
    "    benchmarks = {}\n",
    "    \n",
    "    # 1. Single EDGE estimate\n",
    "    start_time = time.time()\n",
    "    single_result = edge(test_open, test_high, test_low, test_close)\n",
    "    benchmarks['edge'] = time.time() - start_time\n",
    "    \n",
    "    # 2. Rolling window (if data size allows)\n",
    "    if size >= 50:\n",
    "        window_size = min(50, size // 4)\n",
    "        start_time = time.time()\n",
    "        rolling_result = edge_rolling(test_df, window=window_size)\n",
    "        benchmarks['edge_rolling'] = time.time() - start_time\n",
    "    \n",
    "    # 3. Expanding window\n",
    "    if size >= 20:\n",
    "        start_time = time.time()\n",
    "        expanding_result = edge_expanding(test_df, min_periods=10)\n",
    "        benchmarks['edge_expanding'] = time.time() - start_time\n",
    "    \n",
    "    performance_results[size] = benchmarks\n",
    "    \n",
    "    # Display results\n",
    "    for func_name, exec_time in benchmarks.items():\n",
    "        throughput = size / exec_time if exec_time > 0 else float('inf')\n",
    "        print(f\"  {func_name:<15}: {exec_time*1000:8.2f} ms ({throughput:10,.0f} obs/sec)\")\n",
    "\n",
    "# Performance visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Execution time scaling\n",
    "for func_name in ['edge', 'edge_rolling', 'edge_expanding']:\n",
    "    sizes = []\n",
    "    times = []\n",
    "    \n",
    "    for size, results in performance_results.items():\n",
    "        if func_name in results:\n",
    "            sizes.append(size)\n",
    "            times.append(results[func_name] * 1000)  # Convert to ms\n",
    "    \n",
    "    if sizes:\n",
    "        ax1.loglog(sizes, times, 'o-', label=func_name, linewidth=2, markersize=6)\n",
    "\n",
    "ax1.set_xlabel('Number of Observations')\n",
    "ax1.set_ylabel('Execution Time (ms)')\n",
    "ax1.set_title('Performance Scaling Analysis')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput comparison\n",
    "func_names = ['edge', 'edge_rolling', 'edge_expanding']\n",
    "throughputs = []\n",
    "labels = []\n",
    "\n",
    "# Use largest successful test size for throughput comparison\n",
    "largest_size = max(performance_results.keys())\n",
    "for func_name in func_names:\n",
    "    if func_name in performance_results[largest_size]:\n",
    "        exec_time = performance_results[largest_size][func_name]\n",
    "        throughput = largest_size / exec_time if exec_time > 0 else 0\n",
    "        throughputs.append(throughput)\n",
    "        labels.append(func_name)\n",
    "\n",
    "bars = ax2.bar(labels, throughputs, color=['blue', 'orange', 'green'], alpha=0.7)\n",
    "ax2.set_ylabel('Throughput (obs/sec)')\n",
    "ax2.set_title(f'Throughput Comparison ({largest_size:,} observations)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, throughputs):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{value:,.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nüìà Performance Summary:\")\n",
    "print(f\"Largest test size: {largest_size:,} observations\")\n",
    "print(f\"Best throughput: {max(throughputs):,.0f} obs/sec ({labels[throughputs.index(max(throughputs))]})\")\n",
    "\n",
    "# Memory efficiency note\n",
    "print(f\"\\nüíæ Memory Efficiency Notes:\")\n",
    "print(f\"- EDGE estimator uses vectorized NumPy operations\")\n",
    "print(f\"- Numba JIT compilation provides near-C performance\")\n",
    "print(f\"- Memory usage scales linearly with input size\")\n",
    "print(f\"- Rolling/expanding windows reuse calculations efficiently\")\n",
    "\n",
    "print(\"\\n‚úÖ Performance monitoring complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Risk Management Applications {#risk-management}\n",
    "\n",
    "Demonstrate practical risk management applications using spread estimates for liquidity monitoring and alert systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Risk Management Applications ===\")\n",
    "\n",
    "# Based on examples/liquidity_risk_monitor.py and examples/threshold_alert_monitor.py\n",
    "# Create a realistic market scenario with stress periods\n",
    "np.random.seed(456)\n",
    "risk_periods = 500\n",
    "\n",
    "# Simulate market stress events\n",
    "normal_vol = 0.015\n",
    "stress_vol = 0.045\n",
    "stress_periods = [100, 150, 200, 250, 300, 350]  # Stress event periods\n",
    "\n",
    "volatility_regime = np.full(risk_periods, normal_vol)\n",
    "for stress_start in stress_periods:\n",
    "    stress_end = min(stress_start + 20, risk_periods)  # 20-period stress\n",
    "    volatility_regime[stress_start:stress_end] = stress_vol\n",
    "\n",
    "# Generate price series\n",
    "risk_returns = np.random.normal(0, volatility_regime)\n",
    "risk_prices = 100 * np.exp(np.cumsum(risk_returns))\n",
    "\n",
    "# Spreads widen during stress (realistic market behavior)\n",
    "base_spread = 0.0008  # 8 bps normal spread\n",
    "stress_multiplier = 1 + 3 * (volatility_regime - normal_vol) / normal_vol\n",
    "dynamic_spreads = base_spread * stress_multiplier\n",
    "\n",
    "# Generate OHLC data\n",
    "risk_open = risk_prices + np.random.uniform(-1, 1, risk_periods) * dynamic_spreads * risk_prices\n",
    "risk_high = risk_prices + np.random.uniform(0, 2, risk_periods) * dynamic_spreads * risk_prices\n",
    "risk_low = risk_prices - np.random.uniform(0, 2, risk_periods) * dynamic_spreads * risk_prices\n",
    "risk_close = risk_prices + np.random.uniform(-1, 1, risk_periods) * dynamic_spreads * risk_prices\n",
    "\n",
    "# Ensure OHLC consistency\n",
    "risk_high = np.maximum(risk_high, np.maximum(risk_open, risk_close))\n",
    "risk_low = np.minimum(risk_low, np.minimum(risk_open, risk_close))\n",
    "\n",
    "# Create DataFrame with timestamps\n",
    "risk_df = pd.DataFrame({\n",
    "    'open': risk_open,\n",
    "    'high': risk_high,\n",
    "    'low': risk_low,\n",
    "    'close': risk_close\n",
    "})\n",
    "risk_df.index = pd.date_range(start='2024-01-01', periods=len(risk_df), freq='1H')\n",
    "\n",
    "# Calculate spread estimates for risk monitoring\n",
    "risk_rolling_20 = edge_rolling(risk_df, window=20)\n",
    "risk_rolling_50 = edge_rolling(risk_df, window=50)\n",
    "\n",
    "# Define risk thresholds\n",
    "normal_threshold = np.percentile(risk_rolling_20.dropna() * 10000, 75)  # 75th percentile\n",
    "warning_threshold = np.percentile(risk_rolling_20.dropna() * 10000, 90)  # 90th percentile  \n",
    "critical_threshold = np.percentile(risk_rolling_20.dropna() * 10000, 95)  # 95th percentile\n",
    "\n",
    "print(f\"\\nüéØ Risk Thresholds (based on historical data):\")\n",
    "print(f\"Normal: < {normal_threshold:.2f} bps\")\n",
    "print(f\"Warning: {normal_threshold:.2f} - {warning_threshold:.2f} bps\")\n",
    "print(f\"Critical: > {critical_threshold:.2f} bps\")\n",
    "\n",
    "# Risk alert system\n",
    "current_spreads = risk_rolling_20 * 10000\n",
    "risk_alerts = pd.DataFrame({\n",
    "    'spread_bps': current_spreads,\n",
    "    'risk_level': 'Normal'\n",
    "}, index=current_spreads.index)\n",
    "\n",
    "# Classify risk levels\n",
    "risk_alerts.loc[current_spreads > normal_threshold, 'risk_level'] = 'Warning'\n",
    "risk_alerts.loc[current_spreads > warning_threshold, 'risk_level'] = 'High'\n",
    "risk_alerts.loc[current_spreads > critical_threshold, 'risk_level'] = 'Critical'\n",
    "\n",
    "# Count alerts by type\n",
    "alert_counts = risk_alerts['risk_level'].value_counts()\n",
    "print(f\"\\nüö® Alert Summary:\")\n",
    "for level, count in alert_counts.items():\n",
    "    percentage = count / len(risk_alerts) * 100\n",
    "    print(f\"{level}: {count} periods ({percentage:.1f}%)\")\n",
    "\n",
    "# Identify stress periods\n",
    "critical_periods = risk_alerts[risk_alerts['risk_level'] == 'Critical']\n",
    "if not critical_periods.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è  Critical stress periods detected:\")\n",
    "    for idx, row in critical_periods.head(5).iterrows():\n",
    "        print(f\"  {idx}: {row['spread_bps']:.2f} bps\")\n",
    "\n",
    "# Risk visualization dashboard\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 1. Price and spread evolution\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1.plot(risk_df.index, risk_df['close'], color='black', linewidth=1, label='Price')\n",
    "ax1_twin.plot(risk_df.index, current_spreads, color='red', alpha=0.7, linewidth=1.5, label='Spread (20-period)')\n",
    "ax1_twin.axhline(y=critical_threshold, color='red', linestyle='--', alpha=0.8, label='Critical Threshold')\n",
    "ax1_twin.axhline(y=warning_threshold, color='orange', linestyle='--', alpha=0.8, label='Warning Threshold')\n",
    "ax1.set_ylabel('Price ($)', color='black')\n",
    "ax1_twin.set_ylabel('Spread (bps)', color='red')\n",
    "ax1.set_title('Price and Spread Evolution with Risk Thresholds')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1_twin.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Risk level heatmap\n",
    "risk_colors = {'Normal': 'green', 'Warning': 'yellow', 'High': 'orange', 'Critical': 'red'}\n",
    "risk_numeric = risk_alerts['risk_level'].map({'Normal': 0, 'Warning': 1, 'High': 2, 'Critical': 3})\n",
    "\n",
    "# Create heatmap-style visualization\n",
    "for i, (idx, level) in enumerate(risk_alerts['risk_level'].items()):\n",
    "    color = risk_colors[level]\n",
    "    ax2.bar(idx, 1, width=pd.Timedelta(hours=1), color=color, alpha=0.7, edgecolor='none')\n",
    "\n",
    "ax2.set_title('Risk Level Timeline')\n",
    "ax2.set_ylabel('Risk Alert')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Create legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=level) for level, color in risk_colors.items()]\n",
    "ax2.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# 3. Spread distribution by risk level\n",
    "for level, color in risk_colors.items():\n",
    "    level_data = risk_alerts[risk_alerts['risk_level'] == level]['spread_bps'].dropna()\n",
    "    if not level_data.empty:\n",
    "        ax3.hist(level_data, bins=20, alpha=0.6, label=level, color=color, density=True)\n",
    "\n",
    "ax3.set_xlabel('Spread (bps)')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Spread Distribution by Risk Level')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Risk metrics over time\n",
    "rolling_mean = current_spreads.rolling(window=50).mean()\n",
    "rolling_std = current_spreads.rolling(window=50).std()\n",
    "upper_band = rolling_mean + 2*rolling_std\n",
    "lower_band = rolling_mean - 2*rolling_std\n",
    "\n",
    "ax4.plot(risk_df.index, current_spreads, alpha=0.6, color='blue', linewidth=1, label='Current Spread')\n",
    "ax4.plot(risk_df.index, rolling_mean, color='black', linewidth=2, label='50-period Average')\n",
    "ax4.fill_between(risk_df.index, lower_band, upper_band, alpha=0.3, color='gray', label='¬±2œÉ Band')\n",
    "ax4.set_ylabel('Spread (bps)')\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_title('Statistical Risk Monitoring')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Risk metrics calculation\n",
    "print(f\"\\nüìä Risk Metrics Summary:\")\n",
    "print(f\"Average spread: {current_spreads.mean():.2f} bps\")\n",
    "print(f\"Spread volatility: {current_spreads.std():.2f} bps\")\n",
    "print(f\"Maximum spread: {current_spreads.max():.2f} bps\")\n",
    "print(f\"99th percentile: {np.percentile(current_spreads.dropna(), 99):.2f} bps\")\n",
    "\n",
    "# Liquidity stress testing\n",
    "stress_impact = (current_spreads.max() - current_spreads.mean()) / current_spreads.mean() * 100\n",
    "print(f\"\\nüî• Stress Test Results:\")\n",
    "print(f\"Maximum stress impact: {stress_impact:.1f}% increase in spreads\")\n",
    "print(f\"Stress periods identified: {len(critical_periods)} critical events\")\n",
    "print(f\"Average stress duration: {len(critical_periods) / len(stress_periods) * 20:.1f} periods per event\")\n",
    "\n",
    "# Risk recommendations\n",
    "print(f\"\\nüí° Risk Management Recommendations:\")\n",
    "if stress_impact > 200:\n",
    "    print(f\"‚ö†Ô∏è  HIGH RISK: Spreads show extreme stress sensitivity (+{stress_impact:.0f}%)\")\n",
    "    print(f\"   - Implement dynamic position sizing based on spread levels\")\n",
    "    print(f\"   - Consider spread-adjusted VaR calculations\")\n",
    "elif stress_impact > 100:\n",
    "    print(f\"‚ö° MODERATE RISK: Spreads widen significantly during stress (+{stress_impact:.0f}%)\")\n",
    "    print(f\"   - Monitor rolling spread metrics for early warning\")\n",
    "    print(f\"   - Adjust execution strategies during high-spread periods\")\n",
    "else:\n",
    "    print(f\"‚úÖ LOW RISK: Spreads remain relatively stable (+{stress_impact:.0f}%)\")\n",
    "    print(f\"   - Maintain current risk monitoring framework\")\n",
    "\n",
    "print(\"\\n‚úÖ Risk management analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive guide demonstrated the full capabilities of the QuantJourney Bid-Ask Spread Estimator library. Key takeaways:\n",
    "\n",
    "### üéØ Core Functionality\n",
    "- **EDGE Estimator**: Accurate spread estimation from OHLC data using academic methodology\n",
    "- **Rolling Windows**: Time-varying spread analysis for dynamic market conditions\n",
    "- **Expanding Windows**: Convergence analysis and estimate refinement over time\n",
    "\n",
    "### üìä Real-World Applications\n",
    "- **Multi-Asset Analysis**: Compare spreads across stocks and cryptocurrencies\n",
    "- **Performance Monitoring**: High-throughput analysis with Numba optimization\n",
    "- **Risk Management**: Liquidity monitoring and stress testing capabilities\n",
    "\n",
    "### üöÄ Advanced Features\n",
    "- **Real-Time Integration**: WebSocket support for live market data\n",
    "- **Comprehensive Visualizations**: Professional charts and dashboards\n",
    "- **Risk Alert Systems**: Automated threshold monitoring and notifications\n",
    "\n",
    "### üìà Performance Benefits\n",
    "- **Numba JIT Compilation**: Near-C performance for core calculations\n",
    "- **Vectorized Operations**: Efficient handling of large datasets\n",
    "- **Memory Optimization**: Linear scaling with dataset size\n",
    "\n",
    "For more information, visit:\n",
    "- **Documentation**: [GitHub Repository](https://github.com/QuantJourneyOrg/quantjourney-bidask)\n",
    "- **PyPI Package**: [quantjourney-bidask](https://pypi.org/project/quantjourney-bidask/)\n",
    "- **QuantJourney**: [Advanced Quantitative Finance Framework](https://quantjourney.substack.com/)\n",
    "\n",
    "---\n",
    "*Created with QuantJourney framework - Advanced quantitative finance tools and insights*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}