{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahpo_train.learner  import *\n",
    "from yahpo_train.metrics import *\n",
    "from yahpo_train.cont_scalers import *\n",
    "from yahpo_gym.benchmarks import lcbench, rbv2, nb301, fcnet, taskset, fcnet\n",
    "from yahpo_gym.configuration import cfg\n",
    "from fastai.callback.wandb import *\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a surrogate:\n",
    "\n",
    "We provide a function `fit_config` that allows for training a surrogate with a set of hyperparameters and the option to export the surrogate (this can overwrite existing surrogates!).\n",
    "\n",
    "A particularity is that we use a set of so called `ContTransformers` in order to transfer continuous variables to a scale better suited for optimization! \n",
    "This has a strong effect on the resulting performance and should therefore be optimized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_config(key, embds_dbl=None, embds_tgt=None, tfms=None, lr = 1e-4, epochs=25, frac=0.1, bs=2048, export=False):\n",
    "    \"\"\"\n",
    "    Fit function with hyperparameters\n",
    "    \"\"\"\n",
    "    cc = cfg(key)\n",
    "    dls = dl_from_config(cc, bs=bs, frac=frac)\n",
    "\n",
    "    # Construct embds from transforms. tfms overwrites emdbs_dbl, embds_tgt\n",
    "    if tfms is not None:\n",
    "        embds_dbl = [tfms.get(name) if tfms.get(name) is not None else ContTransformerNone for name, cont in dls.all_cols[dls.cont_names].iteritems()]\n",
    "        embds_tgt = [tfms.get(name) if tfms.get(name) is not None else ContTransformerNone for name, cont in dls.ys.iteritems()]\n",
    "\n",
    "    # Instantiate learner\n",
    "    f = ResNet(dls, embds_dbl=embds_dbl, embds_tgt=embds_tgt)\n",
    "    l = SurrogateTabularLearner(dls, f, loss_func=nn.MSELoss(reduction='mean'), metrics=nn.MSELoss)\n",
    "    l.metrics = [AvgTfedMetric(mae),  AvgTfedMetric(r2), AvgTfedMetric(spearman)]\n",
    "    l.add_cb(MixHandler)\n",
    "    l.add_cb(EarlyStoppingCallback(patience=3))\n",
    "\n",
    "    # Fit\n",
    "    l.fit_flat_cos(epochs, lr)\n",
    "\n",
    "    if export:\n",
    "        l.export_onnx(cc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: NasBench 301\n",
    "Find an example for training the `NASBENCH 301` surrogate below:\n",
    "\n",
    "We supply a list of `ContTransformer`'s to our `fit_config` function that define the specific transformers that should be applied for this scenario:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_nb301(key = 'nb301', **kwargs):\n",
    "    embds_dbl = [partial(ContTransformerMultScalar, m = 1/52)]\n",
    "    embds_tgt = [partial(ContTransformerMultScalar, m = 1/100), ContTransformerRange]\n",
    "    fit_config(key, embds_dbl=embds_dbl, embds_tgt=embds_tgt, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: rbv2_super\n",
    "\n",
    "A more involved example is the `rbv2_super` surrogate, where multiple different transformers are used depending on the input and output variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_rbv2_super(key = 'rbv2_super', **kwargs):\n",
    "    # Transforms\n",
    "    tfms = {}\n",
    "    [tfms.update({k:ContTransformerRange}) for k in [\"mmce\", \"f1\", \"auc\", \"aknn.k\", \"aknn.M\", \"rpart.maxdepth\", \"rpart.minsplit\", \"rpart.minbucket\", \"xgboost.max_depth\"]]\n",
    "    [tfms.update({k:partial(ContTransformerLogRange)}) for k in [\"timetrain\", \"timepredict\", \"svm.cost\", \"svm.gamma\"]]\n",
    "    [tfms.update({k:partial(ContTransformerLogRange, logfun=torch.log2,  expfun=torch.exp2 )}) for k in [\"glmnet.s\", \"rpart.cp\", \"aknn.ef\", \"aknn.ef_construction\", \"xgboost.nrounds\", \"xgboost.eta\", \"xgboost.gamma\", \"xgboost.lambda\", \"xgboost.alpha\", \"xgboost.min_child_weight\", \"ranger.num.trees\", \"ranger.min.node.size\", 'ranger.num.random.splits']]\n",
    "    [tfms.update({k:ContTransformerNegExpRange}) for k in [\"logloss\"]]\n",
    "\n",
    "    fit_config(key, tfms=tfms, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = cfg('fcnet')\n",
    "\n",
    "b =BenchmarkSet('fcnet')\n",
    "\n",
    "b.config_space.get_hyperparameter_names() == cc.hp_names"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "935079f3ab4b06ec76910fd5af9cfadee87e8a756fe17d7789065f69c1782d29"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
