{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Transfer\n",
    "\n",
    "In the following example, we aim to find a single configuration \n",
    "from which we hope to achieve optimal performance on future datasets.\n",
    "\n",
    "In this case, we are interested in a solution across `instances`. \n",
    "We therefore do **not** set the instance on construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahpo_gym import * \n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# We do this on the 'lcbench' dataset\n",
    "b = BenchmarkSet(\"lcbench\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to establish this, we define the `FindDefault` class, taking as input the target variable and the name of the instance column.\n",
    "\n",
    "The method `find_default` then performs the search for a good default value:\n",
    "\n",
    "It iteratively evaluates random configurations and keeps track of the configuration with optimal `mean` across training instances.\n",
    "\n",
    "Note, that this example does not make use of multi-fidelity evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindDefault:\n",
    "    \"\"\"\n",
    "    Find a default.\n",
    "    \"\"\"\n",
    "    def __init__(self, target:str, instance_column:str):\n",
    "        self.target = target\n",
    "        self.instance_name = instance_column\n",
    "        \n",
    "        # First we split  the instances (= datasets) into train and  test instances\n",
    "        self.train_ins = random.sample(b.instances, math.ceil(len(b.instances)*0.75))\n",
    "        self.test_ins = [x for x in b.instances if x not in self.train_ins]\n",
    "    \n",
    "    def find_default(self, minimize, n_trials:int = 100):\n",
    "        \"\"\"\n",
    "            Sequentially evaluate n_trials random configurations across all training instances\n",
    "            in a given task only keeping the best.\n",
    "        \"\"\"\n",
    "        best_val = float(\"inf\")\n",
    "        for i in range(n_trials):\n",
    "            xs =  b.get_opt_space().sample_configuration(1).get_dictionary()\n",
    "            val = self.eval_objfun_mean(xs, self.train_ins, self.instance_name)\n",
    "            if not minimize:\n",
    "                val = -val\n",
    "            if val < best_val:\n",
    "                best_val = val\n",
    "                best_xs = xs\n",
    "        \n",
    "        best_xs.pop(self.instance_name, None)\n",
    "        if not minimize:\n",
    "            best_val = -best_val\n",
    "        return best_xs, best_val\n",
    "    \n",
    "    def eval_default(self, xs):\n",
    "        \"\"\"\n",
    "            Evaluate a default on test_instances\n",
    "        \"\"\"\n",
    "        return self.eval_objfun_mean(xs, self.test_ins, self.instance_name)\n",
    "        \n",
    "    \n",
    "    def eval_objfun_mean(self, xs, instances, ins_name):\n",
    "        \"\"\"\n",
    "        Compute the mean of xs evaluated across all instances.\n",
    "        \"\"\"\n",
    "        xb = self._batchify(xs, instances, ins_name)\n",
    "        return np.mean([b.objective_function(xbi)[0][self.target] for xbi in xb])\n",
    "        \n",
    "    def _batchify(self, xs, instances, ins_name):\n",
    "        \"\"\"\n",
    "        Turn a single configuration (Dictionary) into a list of configurations with different instances.\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for idx in instances:\n",
    "            xc = xs.copy()\n",
    "            xc.update({ins_name: idx})\n",
    "            res += [xc]\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: \n",
      "{'batch_size': 23, 'epoch': 49, 'learning_rate': 0.09366817888399974, 'max_dropout': 0.7010170343939439, 'max_units': 78.59175063007591, 'momentum': 0.5395333611999508, 'num_layers': 1, 'weight_decay': 0.03177330980842316}\n",
      "\n",
      "With performance (train): 86.96965789794922\n"
     ]
    }
   ],
   "source": [
    "fd = FindDefault('val_accuracy', 'OpenML_task_id')\n",
    "xs, val = fd.find_default(minimize = False)\n",
    "print(f\"Configuration: \\n{xs}\\n\")\n",
    "print(f\"With performance (train): {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the performance of the found configuration `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With performance (test): 81.81726837158203\n"
     ]
    }
   ],
   "source": [
    "val = fd.eval_default(xs)\n",
    "print(f\"With performance (test): {val}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad33c89a3db67bdfe5fd2b730189669bf1bcf1b69024c438924b107c2144aef0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
